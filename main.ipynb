{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Data-extraction-CRBA\" data-toc-modified-id=\"Data-extraction-CRBA-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Data extraction CRBA</a></span><ul class=\"toc-item\"><li><span><a href=\"#Backlog-of-tasks\" data-toc-modified-id=\"Backlog-of-tasks-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Backlog of tasks</a></span></li><li><span><a href=\"#Import-packages\" data-toc-modified-id=\"Import-packages-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Import packages</a></span></li><li><span><a href=\"#Import-country-list\" data-toc-modified-id=\"Import-country-list-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Import country list</a></span></li></ul></li><li><span><a href=\"#Data-extraction\" data-toc-modified-id=\"Data-extraction-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data extraction</a></span><ul class=\"toc-item\"><li><span><a href=\"#S-11-Economist-Intelligence-Unit,-Out-of-the-Shadows-Index.-Legal-Framework-score-only\" data-toc-modified-id=\"S-11-Economist-Intelligence-Unit,-Out-of-the-Shadows-Index.-Legal-Framework-score-only-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>S-11 Economist Intelligence Unit, Out of the Shadows Index. Legal Framework score only</a></span></li><li><span><a href=\"#S-55-(prev-S-15)-Percentage-of-out-of-school-adolescents-of-lower-secondary-school-age.\" data-toc-modified-id=\"S-55-(prev-S-15)-Percentage-of-out-of-school-adolescents-of-lower-secondary-school-age.-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>S-55 (prev S-15) Percentage of out-of-school adolescents of lower secondary school age.</a></span><ul class=\"toc-item\"><li><span><a href=\"#Extraction\" data-toc-modified-id=\"Extraction-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Extraction</a></span></li><li><span><a href=\"#Cleansing\" data-toc-modified-id=\"Cleansing-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>Cleansing</a></span></li><li><span><a href=\"#Normalization-(scaling)\" data-toc-modified-id=\"Normalization-(scaling)-2.2.3\"><span class=\"toc-item-num\">2.2.3&nbsp;&nbsp;</span>Normalization (scaling)</a></span></li></ul></li><li><span><a href=\"#S-56-(prev-S-16)-Percentage-of-out-of-school-youth-of-upper-secondary-school-age\" data-toc-modified-id=\"S-56-(prev-S-16)-Percentage-of-out-of-school-youth-of-upper-secondary-school-age-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>S-56 (prev S-16) Percentage of out-of-school youth of upper secondary school age</a></span><ul class=\"toc-item\"><li><span><a href=\"#Extraction\" data-toc-modified-id=\"Extraction-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>Extraction</a></span></li><li><span><a href=\"#Cleansing\" data-toc-modified-id=\"Cleansing-2.3.2\"><span class=\"toc-item-num\">2.3.2&nbsp;&nbsp;</span>Cleansing</a></span></li><li><span><a href=\"#Normalization-(scaling)\" data-toc-modified-id=\"Normalization-(scaling)-2.3.3\"><span class=\"toc-item-num\">2.3.3&nbsp;&nbsp;</span>Normalization (scaling)</a></span></li></ul></li><li><span><a href=\"#S-24-(prev.-S-14)-SDG-Indicator-8.7.1-Proportion-of-children-aged-5-17-years-engaged-in-child-labour-SL_TLF_CHLDEA\" data-toc-modified-id=\"S-24-(prev.-S-14)-SDG-Indicator-8.7.1-Proportion-of-children-aged-5-17-years-engaged-in-child-labour-SL_TLF_CHLDEA-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>S-24 (prev. S-14) SDG Indicator 8.7.1 Proportion of children aged 5-17 years engaged in child labour SL_TLF_CHLDEA</a></span><ul class=\"toc-item\"><li><span><a href=\"#Extraction\" data-toc-modified-id=\"Extraction-2.4.1\"><span class=\"toc-item-num\">2.4.1&nbsp;&nbsp;</span>Extraction</a></span></li><li><span><a href=\"#Cleansing\" data-toc-modified-id=\"Cleansing-2.4.2\"><span class=\"toc-item-num\">2.4.2&nbsp;&nbsp;</span>Cleansing</a></span></li><li><span><a href=\"#Normalization-(scaling)\" data-toc-modified-id=\"Normalization-(scaling)-2.4.3\"><span class=\"toc-item-num\">2.4.3&nbsp;&nbsp;</span>Normalization (scaling)</a></span></li><li><span><a href=\"#Left-join-to-counry-list-to-bring-it-into-target-format\" data-toc-modified-id=\"Left-join-to-counry-list-to-bring-it-into-target-format-2.4.4\"><span class=\"toc-item-num\">2.4.4&nbsp;&nbsp;</span>Left join to counry list to bring it into target format</a></span></li></ul></li><li><span><a href=\"#S-23-(prev.-S-17)-ILO-STAT-Informal-Employment-(%-of-total-non-agricultural-employment)-SL_ISV_IFEM\" data-toc-modified-id=\"S-23-(prev.-S-17)-ILO-STAT-Informal-Employment-(%-of-total-non-agricultural-employment)-SL_ISV_IFEM-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>S-23 (prev. S-17) ILO STAT Informal Employment (% of total non-agricultural employment) SL_ISV_IFEM</a></span><ul class=\"toc-item\"><li><span><a href=\"#Extraction\" data-toc-modified-id=\"Extraction-2.5.1\"><span class=\"toc-item-num\">2.5.1&nbsp;&nbsp;</span>Extraction</a></span></li><li><span><a href=\"#Transformation\" data-toc-modified-id=\"Transformation-2.5.2\"><span class=\"toc-item-num\">2.5.2&nbsp;&nbsp;</span>Transformation</a></span></li><li><span><a href=\"#Normalization-(scaling)\" data-toc-modified-id=\"Normalization-(scaling)-2.5.3\"><span class=\"toc-item-num\">2.5.3&nbsp;&nbsp;</span>Normalization (scaling)</a></span></li><li><span><a href=\"#Left-join-to-counry-list-to-bring-it-into-target-format\" data-toc-modified-id=\"Left-join-to-counry-list-to-bring-it-into-target-format-2.5.4\"><span class=\"toc-item-num\">2.5.4&nbsp;&nbsp;</span>Left join to counry list to bring it into target format</a></span></li></ul></li><li><span><a href=\"#S-60-(prev-S-18)-Walk-Free-Foundation.Global-Slavery-Index.-Prevalence-of-Modern-Slavery.-Prevalence-score-only.\" data-toc-modified-id=\"S-60-(prev-S-18)-Walk-Free-Foundation.Global-Slavery-Index.-Prevalence-of-Modern-Slavery.-Prevalence-score-only.-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>S-60 (prev S-18) Walk Free Foundation.Global Slavery Index. Prevalence of Modern Slavery. Prevalence score only.</a></span><ul class=\"toc-item\"><li><span><a href=\"#Extraction\" data-toc-modified-id=\"Extraction-2.6.1\"><span class=\"toc-item-num\">2.6.1&nbsp;&nbsp;</span>Extraction</a></span></li><li><span><a href=\"#Transform\" data-toc-modified-id=\"Transform-2.6.2\"><span class=\"toc-item-num\">2.6.2&nbsp;&nbsp;</span>Transform</a></span></li><li><span><a href=\"#Normalization-(scaling)\" data-toc-modified-id=\"Normalization-(scaling)-2.6.3\"><span class=\"toc-item-num\">2.6.3&nbsp;&nbsp;</span>Normalization (scaling)</a></span></li><li><span><a href=\"#Left-join-to-counry-list-to-bring-it-into-target-format\" data-toc-modified-id=\"Left-join-to-counry-list-to-bring-it-into-target-format-2.6.4\"><span class=\"toc-item-num\">2.6.4&nbsp;&nbsp;</span>Left join to counry list to bring it into target format</a></span></li></ul></li><li><span><a href=\"#S-61-(prev-S-19)-SDG-Indicator-16.2.2-Detected-victims-of-human-trafficking,-by-age-and-sex-(number)--VC_HTF_DETV\" data-toc-modified-id=\"S-61-(prev-S-19)-SDG-Indicator-16.2.2-Detected-victims-of-human-trafficking,-by-age-and-sex-(number)--VC_HTF_DETV-2.7\"><span class=\"toc-item-num\">2.7&nbsp;&nbsp;</span>S-61 (prev S-19) SDG Indicator 16.2.2 Detected victims of human trafficking, by age and sex (number)  VC_HTF_DETV</a></span><ul class=\"toc-item\"><li><span><a href=\"#Extraction\" data-toc-modified-id=\"Extraction-2.7.1\"><span class=\"toc-item-num\">2.7.1&nbsp;&nbsp;</span>Extraction</a></span></li><li><span><a href=\"#Cleanse\" data-toc-modified-id=\"Cleanse-2.7.2\"><span class=\"toc-item-num\">2.7.2&nbsp;&nbsp;</span>Cleanse</a></span></li><li><span><a href=\"#Normalization-(scaling)\" data-toc-modified-id=\"Normalization-(scaling)-2.7.3\"><span class=\"toc-item-num\">2.7.3&nbsp;&nbsp;</span>Normalization (scaling)</a></span></li><li><span><a href=\"#Left-join-to-counry-list-to-bring-it-into-target-format\" data-toc-modified-id=\"Left-join-to-counry-list-to-bring-it-into-target-format-2.7.4\"><span class=\"toc-item-num\">2.7.4&nbsp;&nbsp;</span>Left join to counry list to bring it into target format</a></span></li></ul></li><li><span><a href=\"#S-62-(prev-S-20)-SDG-Indicator-1.1.1.-Proportion-of-population-below-international-poverty-line-(%)-SI_POV_DAY1\" data-toc-modified-id=\"S-62-(prev-S-20)-SDG-Indicator-1.1.1.-Proportion-of-population-below-international-poverty-line-(%)-SI_POV_DAY1-2.8\"><span class=\"toc-item-num\">2.8&nbsp;&nbsp;</span>S-62 (prev S-20) SDG Indicator 1.1.1. Proportion of population below international poverty line (%) SI_POV_DAY1</a></span><ul class=\"toc-item\"><li><span><a href=\"#Extraction\" data-toc-modified-id=\"Extraction-2.8.1\"><span class=\"toc-item-num\">2.8.1&nbsp;&nbsp;</span>Extraction</a></span></li><li><span><a href=\"#Transformation\" data-toc-modified-id=\"Transformation-2.8.2\"><span class=\"toc-item-num\">2.8.2&nbsp;&nbsp;</span>Transformation</a></span></li><li><span><a href=\"#Normalization-(scaling)\" data-toc-modified-id=\"Normalization-(scaling)-2.8.3\"><span class=\"toc-item-num\">2.8.3&nbsp;&nbsp;</span>Normalization (scaling)</a></span></li><li><span><a href=\"#Left-join-to-counry-list-to-bring-it-into-target-format\" data-toc-modified-id=\"Left-join-to-counry-list-to-bring-it-into-target-format-2.8.4\"><span class=\"toc-item-num\">2.8.4&nbsp;&nbsp;</span>Left join to counry list to bring it into target format</a></span></li></ul></li></ul></li><li><span><a href=\"#---------Archive/-Trash/-dev-section-(Disregard-this-section)-----------\" data-toc-modified-id=\"---------Archive/-Trash/-dev-section-(Disregard-this-section)------------3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span><div align=\"center\"> - - - - Archive/ Trash/ dev section (Disregard this section) - - - - - </div></a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Left-join-to-counry-list-to-bring-it-into-target-format\" data-toc-modified-id=\"Left-join-to-counry-list-to-bring-it-into-target-format-3.0.1\"><span class=\"toc-item-num\">3.0.1&nbsp;&nbsp;</span>Left join to counry list to bring it into target format</a></span></li><li><span><a href=\"#Left-join-to-counry-list-to-bring-it-into-target-format\" data-toc-modified-id=\"Left-join-to-counry-list-to-bring-it-into-target-format-3.0.2\"><span class=\"toc-item-num\">3.0.2&nbsp;&nbsp;</span>Left join to counry list to bring it into target format</a></span></li></ul></li><li><span><a href=\"#Define-function\" data-toc-modified-id=\"Define-function-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Define function</a></span><ul class=\"toc-item\"><li><span><a href=\"#Generic-functions\" data-toc-modified-id=\"Generic-functions-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Generic functions</a></span></li><li><span><a href=\"#UNESCO-API-specific-functions\" data-toc-modified-id=\"UNESCO-API-specific-functions-3.1.2\"><span class=\"toc-item-num\">3.1.2&nbsp;&nbsp;</span>UNESCO API specific functions</a></span></li><li><span><a href=\"#SDG-API-specific-functions\" data-toc-modified-id=\"SDG-API-specific-functions-3.1.3\"><span class=\"toc-item-num\">3.1.3&nbsp;&nbsp;</span>SDG API specific functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Transforming-ILO-data\" data-toc-modified-id=\"Transforming-ILO-data-3.1.3.1\"><span class=\"toc-item-num\">3.1.3.1&nbsp;&nbsp;</span>Transforming ILO data</a></span></li><li><span><a href=\"#Extracting-data-through-ILO-API\" data-toc-modified-id=\"Extracting-data-through-ILO-API-3.1.3.2\"><span class=\"toc-item-num\">3.1.3.2&nbsp;&nbsp;</span>Extracting data through ILO API</a></span></li></ul></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data extraction CRBA\n",
    "\n",
    "* This is the main file, which must be executed to extract data from all soures for which the process has been automated\n",
    "* For a list of all soures along with the information whether its a manual or atutomated extraction please consult the follownig file: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backlog of tasks\n",
    "\n",
    " - [X] Write the functions to extract data from UNESCO API into one function which takes the indicator code as argument (rather than having two functions with the same functionatlity for two indicators | and check if that ingo is not also in the SDG API anyways\n",
    " - [ ] Finalize understanding the maplecroft logic for a quantaitive indicator and then for: qualitative indicator and also understand all the exceptions (.e.g inverted indicator etc.) --> Draw a decision tree of all if-then statements and check if this logic is desired by Alex and Tomas\n",
    " - [ ] Implement this logic in the python function started below\n",
    " - [ ] Check with Alex/ Tomas what the final result should look like (raw data + scaled data + aggregated data = final index score for each indicator + aggregrated index scores\n",
    " - [ ] Check with Daniele in what format these things should be in SDMX (should they include dimensions like sex, age, ... SHould they include units etc.?\n",
    " - [ ] Start implementing INdicator by indicator (rather than extracting data from all sources first before transforming anything)\n",
    " - [ ] For log file: \n",
    "    * Summary statistics of indicator\n",
    "    * Coverage (how many NA values) for indicator\n",
    "    * Data on average from which year \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import urllib\n",
    "import requests\n",
    "import os\n",
    "import numpy as np\n",
    "from extract.unesco_extractor import extract_unesco_api_data\n",
    "from extract.sdg_extractor import extract_sdg_api_data\n",
    "from cleanse.unesco_cleanser import cleanse_unesco_api_data\n",
    "from cleanse.sdg_cleanser import cleanse_sdg_api_data\n",
    "from cleanse.save_cleansed_data import save_cleansed_data\n",
    "from extract import save_raw_data\n",
    "from normalize import scaler\n",
    "from normalize import save_normalized_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import country list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the export path for all RAW data sources\n",
    "cwd = os.getcwd()\n",
    "\n",
    "data_sources_raw = cwd + \"\\data\\data_raw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the list of countries which contains all different variations of country names \n",
    "country_full_list = pd.read_excel(cwd + '\\\\all_countrynames_list.xlsx',\n",
    "                                 keep_default_na = False).drop_duplicates()\n",
    "\n",
    "# Create a version of the list with unique ISO2 and ISO3 codes\n",
    "country_iso_list = country_full_list.drop_duplicates(subset = 'CountryIso2')\n",
    "\n",
    "# Country CRBA list, this is the list of the countries that should be contained \n",
    "country_crba_list = pd.read_excel(cwd + '\\\\crba_country_list.xlsx',\n",
    "                                 header = None,\n",
    "                                 usecols = [0, 1], \n",
    "                                 names = ['COUNTRY_ISO_3', 'COUNTRY_NAME']).merge(\n",
    "                                        right = country_iso_list,\n",
    "                                        how = 'left',\n",
    "                                        left_on = 'COUNTRY_ISO_3',\n",
    "                                        right_on = 'CountryIso3',\n",
    "                                        validate = 'one_to_one')[\n",
    "    ['COUNTRY_ISO_3', 'COUNTRY_NAME', 'CountryIso2']].rename(columns = {'CountryIso2': \"COUNTRY_ISO_2\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S-11 Economist Intelligence Unit, Out of the Shadows Index. Legal Framework score only\n",
    "\n",
    "I have decided to not automate this process, because the Excel is so nested that it is very difficult to do that. And it's faster and less erorr prone to retrieve the data manually. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S-55 (prev S-15) Percentage of out-of-school adolescents of lower secondary school age. \n",
    "\n",
    "The data of this source is retrieved from the UNSECO API, for which you must have an API key to acces it. For that, you must sign up. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The raw data has been saved as .xlsx file in: D:\\Documents\\2020\\28_UNICEF\\10_working_repo\\data-etl\\data\\data_raw\\\n"
     ]
    }
   ],
   "source": [
    "# Extract data\n",
    "s55_raw = extract_unesco_api_data(\n",
    "    stat_unit = 'ROFST',\n",
    "    unit = 'PT',\n",
    "    edu_level = 'L2',\n",
    "    subs_key = '460ab272abdd43c892bb59c218c22c09',\n",
    "    start_period = '2010',\n",
    "    end_period = '2020')\n",
    "\n",
    "# Save raw data\n",
    "save_raw_data.save_raw_data(dataframe = s55_raw,\n",
    "             filename = 'S_55_raw.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanse raw data\n",
    "s55_cleansed = cleanse_unesco_api_data(\n",
    "    uis_data = s55_raw,\n",
    "    country_df = country_crba_list,\n",
    "    country_df_iso2_col = 'COUNTRY_ISO_2',\n",
    "    columns = ['EDU_CAT', 'SEX', 'AGE'],\n",
    "    most_recent_only = True)\n",
    "\n",
    "# save cleansed data\n",
    "save_cleansed_data(dataframe = s55_cleansed,\n",
    "             filename = 'S_55_cleansed.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization (scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Scale the raw data and bring the dataframe into long formmat\n",
    "s55_scaled = scaler.scaler(raw_dataframe = s55_cleansed,\n",
    "                   indicator_raw_value = 'OBS_VALUE',\n",
    "                   inverted = True)\n",
    "\n",
    "# save cleansed data\n",
    "save_normalized_data.save_normalized_data(dataframe = s55_scaled,\n",
    "             filename = 'S_55_normalized.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S-56 (prev S-16) Percentage of out-of-school youth of upper secondary school age\n",
    "\n",
    "The data of this source is retrieved from the UNSECO API, for which you must have an API key to acces it. For that, you must sign up. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data\n",
    "s56_raw = extract_unesco_api_data(\n",
    "    stat_unit = 'ROFST',\n",
    "    unit = 'PT',\n",
    "    edu_level = 'L3',\n",
    "    subs_key = '460ab272abdd43c892bb59c218c22c09',\n",
    "    start_period = '2010',\n",
    "    end_period = '2020')\n",
    "\n",
    "# Save raw data\n",
    "save_raw_data.save_raw_data(dataframe = s56_raw,\n",
    "             filename = 'S_56_raw.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform raw data\n",
    "s56_cleansed = cleanse_unesco_api_data(\n",
    "    uis_data = s56_raw,\n",
    "    country_df = country_crba_list,\n",
    "    country_df_iso2_col = 'COUNTRY_ISO_2',\n",
    "    columns = ['EDU_CAT', 'SEX', 'AGE'],\n",
    "    most_recent_only = True)\n",
    "\n",
    "# save cleansed data\n",
    "save_cleansed_data(dataframe = s55_cleansed,\n",
    "             filename = 'S_56_cleansed.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization (scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Scale the raw data and bring the dataframe into long formmat\n",
    "s56_scaled = scaler.scaler(raw_dataframe = s56_cleansed,\n",
    "                   indicator_raw_value = 'OBS_VALUE',\n",
    "                   inverted = True)\n",
    "\n",
    "# save cleansed data\n",
    "save_cleansed_data(dataframe = s56_scaled,\n",
    "             filename = 'S_56_scaled.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S-24 (prev. S-14) SDG Indicator 8.7.1 Proportion of children aged 5-17 years engaged in child labour SL_TLF_CHLDEA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The extracted raw data contains the following columns: Index(['goal', 'target', 'indicator', 'series', 'seriesDescription',\n",
      "       'seriesCount', 'geoAreaCode', 'geoAreaName', 'timePeriodStart', 'value',\n",
      "       'valueType', 'time_detail', 'timeCoverage', 'upperBound', 'lowerBound',\n",
      "       'basePeriod', 'source', 'geoInfoUrl', 'footnotes', 'attributes.Nature',\n",
      "       'attributes.Units', 'dimensions.Age', 'dimensions.Sex',\n",
      "       'dimensions.Reporting Type'],\n",
      "      dtype='object')\n",
      "The raw data has been saved as .xlsx file in: D:\\Documents\\2020\\28_UNICEF\\10_working_repo\\data-etl\\data\\data_raw\\\n"
     ]
    }
   ],
   "source": [
    "# Extract data and directly flatten it out into a pandas dataframe\n",
    "s24_raw = extract_sdg_api_data(series_code = 'SL_TLF_CHLDEA')\n",
    "\n",
    "# Save data to raw data folder\n",
    "from extract import save_raw_data\n",
    "save_raw_data.save_raw_data(dataframe = s24_raw,\n",
    "             filename = 'S_24.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The raw data has been saved as .xlsx file in: D:\\Documents\\2020\\28_UNICEF\\10_working_repo\\data-etl\\data\\data_cleansed\\\n"
     ]
    }
   ],
   "source": [
    "s24_cleansed = cleanse_sdg_api_data(raw_data = s24_raw,\n",
    "                                   country_list_full = country_full_list,\n",
    "                                   country_list_full_name_col = 'CountryDesc',\n",
    "                                   country_list_full_iso2_col = 'CountryIso2',\n",
    "                                   country_df = country_crba_list,\n",
    "                                   country_df_iso2_col = 'COUNTRY_ISO_2'\n",
    "                                   )\n",
    "\n",
    "# save cleansed data\n",
    "save_cleansed_data(dataframe = s24_cleansed,\n",
    "             filename = 'S_24_cleansed.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization (scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # # # # # # # stopped here --> Need to modify scaler function so that it is able to deal with dimensions # # # # #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The distribution of the raw data values contains outliers or is too skewed on the upper end. The maximum value to be used for the normalisation is: 3rd quartile or distribution + 1.5 * IQR. It is: 39.64999999999999 \n",
      " See histogram printed below for info. \n",
      "\n",
      "The distribution of the raw data does not contain outliers or is too skewed on the lower end. The minimum value used for the normalisation is the minimum value in the dataset, which is 0.11869. This value corresponds to country: 246    Turkmenistan\n",
      "Name: COUNTRY_NAME, dtype: object \n",
      "\n",
      "\n",
      " This is the distribution of the raw data of the indicator.\n",
      "AxesSubplot(0.125,0.125;0.775x0.755)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scale the raw data and bring the dataframe into long formmat\n",
    "s24_scaled = scaler.scaler(raw_dataframe = s24_cleansed,\n",
    "                   indicator_raw_value = 'value',\n",
    "                   inverted = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Left join to counry list to bring it into target format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate data to the\n",
    "s24_final = country_full_list.merge(right = s24_scaled,\n",
    "                 indicator = True,\n",
    "                 how = 'left',\n",
    "                 left_on = 'CountryDesc',\n",
    "                 right_on = 'geoAreaName',\n",
    "                 validate = 'many_to_one').assign(indicator ='I-14')\n",
    "\n",
    "# Check if some countries got lost during the join\n",
    "print('The number of rows in the raw data set was {}. The number of available datapoints in the final dataframe is {}. Therefore, {} rows got lost during the join.'.format(\n",
    "    len(s24_scaled), \n",
    "    s24_final.value.notna().sum(), \n",
    "    len(s24_scaled) - s24_final.value.notna().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S-23 (prev. S-17) ILO STAT Informal Employment (% of total non-agricultural employment) SL_ISV_IFEM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction\n",
    "\n",
    "This data can be extracted from two different sources: \n",
    "    \n",
    "* ILO\n",
    "* SDG database\n",
    "\n",
    "After consultation with Alex and Tomás we have decided to extract the data from the SDG API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The extracted raw data contains the following columns: Index(['goal', 'target', 'indicator', 'series', 'seriesDescription',\n",
      "       'seriesCount', 'geoAreaCode', 'geoAreaName', 'timePeriodStart', 'value',\n",
      "       'valueType', 'time_detail', 'timeCoverage', 'upperBound', 'lowerBound',\n",
      "       'basePeriod', 'source', 'geoInfoUrl', 'footnotes', 'attributes.Nature',\n",
      "       'attributes.Units', 'dimensions.Sex', 'dimensions.Reporting Type',\n",
      "       'dimensions.Activity'],\n",
      "      dtype='object')\n",
      "The raw data has been saved as .xlsx file in: D:\\Documents\\2020\\28_UNICEF\\10_working_repo\\data-etl\\data\\data_raw\\\n"
     ]
    }
   ],
   "source": [
    "# Extract data and directly flatten it out into a pandas dataframe\n",
    "s23_raw = extract_sdg_api_data(series_code = 'SL_ISV_IFEM')\n",
    "\n",
    "# Save data to raw data folder\n",
    "save_raw_data.save_raw_data(dataframe = s23_raw,\n",
    "             filename = 'S_23.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We are intested in sector = non-agricultural and want the data for both sexes. Filter the other rows out:\n",
    "temp = s23_raw[(s23_raw['dimensions.Sex'] == 'BOTHSEX') & (s23_raw['dimensions.Activity'] == 'NONAGR')]\n",
    "\n",
    "# transform data\n",
    "s23_cleansed = transform_sdg_api_data(raw_data = temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization (scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the raw data and \n",
    "s23_scaled = s23_cleansed.assign(\n",
    "    scaled = scaler(s23_cleansed.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Left join to counry list to bring it into target format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate data to the\n",
    "s23_final = country_full_list.merge(right = s23_scaled,\n",
    "                 indicator = True,\n",
    "                 how = 'outer',\n",
    "                 left_on = 'CountryDesc',\n",
    "                 right_on = 'geoAreaName',\n",
    "                 validate = 'many_to_one').assign(indicator ='I-17')\n",
    "\n",
    "# Check if some countries got lost during the join\n",
    "print('The number of rows in the raw data set was {}. The number of available datapoints in the final dataframe is {}. Therefore, {} rows got lost during the join.'.format(\n",
    "    len(s23_scaled), \n",
    "    s23_final.value.notna().sum(), \n",
    "    len(s23_scaled) - s23_final.value.notna().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S-60 (prev S-18) Walk Free Foundation.Global Slavery Index. Prevalence of Modern Slavery. Prevalence score only.\n",
    "### Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data\n",
    "url = 'http://downloads.globalslaveryindex.org/ephemeral/FINAL-GSI-2018-DATA-G20-AND-FISHING-1597151668.xlsx'\n",
    "\n",
    "s60_raw = pd.read_excel(io = url, \n",
    "                      sheet_name = 'Global prev, vuln, govt table',\n",
    "                       skiprows = 2)\n",
    "\n",
    "# Save data to raw data folder\n",
    "save_raw_data.save_raw_data(dataframe = s60_raw,\n",
    "             filename = 'S_60.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Filter relevant columns\n",
    "s60_raw = s60_raw[['Country ', # FYI: note the trailing space after Country\n",
    "        'Est. prevalence of population in modern slavery (victims per 1,000 population)']]\n",
    "\n",
    "# The data is from 2018, and is therefore assumed to be reflecting the prevalen in the year 2018\n",
    "s60_cleansed = s60_raw.assign(Year = 2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization (scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the raw data and \n",
    "s60_scaled = s60_cleansed.assign(\n",
    "    scaled = scaler(s60_cleansed['Est. prevalence of population in modern slavery (victims per 1,000 population)']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Left join to counry list to bring it into target format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate data to the\n",
    "s60_final = country_full_list.merge(right = s60_scaled,\n",
    "                 indicator = True,\n",
    "                 how = 'inner',\n",
    "                 left_on = 'CountryDesc',\n",
    "                 right_on = 'Country ',\n",
    "                 validate = 'many_to_one').assign(indicator ='I-18')\n",
    "\n",
    "# Check if some countries got lost during the join\n",
    "print('The number of rows in the raw data set was {}. The number of available datapoints in the final dataframe is {}. This difference is explicable because there were already missing values for the value in sxx_ scaled dataset'.format(\n",
    "    len(s60_scaled), \n",
    "    s60_final['Est. prevalence of population in modern slavery (victims per 1,000 population)'].notna().sum(), \n",
    "    len(s60_scaled) - s60_final['Est. prevalence of population in modern slavery (victims per 1,000 population)'].notna().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S-61 (prev S-19) SDG Indicator 16.2.2 Detected victims of human trafficking, by age and sex (number)  VC_HTF_DETV \n",
    "### Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data\n",
    "s61_raw = extract_sdg_api_data(series_code = 'VC_HTF_DETV')\n",
    "\n",
    "# Save raw data\n",
    "save_raw_data.save_raw_data(dataframe = s61_raw,\n",
    "              filename = 'S_61.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s61_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the raw data\n",
    "s61_cleansed = transform_sdg_api_data(raw_data = s61_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization (scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the raw data and \n",
    "s61_scaled = s61_cleansed.assign(\n",
    "    scaled = scaler(s61_cleansed.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Left join to counry list to bring it into target format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Concatenate data to the\n",
    "s61_final = country_full_list.merge(right = s61_scaled,\n",
    "                 indicator = True,\n",
    "                 how = 'outer',\n",
    "                 left_on = 'CountryDesc',\n",
    "                 right_on = 'geoAreaName',\n",
    "                 validate = 'many_to_one').assign(indicator ='I-19')\n",
    "\n",
    "# Check if some countries got lost during the join\n",
    "print('The number of rows in the raw data set was {}. The number of available datapoints in the final dataframe is {}. Therefore, {} rows got lost during the join.'.format(\n",
    "    len(s61_scaled), \n",
    "    s61_final.value.notna().sum(), \n",
    "    len(s61_scaled) - s61_final.value.notna().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S-62 (prev S-20) SDG Indicator 1.1.1. Proportion of population below international poverty line (%) SI_POV_DAY1 \n",
    "### Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data\n",
    "s62_raw = extract_sdg_api_data(series_code = 'SI_POV_DAY1')\n",
    "\n",
    "# Save raw data\n",
    "save_raw_data(dataframe = s62_raw,\n",
    "              filename = 'S_62.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the raw data\n",
    "s62_cleansed = transform_sdg_api_data(raw_data = s62_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization (scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the raw data and \n",
    "s62_scaled = s62_cleansed.assign(\n",
    "    scaled = scaler(s62_cleansed.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Left join to counry list to bring it into target format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate data to the\n",
    "s62_final = country_full_list.merge(right = s62_scaled,\n",
    "                 indicator = True,\n",
    "                 how = 'outer',\n",
    "                 left_on = 'CountryDesc',\n",
    "                 right_on = 'geoAreaName', # validate = 'many_to_one'\n",
    "                 ).assign(indicator ='I-20')\n",
    "\n",
    "# Check if some countries got lost during the join\n",
    "print('The number of rows in the raw data set was {}. The number of available datapoints in the final dataframe is {}. Therefore, {} rows got lost during the join.'.format(\n",
    "    len(s62_scaled), \n",
    "    s61_final.value.notna().sum(), \n",
    "    len(s62_scaled) - s62_final.value.notna().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#  <div align=\"center\"> - - - - Archive/ Trash/ dev section (Disregard this section) - - - - - </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def cleanse_sdg_api_data(raw_data):\n",
    "    \"\"\"Transform raw data from sdg API source to distil relevant data \n",
    "\n",
    "    Discard columns and aggregate data to retrieve the latest value for all countries in a given dataframe for a given SDG API series.\n",
    "\n",
    "    Parameters:\n",
    "    raw_data (obj): Return of function 'extract_sdg_api_data'. Should be a pandas dataframe \n",
    "    \n",
    "    Returns:\n",
    "    obj: Returns pandas dataframe\n",
    "\n",
    "   \"\"\"\n",
    "    # Extract relevant columns\n",
    "    stage_data = raw_data[['geoAreaCode',\n",
    "            'geoAreaName',\n",
    "            'timePeriodStart',\n",
    "            'value']]\n",
    "    \n",
    "    # Get the latest value for each country of the series\n",
    "    return(stage_data[stage_data['timePeriodStart'] == stage_data.groupby('geoAreaName')['timePeriodStart'].transform('max')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Raw data contains data on BOTHSEX, MALE AND FEMALE, limit ourseleves to BOTHSEXES\n",
    "temp = s24_raw[s24_raw['dimensions.Sex'] == 'BOTHSEX']\n",
    "\n",
    "# Transform the raw data\n",
    "s24_cleansed = cleanse_sdg_api_data(raw_data = temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s24_raw[s24_raw['geoAreaName'] == 'Burkina Faso']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "    #\n",
    "    #\n",
    "    #\n",
    "    '''\n",
    "    # Discard rows of countries that are not in the master country list\n",
    "        # The raw data only contains country names. Assign ISO codes to these country names    \n",
    "    raw_data_iso = raw_data.merge(\n",
    "        right = country_list_full,\n",
    "        how = 'left',\n",
    "        left_on = 'geoAreaName',\n",
    "        right_on = country_list_full_name_col,\n",
    "        validate = 'many_to_one')\n",
    "    \n",
    "        # Discard countries that aren't part of the final CRBA master list\n",
    "    contry_filt_data = raw_data_iso.merge(\n",
    "        right = country_df,\n",
    "        how = 'right', \n",
    "        left_on = country_list_full_iso2_col,\n",
    "        right_on = country_df_iso2_col,\n",
    "        indicator = True,\n",
    "        validate = 'many_to_one')\n",
    "         \n",
    "    # Get the latest value for each country of the series\n",
    "    col_list = contry_filt_data.columns.to_list() # list of all columns in the dataframe\n",
    "    col_list_gb = [e for e in col_list if e not in ('value', 'timePeriodStart')] # exclude timePeriodStart and value, because these one'saren't used for the groupby statement\n",
    "    \n",
    "    # Some of the columns contain values which Python interprets as lists (e.g. [8.1]). These make the groupby statement malfunction. Convert them to string\n",
    "    contry_filt_data[col_list_gb] =  contry_filt_data[col_list_gb].astype(str)\n",
    "    \n",
    "    # Retreive the latest available data for each group, where group is 'col_list_gb'\n",
    "    latest_year_data = contry_filt_data[contry_filt_data['timePeriodStart'] == contry_filt_data.groupby(col_list_gb)['timePeriodStart'].transform('max')]\n",
    "    \n",
    "    # return result\n",
    "    return(latest_year_data.sort_values(by = country_df_iso2_col, axis = 0))\n",
    "    ''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "    print(latest_year_data.COUNTRY_ISO_2.describe())\n",
    "    print(contry_filt_data.COUNTRY_ISO_2.describe())\n",
    "    \n",
    "# Include a confirmation whether all of this went well\n",
    "    print(raw_data.shape)\n",
    "    print(country_df.shape)\n",
    "    print(raw_data_iso.shape)\n",
    "    print(contry_filt_data.shape)\n",
    "    print(contry_filt_data[contry_filt_data._merge == 'left_only'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s24_raw.geoAreaName.describe()\n",
    "country_full_list.CountryDesc.describe()\n",
    "\n",
    "col_list = s24_raw.columns.to_list() # list of all columns in the dataframe\n",
    "print(col_list)\n",
    "col_list_gb = [e for e in col_list if e not in ('value', 'timePeriodStart')]\n",
    "print(col_list_gb)\n",
    "\n",
    "# s24_raw['timePeriodStart'] = \n",
    "\n",
    "# s24_raw.groupby(by = 'geoAreaName')['timePeriodStart'].max()\n",
    "\n",
    "s24_raw[col_list_gb] = s24_raw[col_list_gb].astype(str)\n",
    "\n",
    "s24_raw[s24_raw['timePeriodStart'] == s24_raw.groupby(col_list_gb)['timePeriodStart'].transform('max')]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "    \n",
    "    ''' No this codeblock will probably be discarded\n",
    "    # Extract relevant columns\n",
    "    stage_data = raw_data[['geoAreaCode',\n",
    "            'geoAreaName',\n",
    "            'timePeriodStart',\n",
    "            'value']]\n",
    "    '''\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "THIS IS ONLY ABOUT EXTRACTING THE LATEST VALUE AND ABOUT FILTERING OUT COUNTRIES WHO SHOULDN'T BE IN THERE\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "\n",
    "    # Discard unnecessay columns\n",
    "    columns_kept = ['REF_AREA', 'TIME_PERIOD','OBS_VALUE'] + columns\n",
    "    uis_data = uis_data[columns_kept]\n",
    "    \n",
    "    if most_recent_only == True:\n",
    "        # Retrieve the most up-to-date number for each country\n",
    "        uis_data = uis_data[uis_data['TIME_PERIOD']==uis_data.groupby('REF_AREA')['TIME_PERIOD'].transform('max')]\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    # Discard rows of countries that are not in the master country list\n",
    "    cleansed_final = uis_data.merge(\n",
    "        right = country_df,\n",
    "        how = 'right', \n",
    "        left_on = 'REF_AREA',\n",
    "        right_on = country_df_iso2_col,\n",
    "        validate = 'one_to_one').assign(indicator = 'I-15')\n",
    "    \n",
    "    return(cleansed_final.sort_values(by = country_df_iso2_col, axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Left join to counry list to bring it into target format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Concatenate data to the\n",
    "s56_final = country_iso_list.merge(right = s56_scaled,\n",
    "                 how = 'left',\n",
    "                 left_on = 'CountryIso2',\n",
    "                 right_on = 'REF_AREA',\n",
    "                 validate = 'many_to_one').assign(indicator ='I-16')\n",
    "\n",
    "# Check if some countries got lost during the join\n",
    "print('The number of rows in the raw data set was {}. The number of available datapoints in the final dataframe is {}. Therefore, {} rows got lost during the join.'.format(\n",
    "    len(s56_scaled), \n",
    "    s56_final.OBS_VALUE.notna().sum(), \n",
    "    len(s56_scaled) - s56_final.OBS_VALUE.notna().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Left join to counry list to bring it into target format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Left join data to the target format\n",
    "s55_final = country_iso_list.merge(right = s55_scaled,\n",
    "                 how = 'left',\n",
    "                 left_on = 'CountryIso2',\n",
    "                 right_on = 'REF_AREA',\n",
    "                 validate = 'one_to_many').assign(indicator = 'I-15')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Define function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Generic functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def save_raw_data(dataframe, filename, output_path = data_sources_raw):\n",
    "    \"\"\"Save raw data \n",
    "\n",
    "    Parameters:\n",
    "    dataframe (obj): Pandas dataframe to be stored\n",
    "    filename (string): The way you would like o anem your file. Must include the extension (for example .xlsx)\n",
    "    output_path (string): Folder where data is stored\n",
    "\n",
    "    Returns:\n",
    "    obj: \n",
    "\n",
    "   \"\"\"\n",
    "    dataframe.to_excel(output_path + filename)\n",
    "    print('The raw data has been saved as .xlsx file in: ' + output_path)\n",
    "\n",
    "    \n",
    "def scaler(raw_dataframe, indicator_raw_value, cat_var = False, cat_scoring_type = None, inverted = False, whisker_factor = 1.5):\n",
    "    \"\"\" Transform raw data into scaled (normalized) data in long format\n",
    "    \n",
    "    Parameters:\n",
    "    raw_dataframe (obj): Raw dataset, pandas dataframe\n",
    "    indicator_raw_value (str): Column containing the actual raw data values \n",
    "    cat_var (bool): is variable of type categorical or numerical?\n",
    "    cat_scoring_type (str): For categorical variables, specify the scoring type (i.e. what values the categorical variable can take)\n",
    "    inverted (bool): If we are dealing with a numerical variable, should the values be inverted?\n",
    "    whisker_factor (float): \n",
    "\n",
    "    Returns:\n",
    "    obj: Returns numpy array with transformed values\n",
    "    \"\"\"\n",
    "    # Check if an indicator is numeric of categorical\n",
    "    if cat_var == True:\n",
    "        # This is the section dealing with categorical indicators\n",
    "        # There are various scoring_types a categorical variable can take. For each one, different labels apply to the values\n",
    "        # Type 2-1-0\n",
    "        if cat_scoring_type == 'Type 2-1-0':\n",
    "            # Specify the values the raw data can take\n",
    "            conditions = [\n",
    "                (raw_dataframe[indicator_raw_value] == 0),\n",
    "                (raw_dataframe[indicator_raw_value] == 1),\n",
    "                (raw_dataframe[indicator_raw_value] == 2)\n",
    "                ]\n",
    "            \n",
    "            # Specify the normalized values\n",
    "            norm_values = ['No data', '0.00', '10.00']\n",
    "\n",
    "        # Type 2-1\n",
    "        elif cat_scoring_type == 'Type 2-1':\n",
    "            # Specify the values the raw data can take\n",
    "            conditions = [\n",
    "                (raw_dataframe[indicator_raw_value] == 1),\n",
    "                (raw_dataframe[indicator_raw_value] == 2)\n",
    "                ]\n",
    "            \n",
    "            # Specify the normalized values\n",
    "            norm_values = ['0.00', '10.00']\n",
    "\n",
    "        # Type 3-2-1\n",
    "        elif cat_scoring_type == 'Type 3-2-1':\n",
    "            # Specify the values the raw data can take\n",
    "            conditions = [\n",
    "                (raw_dataframe[indicator_raw_value] == 1),\n",
    "                (raw_dataframe[indicator_raw_value] == 2),\n",
    "                (raw_dataframe[indicator_raw_value] == 3)\n",
    "                ]\n",
    "            \n",
    "            # Specify the normalized values\n",
    "            norm_values = ['0.00', '5.00', '10.00']\n",
    "\n",
    "        # Type 3-2-1-0\n",
    "        elif cat_scoring_type == 'Type 3-2-1-0':\n",
    "            # Specify the values the raw data can take\n",
    "            conditions = [\n",
    "                (raw_dataframe[indicator_raw_value] == 0)\n",
    "                (raw_dataframe[indicator_raw_value] == 1),\n",
    "                (raw_dataframe[indicator_raw_value] == 2),\n",
    "                (raw_dataframe[indicator_raw_value] == 3)\n",
    "                ]\n",
    "            \n",
    "            # Specify the normalized values\n",
    "            norm_values = ['No data', '0.00', '5.00', '10.00']            \n",
    "            \n",
    "        # Type 4-3-2-1\n",
    "        elif cat_scoring_type == 'Type 4-3-2-1':\n",
    "            # Specify the values the raw data can take\n",
    "            conditions = [\n",
    "                (raw_dataframe[indicator_raw_value] == 1),\n",
    "                (raw_dataframe[indicator_raw_value] == 2),\n",
    "                (raw_dataframe[indicator_raw_value] == 3),\n",
    "                (raw_dataframe[indicator_raw_value] == 4)\n",
    "                ]\n",
    "            \n",
    "            # Specify the normalized values\n",
    "            norm_values = ['0.00', '3.33', '6.67', '10.00']                \n",
    "            \n",
    "            \n",
    "            # create a new column and use np.select to assign values to it using our lists as arguments\n",
    "            raw_dataframe['SCALED'] = np.select(conditions, values)\n",
    "\n",
    "        # Type 4-3-2-1-0\n",
    "        elif cat_scoring_type == 'Type 4-3-2-1-0':\n",
    "            # Specify the values the raw data can take\n",
    "            conditions = [\n",
    "                (raw_dataframe[indicator_raw_value] == 0)\n",
    "                (raw_dataframe[indicator_raw_value] == 1),\n",
    "                (raw_dataframe[indicator_raw_value] == 2),\n",
    "                (raw_dataframe[indicator_raw_value] == 3),\n",
    "                (raw_dataframe[indicator_raw_value] == 4)\n",
    "                ]\n",
    "            \n",
    "            # Specify the normalized values\n",
    "            norm_values = ['No data', '0.00', '3.33', '6.67', '10.00'] \n",
    "\n",
    "\n",
    "        # Type 5-4-3-2-1-0\n",
    "        elif cat_scoring_type == 'Type 5-4-3-2-1-0':\n",
    "            # Specify the values the raw data can take\n",
    "            conditions = [\n",
    "                (raw_dataframe[indicator_raw_value] == 0)\n",
    "                (raw_dataframe[indicator_raw_value] == 1),\n",
    "                (raw_dataframe[indicator_raw_value] == 2),\n",
    "                (raw_dataframe[indicator_raw_value] == 3),\n",
    "                (raw_dataframe[indicator_raw_value] == 4),\n",
    "                (raw_dataframe[indicator_raw_value] == 5)\n",
    "                ]\n",
    "            \n",
    "            # Specify the normalized values\n",
    "            norm_values = ['No data', '0.00', '2.50', '5.00', '7.50', '10.00']\n",
    "            \n",
    "        # Type 7-6-5-4-3-2-1-0\n",
    "        elif cat_scoring_type == 'Type 7-6-5-4-3-2-1-0':\n",
    "            # Specify the values the raw data can take\n",
    "            conditions = [\n",
    "                (raw_dataframe[indicator_raw_value] == 0)\n",
    "                (raw_dataframe[indicator_raw_value] == 1),\n",
    "                (raw_dataframe[indicator_raw_value] == 2),\n",
    "                (raw_dataframe[indicator_raw_value] == 3),\n",
    "                (raw_dataframe[indicator_raw_value] == 4),\n",
    "                (raw_dataframe[indicator_raw_value] == 5),\n",
    "                (raw_dataframe[indicator_raw_value] == 6),\n",
    "                (raw_dataframe[indicator_raw_value] == 7)\n",
    "                ]\n",
    "            \n",
    "            # Specify the normalized values\n",
    "            norm_values = ['No data', '0.00', '1.67', '3.33', '5.00', '6.67', '8.33', '10.00'] \n",
    "        \n",
    "        else:\n",
    "            raise('The scoring type you have specified does not exist. Make sure your scoring type is listed in the documentation of this fuction.') \n",
    "        \n",
    "        # create a new column and use np.select to assign values to it using our lists as arguments\n",
    "        raw_dataframe['SCALED'] = np.select(conditions, values)\n",
    "            \n",
    "    else:\n",
    "        # This is the section dealing with numerical indicators\n",
    "\n",
    "        # Determine basic descriptive statistics of the distribution\n",
    "        min_val = min(raw_dataframe[indicator_raw_value].astype('float'))\n",
    "        max_val = max(raw_dataframe[indicator_raw_value].astype('float'))\n",
    "        q1 =  raw_dataframe[indicator_raw_value].astype('float').quantile(q=0.25)\n",
    "        q2 =  raw_dataframe[indicator_raw_value].astype('float').quantile(q=0.50)\n",
    "        q3 =  raw_dataframe[indicator_raw_value].astype('float').quantile(q=0.75)                      \n",
    "        iqr = q3 - q1\n",
    "        \n",
    "        # Define what max value to use for the normalization            \n",
    "        if max_val > q3 + whisker_factor * iqr:\n",
    "            max_to_use = q3 + whisker_factor * iqr\n",
    "            print('The distribution of the raw data values contains outliers or is too skewed on the upper end. The maximum value to be used for the normalisation is the the 3rd quartile added to IQR multiplied by the {}. It is: {} \\n See histogram printed below for info. \\n'.format(whisker_factor, max_to_use))\n",
    "        else:\n",
    "            max_to_use = max_val\n",
    "            print('The distribution of the raw data does not contain outliers on the upper end. The maximum value used for the normalisation is the maximum value in the dataset, which is {}. \\n'.format(max_to_use))\n",
    "                      \n",
    "        # Define what min value to use for the normalization\n",
    "        if min_val < q1 - whisker_factor * iqr:\n",
    "            min_to_use = q1 - whisker_factor * iqr\n",
    "            print('The distribution of the raw data values contains outliers or is too skewed on the lower end. The minimum value to be used for the normalisation is the the 3rd quartile added to IQR multiplied by the the {}. It is: {} \\n See histogram printed below for info. \\n'.format(whisker_factor, min_to_use))\n",
    "        else:\n",
    "            min_to_use = min_val\n",
    "            print('The distribution of the raw data does not contain outliers or is too skewed on the lower end. The minimum value used for the normalisation is the maximum value in the dataset, which is {}. \\n'.format(min_to_use))\n",
    "        \n",
    "        # If there are outliers or a skewed distribution, print the distribution for the user.\n",
    "        if (min_val < q1 - whisker_factor * iqr) or (max_val > q3 + whisker_factor * iqr):\n",
    "            print('\\n This is the distribution of the raw data of the indicator.')\n",
    "            print(raw_dataframe[indicator_raw_value].hist(bins = 30))\n",
    "                       \n",
    "        # Define the value range that is used for the scaling (normalization)\n",
    "        tot_range = max_val - min_val \n",
    "        \n",
    "        # Distinguish between indicators, whose value must be inverted\n",
    "        if inverted == True:\n",
    "            temp = raw_dataframe.assign(SCALED = round(10 - 10 * (raw_dataframe[indicator_raw_value].astype('float') - min_val)/ tot_range, 2))\n",
    "        else:\n",
    "            temp = raw_dataframe.assign(SCALED = round(10 * (raw_dataframe[indicator_raw_value].astype('float') - min_val)/ tot_range, 2))\n",
    "        \n",
    "        # bring dataframe from wide to long format\n",
    "        # Prepare the melting of the dataframe, by defining what columns remain untouched by the melt\n",
    "    kept_columns = [x for x in raw_dataframe.columns.tolist() if x not in [indicator_raw_value, 'SCALED']]\n",
    "\n",
    "    return(\n",
    "        pd.melt(temp,\n",
    "           id_vars = kept_columns,\n",
    "           value_vars = [indicator_raw_value, 'SCALED'],\n",
    "           var_name = 'VALUE_TYPE')\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### UNESCO API specific functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def extract_unesco_api_data(stat_unit, unit, edu_level, subs_key, start_period = 2010, end_period = 2020):\n",
    "    \"\"\"Extract raw data from the UNESCO API\n",
    "\n",
    "    The API can be called with a API key, which you must generate by creating an account. \n",
    "    The calls can be done with simple https requests, in which many parameter must be specified (e.g. indicator code, sexes, age, ...). TO understand the structure, please check the query builder https://apiportal.uis.unesco.org/query-builder\n",
    "\n",
    "    Parameters:\n",
    "    stat_unit (string): Indicatiro code, I suggest you check the query builder https://apiportal.uis.unesco.org/query-builder to retrieve the right code\n",
    "    unit (string): Specify the unnit of measure, e.g. 'PT' for percentage total\n",
    "    edu_level (string): Choose educational level, L2 = lower secondary, L3 = upper secondary age\n",
    "    subs_key (string): Your API key. You must create one. Visit https://apiportal.uis.unesco.org/getting-started for more info\n",
    "    start_period (int): Year which is the beginning of series for which you want to collect data\n",
    "    end_period (ing): Year which is the end of series for which you want to collect data\n",
    "\n",
    "    Returns:\n",
    "    obj: Returns pandas dataframe\n",
    "\n",
    "   \"\"\"\n",
    "    url = 'https://api.uis.unesco.org/sdmx/data/SDG4/{sunit}.{umeasure}.{elevel}._T._T.SCH_AGE_GROUP._T.INST_T._Z._T._Z._Z._Z._T._T._Z._Z._Z.?startPeriod={syear}&endPeriod={eyear}&format=csv-sdmx&subscription-key={skey}'.format(\n",
    "        sunit = stat_unit,\n",
    "        umeasure = unit,\n",
    "        elevel = edu_level,\n",
    "        syear = start_period,\n",
    "        eyear = end_period,\n",
    "        skey = subs_key)\n",
    "    return(pd.read_csv(filepath_or_buffer = url, \n",
    "                      sep = ',',))\n",
    "\n",
    "def cleanse_unesco_api_data(uis_data, country_iso2_list, columns, most_recent_only = True):\n",
    "    \"\"\"< To do >\n",
    "\n",
    "    Parameters:\n",
    "    s15_raw (obj): Should be return of function s55_extract or s56_extract\n",
    "    country_iso2_list (array): A numpy array of 2-character country codes.\n",
    "    columns: List of the dimensions (i.e. columns) that are you want to keep for the indicator beyond the main necessary ones\n",
    "\n",
    "    Returns:\n",
    "    obj: Returns pandas dataframe with the relevant data of the indicator\n",
    "\n",
    "   \"\"\"\n",
    "    # Discard unnecessay columns\n",
    "    columns_kept = ['REF_AREA', 'TIME_PERIOD','OBS_VALUE'] + columns\n",
    "    uis_data = uis_data[columns_kept]\n",
    "    \n",
    "    # Discard rows of countries that are in the master country list\n",
    "    uis_data = uis_data[uis_data.REF_AREA.isin(country_iso2_list)]\n",
    "    \n",
    "    if most_recent_only == True:\n",
    "        # Retrieve the most up-to-date number for each country\n",
    "        uis_data = uis_data[uis_data['TIME_PERIOD']==uis_data.groupby('REF_AREA')['TIME_PERIOD'].transform('max')]\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return(uis_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### SDG API specific functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def extract_sdg_api_data(series_code):\n",
    "    \"\"\"Extract raw data for a series \n",
    "\n",
    "    Extract data on the specifies series as json and flatten it out into a pandas dataframe\n",
    "    To retrieve the data from the API of the SDG indicators, you must proceed as follows: \n",
    "    \n",
    "    * Find the seriesCode of the indicator for which you want to retrieve data: I recommend visiting https://unstats.un.org/sdgs/indicators/database/ and browse the indicator you want and the series code will be indicated there\n",
    "    * Visit https://unstats.un.org/SDGAPI/swagger/#!/Series/V1SdgSeriesDataGet\n",
    "    * Expand the tab GET /v1/sdg/Series/Data\n",
    "    * Type in the seriesCode, this will provide you with the link to the JSON\n",
    "    * Important: The results will be on shwon on various pages if the dataset is too large. That is why it is a good idea to set a pageSize value large enough to accomodate all data in just one page to not have to iterate over various pages\n",
    "\n",
    "        \n",
    "    Parameters:\n",
    "    series (string): Code of the series, which can be found by browsing for the relevant series here:https://unstats.un.org/sdgs/indicators/database?indicator=16.2.2\n",
    "\n",
    "    Returns:\n",
    "    obj: Returns pandas dataframe\n",
    "\n",
    "   \"\"\"\n",
    "    url = 'https://unstats.un.org/SDGAPI/v1/sdg/Series/Data?seriesCode={}&pageSize=999999999'.format(series_code)\n",
    "    return(pd.json_normalize(requests.get(url).json()['data']))    \n",
    "\n",
    "\n",
    "def cleanse_sdg_api_data(raw_data):\n",
    "    \"\"\"Transform raw data from sdg API source to distil relevant data \n",
    "\n",
    "    Discard columns and aggregate data to retrieve the latest value for all countries in a given dataframe for a given SDG API series.\n",
    "\n",
    "    Parameters:\n",
    "    raw_data (obj): Return of function 'extract_sdg_api_data'. Should be a pandas dataframe \n",
    "    \n",
    "    Returns:\n",
    "    obj: Returns pandas dataframe\n",
    "\n",
    "   \"\"\"\n",
    "    # Extract relevant columns\n",
    "    stage_data = raw_data[['geoAreaCode',\n",
    "            'geoAreaName',\n",
    "            'timePeriodStart',\n",
    "            'value']]\n",
    "    # Get the latest value for each country of the series\n",
    "    return(stage_data[stage_data['timePeriodStart'] == stage_data.groupby('geoAreaName')['timePeriodStart'].transform('max')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "    \n",
    "def scaler(raw_dataframe, indicator_value, cat_var = False, inverted = False, up_lim = None, low_lim = None):\n",
    "    \"\"\" Transform raw data into scaled (normalized) data in long format\n",
    "    \n",
    "    Parameters:\n",
    "    raw_dataframe (obj): Raw dataset, pandas dataframe\n",
    "    indicator_value (str): Column containing the actual raw data values \n",
    "    cat_var (bool): is variable of type categorical or numerical?\n",
    "    inverted (bool): If we are dealing with a numerical variable, should the values be inverted?\n",
    "    up_lim_req (float): For numerical variables, is there an artificial upper limit \n",
    "    low_lim_req (bool): For numerical variables, is there an artificial upper limit\n",
    "\n",
    "    Returns:\n",
    "    obj: Returns numpy array with transformed values\n",
    "    \"\"\"\n",
    "    # Check if an indicator is numeric of categorical\n",
    "    if cat_var == True:\n",
    "        pass\n",
    "    else:\n",
    "        # This is the section dealing with numerical indicators\n",
    "        # Define the range of the indicator values (some take their max/min value, sometimes an upper/lower limit)\n",
    "        if up_lim == None:\n",
    "            max_val = max(raw_dataframe[indicator_value].astype('float'))\n",
    "        else:\n",
    "            max_val = up_lim\n",
    "        if low_lim == None:\n",
    "            min_val = min(raw_dataframe[indicator_value].astype('float'))\n",
    "        else:\n",
    "            min_val = low_lim \n",
    "        \n",
    "        # Define the value range that is used for the scaling (normalization)\n",
    "        tot_range = max_val - min_val \n",
    "        \n",
    "        # Distinguish between indicators, whose value must be inverted\n",
    "        if inverted == True:\n",
    "            temp = raw_dataframe.assign(SCALED = round(10 - 10 * (raw_dataframe[indicator_value].astype('float') - min_val)/ tot_range, 2))\n",
    "        else:\n",
    "            temp = raw_dataframe.assign(SCALED = round(10 * (raw_dataframe[indicator_value].astype('float') - min_val)/ tot_range, 2))\n",
    "        \n",
    "        # bring dataframe from wide to long format\n",
    "        # Prepare the melting of the dataframe, by defining what columns remain\n",
    "        kept_columns = [x for x in s55_cleansed.columns.tolist() if x not in [indicator_value, 'SCALED']]\n",
    "        \n",
    "        return(\n",
    "            pd.melt(temp,\n",
    "               id_vars = ['REF_AREA', 'TIME_PERIOD', 'EDU_CAT', 'SEX'],\n",
    "               value_vars = [indicator_value, 'SCALED'],\n",
    "               var_name = 'VALUE_TYPE')\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(s55_final) == len(s55_scaled) + len(country_iso_list) - len(s55_scaled)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def scaler(indicator, cat_var = False, inverted = False, up_lim = None, low_lim = None):\n",
    "    \"\"\" Transform raw data into scaled (normalized) data in long format\n",
    "    \n",
    "    Parameters:\n",
    "    indicator (obj): numpy array containing the indicator data\n",
    "    cat_var (bool): is variable of type categorical or numerical?\n",
    "    inverted (bool): If we are dealing with a numerical variable, should the values be inverted?\n",
    "    up_lim_req (float): For numerical variables, is there an artificial upper limit \n",
    "    low_lim_req (bool): For numerical variables, is there an artificial upper limit\n",
    "\n",
    "    Returns:\n",
    "    obj: Returns numpy array with transformed values\n",
    "    \"\"\"\n",
    "    # Check if an indicator is numeric of categorical\n",
    "    if cat_var == True:\n",
    "        pass\n",
    "    else:\n",
    "        # This is the section dealing with numerical indicators\n",
    "        # Define the range of the indicator values (some take their max/min value, sometimes an upper/lower limit)\n",
    "        if up_lim == None:\n",
    "            max_val = max(indicator.astype('float'))\n",
    "        else:\n",
    "            max_val = up_lim\n",
    "        if low_lim == None:\n",
    "            min_val = min(indicator.astype('float'))\n",
    "        else:\n",
    "            min_val = low_lim \n",
    "        \n",
    "        # Define the value range that is used for the scaling (normalization)\n",
    "        tot_range = max_val - min_val \n",
    "        \n",
    "        # Distinguish between indicators, whose value must be inverted\n",
    "        if inverted == False:\n",
    "            return(round(10 * (indicator.astype('float') - min_val)/ tot_range, 2))\n",
    "        else:\n",
    "            return(round(10 - 10 * (indicator.astype('float') - min_val)/ tot_range, 2))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def transform_unesco_api_data(uis_data, country_code_list):\n",
    "    \"\"\"< To do >\n",
    "\n",
    "    Parameters:\n",
    "    s15_raw (obj): Should be return of function s55_extract or s56_extract\n",
    "    country_code_list (array): A numpy array of 2-character country codes.  \n",
    "\n",
    "    Returns:\n",
    "    obj: Returns pandas dataframe with the relevant data of the indicator\n",
    "\n",
    "   \"\"\"\n",
    "    # Discard unnecessay columns\n",
    "    uis_data = uis_data[['REF_AREA',\n",
    "         'TIME_PERIOD',\n",
    "         'OBS_VALUE']]\n",
    "    \n",
    "    # Discard unnecessary rows\n",
    "    uis_data = uis_data[uis_data.REF_AREA.isin(country_code_list)]\n",
    "    \n",
    "    # Retrieve the most up-to-date number for each country\n",
    "    uis_data = uis_data[uis_data['TIME_PERIOD']==uis_data.groupby('REF_AREA')['TIME_PERIOD'].transform('max')]\n",
    "    \n",
    "    return(uis_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Code to create the country list (which has then been manually modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# This is the list which contains all variations of names a country can have. \n",
    "# NB: CountryDesc is the key, so there are duplicate ISO codes in this list \n",
    "\n",
    "# This is the list of James, which already contains the \"normal\" and the \"long\" name of countries\n",
    "daniele_list_1 = pd.read_csv(filepath_or_buffer = \"D:/Documents/2020/28_UNICEF/9_misc/compare_UNICEF_Transmonee.csv\", \n",
    "                      sep = ',',\n",
    "                      usecols = ['Country', 'Alpha2', 'Alpha3'])\n",
    "daniele_list_1.columns = ['CountryDesc', 'CountryIso2', 'CountryIso3']\n",
    "\n",
    "# This is the list of Daniele \n",
    "daniele_list_2 = pd.read_csv(filepath_or_buffer = \"D:/Documents/2020/28_UNICEF/9_misc/compare_UNICEF_Transmonee.csv\", \n",
    "                      sep = ',',\n",
    "                      usecols = ['CountryDesc', 'CountryIso2', 'CountryIso3'])\n",
    "\n",
    "# This is the list of me\n",
    "my_list = pd.read_excel(\n",
    "    io = \"D:/Documents/2020/28_UNICEF/9_misc/CountryMaster_v3.xlsx\", \n",
    "    )[[#'CountryID',\n",
    "   'CountryDesc',\n",
    "   'CountryIso2',\n",
    "   'CountryIso3']]\n",
    "\n",
    "# Create list with all possible variations of country names\n",
    "country_full_list = daniele_list_1.append(daniele_list_2).append(my_list).drop_duplicates()\n",
    "    \n",
    "# Create list which has countryIso as unique key\n",
    "country_iso_list = country_full_list.drop_duplicates(subset = 'CountryIso2')\n",
    "\n",
    "# Save files \n",
    "country_full_list.to_excel(cwd + 'all_countrynames_list.xlsx')\n",
    "country_full_list.to_csv(cwd + 'all_countrynames_list.csv',\n",
    "                        sep = \";\") # there were a few empty fields for ISO2 and ISO3 which I filled manually. \n",
    "# Furthermore I have deleted the following from the dataset: Channel Islands, Serbia and Montenegro, United nations, Unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# This is the list which contains all variations of names a country can have. \n",
    "# NB: CountryDesc is the key, so there are duplicate ISO codes in this list \n",
    "\n",
    "# This is the list of James, which already contains the \"normal\" and the \"long\" name of countries\n",
    "daniele_list_1 = pd.read_csv(filepath_or_buffer = \"D:/Documents/2020/28_UNICEF/9_misc/compare_UNICEF_Transmonee.csv\", \n",
    "                      sep = ',',\n",
    "                      usecols = ['Country', 'Alpha2', 'Alpha3'])\n",
    "daniele_list_1.columns = ['CountryDesc', 'CountryIso2', 'CountryIso3']\n",
    "\n",
    "# This is the list of Daniele \n",
    "daniele_list_2 = pd.read_csv(filepath_or_buffer = \"D:/Documents/2020/28_UNICEF/9_misc/compare_UNICEF_Transmonee.csv\", \n",
    "                      sep = ',',\n",
    "                      usecols = ['CountryDesc', 'CountryIso2', 'CountryIso3'])\n",
    "\n",
    "# This is the list of me\n",
    "my_list = pd.read_excel(\n",
    "    io = \"D:/Documents/2020/28_UNICEF/9_misc/CountryMaster_v3.xlsx\", \n",
    "    )[[#'CountryID',\n",
    "   'CountryDesc',\n",
    "   'CountryIso2',\n",
    "   'CountryIso3']]\n",
    "\n",
    "# Create list with all possible variations of country names\n",
    "country_full_list = daniele_list_1.append(daniele_list_2).append(my_list).drop_duplicates()\n",
    "    \n",
    "# Create list which has countryIso as unique key\n",
    "country_iso_list = country_full_list.drop_duplicates(subset = 'CountryIso2')\n",
    "\n",
    "# Save files \n",
    "country_full_list.to_excel(cwd + 'all_countrynames_list.xlsx')\n",
    "country_full_list.to_csv(cwd + 'all_countrynames_list.csv',\n",
    "                        sep = \";\") # there were a few empty fields for ISO2 and ISO3 which I filled manually. \n",
    "# Furthermore I have deleted the following from the dataset: Channel Islands, Serbia and Montenegro, United nations, Unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "country_full_list = pd.read_excel(\n",
    "    io = \"D:/Documents/2020/28_UNICEF/9_misc/CountryMaster_v3.xlsx\", \n",
    "    )[[#'CountryID',\n",
    "   'CountryDesc',\n",
    "   'CountryIso2',\n",
    "   'CountryIso3']].drop_duplicates()\n",
    "\n",
    "country_iso_list = country_full_list.drop_duplicates(subset = 'CountryIso2')\n",
    "\n",
    "list_Daniele = \n",
    "\n",
    "temp = pd.read_csv(filepath_or_buffer = \"D:/Documents/2020/28_UNICEF/9_misc/compare_UNICEF_Transmonee.csv\", \n",
    "                      sep = ',',\n",
    "                      usecols = ['CountryDesc', 'CountryIso2', 'CountryIso3'])\n",
    "list_Daniele.columns = ['CountryDesc', 'CountryIso2', 'CountryIso3']\n",
    "\n",
    "list_Daniele = list_Daniele.append(temp)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Load the master country-reference lit\n",
    "'''\n",
    "url = 'https://pkgstore.datahub.io/core/country-list/data_csv/data/d7c9d7cfb42cb69f4422dec222dbbaa8/data_csv.csv'\n",
    "\n",
    "country_list = pd.read_csv(\n",
    "    filepath_or_buffer = url, \n",
    "    sep = ',',\n",
    "    )\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Reproducing Maplecroft transformation (dev section)\n",
    "\n",
    "Using indicator 3.1.1. as a first example (quantitative variable): Child labour rate (5-17) \n",
    "\n",
    "The maplecroft sheet has each indicator undergo a set of tranformations: \n",
    "\n",
    "1. Indicator raw data = basis\n",
    "2. Normalize indicator raw data with certain transformation (e.g. 0.3.1.1. becomes 0.3.1.1_s)\n",
    "3. Aggregate various normalized indicator data into a cluster value (e.g. 0.3.1.1 and 0.3.1.2. become 0.3_s)\n",
    "\n",
    "In the following I am reproducing these steps, each of which contains many different nested if-then statements\n",
    "\n",
    "## 1 Indicator raw data\n",
    "\n",
    "s. above\n",
    "\n",
    "## 2 Normalize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "min_val = min(s24_transformed.value.astype('float'))\n",
    "max_val = max(s24_transformed.value.astype('float'))\n",
    "tot_range = max_val - min_val \n",
    "\n",
    "if indicator = \n",
    "\n",
    "# Do the actual normalization\n",
    "s24_transformed.assign(norm_val = (s24_transformed.value.astype('float') - min_val)/ tot_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "These if-statements must be included: \n",
    "\n",
    "* If categorical --> Something I still have to figure out\n",
    "    * Else: If upper limit required --> take upper limit value.\n",
    "        * Else: Take max value of series. And then: \n",
    "            * If raw data value > max to use\n",
    "            \n",
    "* Categorical vs numerical\n",
    "    * Inverted\n",
    "        * Is raw data point (for a country) available? --> \n",
    "            * Max to use as upper limit or max value\n",
    "                * Min to use as lower limit of min value\n",
    "                    *  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "sklearn.preprocessing.MinMaxScaler().fit_transform(s24_transformed.value)\n",
    "\n",
    "\n",
    "\n",
    "# Create x, where x the 'scores' column's values as floats\n",
    "x = s24_transformed.value.astype(float)\n",
    "\n",
    "temp = x.reshape(1, -1)\n",
    "\n",
    "# Create a minimum and maximum processor object\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "# Create an object to transform the data to fit minmax processor\n",
    "x_scaled = min_max_scaler.fit_transform(temp)\n",
    "\n",
    "# Run the normalizer on the dataframe\n",
    "df_normalized = pd.DataFrame(x_scaled)\n",
    "\n",
    "\n",
    "# s56_transformed.head()\n",
    "\n",
    "# The maplecroft sheet has each indicator undergo the following tran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Extract the relevant columns\n",
    "temp = s24_raw[['geoAreaCode',\n",
    "            'geoAreaName',\n",
    "            'timePeriodStart',\n",
    "            'value']]\n",
    "\n",
    "# Extract the latest value for each country\n",
    "temp[temp['timePeriodStart'] == temp.groupby('geoAreaName')['timePeriodStart'].transform('max')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Transform the raw data\n",
    "s61_transformed = transform_sdg_api_data(raw_data = s61_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Extract data and directly flatten it out into a pandas dataframe\n",
    "s24_raw = extract_sdg_api_data(series = 'VC_HTF_DETV')\n",
    "\n",
    "# Save data to raw data folder\n",
    "save_raw_data(dataframe = s24_raw,\n",
    "             filename = 'S_24.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Extract data and directly flatten it out into a pandas dataframe\n",
    "url = 'https://unstats.un.org/SDGAPI/v1/sdg/Series/Data?seriesCode=VC_HTF_DETV&pageSize=999999999' # choose page size large enough to make sure data all fits in one page\n",
    "s24_raw = pd.json_normalize(requests.get(url).json()['data'])\n",
    "\n",
    "# Save data to raw data folder\n",
    "save_raw_data(dataframe = s24_raw,\n",
    "             filename = 'S_24.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Extract raw data\n",
    "s56_raw = s56_extract(subs_key = '460ab272abdd43c892bb59c218c22c09',\n",
    "                      start_period = '2010',\n",
    "                      end_period = '2020')\n",
    "\n",
    "# Save raw data\n",
    "save_raw_data(dataframe = s56_raw,\n",
    "             filename = 'S_56.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def s56_extract(subs_key, start_period = 2010, end_period = 2020):\n",
    "    \"\"\"Extract raw data of source S-15 from the UNESCO api UIS\n",
    "\n",
    "    Parameters:\n",
    "    subs_key (string): Your API key. You must create one. Visit https://apiportal.uis.unesco.org/getting-started for more info\n",
    "    start_period (int): Year which is the beginning of series for which you want to collect data\n",
    "    end_period (ing): Year which is the end of series for which you want to collect data\n",
    "\n",
    "    Returns:\n",
    "    obj: Returns pandas dataframe and saves raw data to specified output_path\n",
    "\n",
    "   \"\"\"\n",
    "    url = 'https://api.uis.unesco.org/sdmx/data/SDG4/ROFST.PT.L3._T._T.SCH_AGE_GROUP._T.INST_T._Z._T._Z._Z._Z._T._T._Z._Z._Z.?startPeriod={syear}&endPeriod={eyear}&format=csv-sdmx&subscription-key={skey}'.format(syear = start_period,\n",
    "        eyear = end_period,\n",
    "        skey = subs_key)\n",
    "    return(pd.read_csv(filepath_or_buffer = url, \n",
    "                      sep = ',',))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Extract data\n",
    "s55_raw = s55_extract(subs_key = '460ab272abdd43c892bb59c218c22c09',\n",
    "                      start_period = '2010',\n",
    "                      end_period = '2020')\n",
    "\n",
    "# Save raw data\n",
    "save_raw_data(dataframe = s55_raw,\n",
    "             filename = 'S_55.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def s55_extract(subs_key, start_period = 2010, end_period = 2020):\n",
    "    \"\"\"Extract raw data from the UNESCO API\n",
    "\n",
    "    The API can be called with a API key, which you must generate by creating an account. \n",
    "    The calls can be done with simple https requests, in which many parameter must be specified (e.g. indicator code, sexes, age, ...). TO understand the structure, please check the query builder https://apiportal.uis.unesco.org/query-builder\n",
    "\n",
    "    Parameters:\n",
    "    subs_key (string): Your API key. You must create one. Visit https://apiportal.uis.unesco.org/getting-started for more info\n",
    "    start_period (int): Year which is the beginning of series for which you want to collect data\n",
    "    end_period (ing): Year which is the end of series for which you want to collect data\n",
    "\n",
    "    Returns:\n",
    "    obj: Returns pandas dataframe and saves raw data to specified output_path\n",
    "\n",
    "   \"\"\"\n",
    "    url = 'https://api.uis.unesco.org/sdmx/data/SDG4/ROFST.PT.L2._T._T.SCH_AGE_GROUP._T.INST_T._Z._T._Z._Z._Z._T._T._Z._Z._Z.?startPeriod={syear}&endPeriod={eyear}&format=csv-sdmx&subscription-key={skey}'.format(syear = start_period,\n",
    "        eyear = end_period,\n",
    "        skey = subs_key)\n",
    "    return(pd.read_csv(filepath_or_buffer = url, \n",
    "                      sep = ',',))\n",
    "\n",
    "https://api.uis.unesco.org/sdmx/dataflow/all/all/latest?subscription-key=<your api key>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Extract the relevant columns\n",
    "temp = s61_raw[['geoAreaCode',\n",
    "            'geoAreaName',\n",
    "            'timePeriodStart',\n",
    "            'value']]\n",
    "\n",
    "# Extract the latest value for each country\n",
    "temp = temp[temp['timePeriodStart'] == temp.groupby('geoAreaName')['timePeriodStart'].transform('max')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Extract the relevant columns\n",
    "temp = temp[['geoAreaCode',\n",
    "            'geoAreaName',\n",
    "            'timePeriodStart',\n",
    "            'value']]\n",
    "\n",
    "# Extract the latest value for each country\n",
    "temp = temp[temp['timePeriodStart'] == temp.groupby('geoAreaName')['timePeriodStart'].transform('max')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Transforming ILO data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Filter out rows to get both sexes\n",
    "temp  = s23_raw_ilo[s23_raw_ilo['SEX'] == 'SEX_T']\n",
    "\n",
    "# Extract the relevant columns\n",
    "temp = temp[['REF_AREA',\n",
    "            'TIME_PERIOD',\n",
    "            'OBS_VALUE']]\n",
    "\n",
    "# Extract the latest value for each country\n",
    "temp = temp[temp['TIME_PERIOD'] == temp.groupby('REF_AREA')['TIME_PERIOD'].transform('max')]\n",
    "\n",
    "# Save file to examine it\n",
    "temp.to_excel(cwd + \"\\data_transformed\\S_23_SDG_ILO.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Extracting data through ILO API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Extract data from ILO API. The link is generated from this website: https://ilostat.ilo.org/data/sdmx-query-builder/ . \n",
    "# The code of the indicator I have from here: \n",
    "url = 'https://www.ilo.org/sdmx/rest/data/ILO,DF_SDG_ALL_SDG_A831_SEX_RT/?format=csv&startPeriod=2010-01-01&endPeriod=2020-12-31'\n",
    "\n",
    "s23_raw_ilo = pd.read_csv(filepath_or_buffer = url, \n",
    "                      sep = ',',)\n",
    "\n",
    "# Save data to raw data folder\n",
    "s23_raw_ilo.to_excel(dataframe = s23_raw,\n",
    "             filename = 'S_23.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def s56_save_raw(s56_data, output_path = data_sources_raw + '\\S_56.xlsx'):\n",
    "    \"\"\"Save raw data of source s56 as .xlsx file\n",
    "\n",
    "    Parameters:\n",
    "    s15_data (obj): Raw data to be be saved. Must be ob class pandas.DataFrame\n",
    "    output_path (string): Output path of where to save the raw data if save_raw is set to true\n",
    "\n",
    "    Returns:\n",
    "    obj: Returns pandas dataframe and saves raw data to specified output_path\n",
    "\n",
    "   \"\"\"\n",
    "    s56_data.to_excel(output_path)\n",
    "    print('The raw data has been saved as .xlsx file in: ' + output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def s55_save_raw(s55_data, output_path = data_sources_raw + '\\S_55.xlsx'):\n",
    "    \"\"\"Save raw data of source s55 as .xlsx file\n",
    "\n",
    "    Parameters:\n",
    "    s15_data (obj): Raw data to be be saved. Must be ob class pandas.DataFrame\n",
    "    output_path (string): Output path of where to save the raw data if save_raw is set to true\n",
    "\n",
    "    Returns:\n",
    "    obj: Returns pandas dataframe and saves raw data to specified output_path\n",
    "\n",
    "   \"\"\"\n",
    "    s55_data.to_excel(output_path)\n",
    "    print('The raw data has been saved as .xlsx file in: ' + output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Retrieve the most up-to-date number for each country\n",
    "s16_data[s16_data['TIME_PERIOD']==s16_data.groupby('REF_AREA')['TIME_PERIOD'].transform('max')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Discard unnecessay columns\n",
    "s16_data = s16_data[['REF_AREA',\n",
    "         'TIME_PERIOD',\n",
    "         'OBS_VALUE']]\n",
    "\n",
    "# Discard unnecessary rows\n",
    "s16_data = s16_data[s16_data.REF_AREA.isin(counry_list.Code)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Define parameters of the API query\n",
    "subs_key = '460ab272abdd43c892bb59c218c22c09' # must generate an KEY on the UIS page\n",
    "start_period = '2010' # define from which year you wan to retrieve data \n",
    "end_period = '2020' # define until what you want to retreive data\n",
    "\n",
    "# Build URL query based on parameters defined\n",
    "url = 'https://api.uis.unesco.org/sdmx/data/SDG4/ROFST.PT.L3._T._T.SCH_AGE_GROUP._T.INST_T._Z._T._Z._Z._Z._T._T._Z._Z._Z.?startPeriod={syear}&endPeriod={eyear}&format=csv-sdmx&subscription-key={skey}'.format(syear = start_period,\n",
    "eyear = end_period,\n",
    "skey = subs_key)\n",
    "\n",
    "# Extract the data\n",
    "s16_data = pd.read_csv(filepath_or_buffer = url, \n",
    "                      sep = ',',\n",
    "                      )\n",
    "\n",
    "# Save data in raw version as excel file # TO do: Create a versioning\n",
    "s16_data.to_excel(data_sources_raw + '\\S_16.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Retrieve the most up-to-date number for each country\n",
    "s15_data[s15_data['TIME_PERIOD']==s15_data.groupby('REF_AREA')['TIME_PERIOD'].transform('max')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Discard unnecessay columns\n",
    "s15_data = s15_data[['REF_AREA',\n",
    "         'TIME_PERIOD',\n",
    "         'OBS_VALUE']]\n",
    "\n",
    "# Discard unnecessary rows\n",
    "s15_data = s15_data[s15_data.REF_AREA.isin(counry_list.Code)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Define parameters of the API query\n",
    "subs_key = '460ab272abdd43c892bb59c218c22c09' # must generate an KEY on the UIS page\n",
    "start_period = '2010' # define from which year you wan to retrieve data \n",
    "end_period = '2020' # define until what you want to retreive data\n",
    "\n",
    "# Build URL query based on parameters defined\n",
    "url = 'https://api.uis.unesco.org/sdmx/data/SDG4/ROFST.PT.L2._T._T.SCH_AGE_GROUP._T.INST_T._Z._T._Z._Z._Z._T._T._Z._Z._Z.?startPeriod={syear}&endPeriod={eyear}&format=csv-sdmx&subscription-key={skey}'.format(syear = start_period,\n",
    "eyear = end_period,\n",
    "skey = subs_key)\n",
    "\n",
    "# Extract the data\n",
    "s15_data = pd.read_csv(filepath_or_buffer = url, \n",
    "                      sep = ',',\n",
    "                      )\n",
    "\n",
    "# Save data in raw version as excel file # TO do: Create a versioning\n",
    "s15_data.to_excel(data_sources_raw + '\\S_15.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s15_data.REF_AREA.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Checking to see if ILO data can be retrieved from their SDMX data warehouse\n",
    "url = \"https://www.ilo.org/sdmx/rest/data/ILO,DF_YI_ALL_IFL_4IEM_SEX_ECO_IFL_RT/?format=jsondata&startPeriod=2010-01-01&endPeriod=2020-12-31\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "my_json = json.loads(response.text)\n",
    "\n",
    "type(my_json)\n",
    "\n",
    "# response = json.loads(response.text)\n",
    "\n",
    "print(my_json)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Check out the UN SDG Goals API\n",
    "\n",
    "url = \"https://unstats.un.org/SDGAPI/v1/sdg/Indicator/VC_HTF_DETV%20/Series/List\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "my_json = json.loads(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# S-11\n",
    "url = 'https://outoftheshadows.eiu.com/wp-content/uploads/2019/05/OOSI_Out_of_the_shadows_index_60-countries_May2019.xlsm'\n",
    "\n",
    "data = pd.read_excel(url,\n",
    "                    sheet_name = 'Dataset',\n",
    "                    skiprows = 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# s15_data.groupby(['REF_AREA'], sort = False)[['TIME_PERIOD', 'OBS_VALUE']].max()\n",
    "# s15_data.groupby(['REF_AREA'], sort = False).max()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "274.633px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
