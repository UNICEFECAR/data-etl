{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Data-extraction-CRBA\" data-toc-modified-id=\"Data-extraction-CRBA-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Data extraction CRBA</a></span><ul class=\"toc-item\"><li><span><a href=\"#Backlog-of-tasks\" data-toc-modified-id=\"Backlog-of-tasks-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Backlog of tasks</a></span></li><li><span><a href=\"#Import-packages\" data-toc-modified-id=\"Import-packages-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Import packages</a></span></li><li><span><a href=\"#Import-country-list\" data-toc-modified-id=\"Import-country-list-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Import country list</a></span></li></ul></li><li><span><a href=\"#UNESCO-Sources:-Data-extraction---cleansing---normalization\" data-toc-modified-id=\"UNESCO-Sources:-Data-extraction---cleansing---normalization-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>UNESCO Sources: Data extraction - cleansing - normalization</a></span><ul class=\"toc-item\"><li><span><a href=\"#S-55-(prev-S-15)-Percentage-of-out-of-school-adolescents-of-lower-secondary-school-age.\" data-toc-modified-id=\"S-55-(prev-S-15)-Percentage-of-out-of-school-adolescents-of-lower-secondary-school-age.-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>S-55 (prev S-15) Percentage of out-of-school adolescents of lower secondary school age.</a></span><ul class=\"toc-item\"><li><span><a href=\"#Extraction\" data-toc-modified-id=\"Extraction-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Extraction</a></span></li><li><span><a href=\"#Cleansing\" data-toc-modified-id=\"Cleansing-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Cleansing</a></span></li><li><span><a href=\"#Normalization-(scaling)\" data-toc-modified-id=\"Normalization-(scaling)-2.1.3\"><span class=\"toc-item-num\">2.1.3&nbsp;&nbsp;</span>Normalization (scaling)</a></span></li><li><span><a href=\"#Select-only-those-rows/-columns-which-are-relevant\" data-toc-modified-id=\"Select-only-those-rows/-columns-which-are-relevant-2.1.4\"><span class=\"toc-item-num\">2.1.4&nbsp;&nbsp;</span>Select only those rows/ columns which are relevant</a></span></li></ul></li><li><span><a href=\"#S-56-(prev-S-16)-Percentage-of-out-of-school-youth-of-upper-secondary-school-age\" data-toc-modified-id=\"S-56-(prev-S-16)-Percentage-of-out-of-school-youth-of-upper-secondary-school-age-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>S-56 (prev S-16) Percentage of out-of-school youth of upper secondary school age</a></span><ul class=\"toc-item\"><li><span><a href=\"#Extraction\" data-toc-modified-id=\"Extraction-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Extraction</a></span></li><li><span><a href=\"#Cleansing\" data-toc-modified-id=\"Cleansing-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>Cleansing</a></span></li><li><span><a href=\"#Normalization-(scaling)\" data-toc-modified-id=\"Normalization-(scaling)-2.2.3\"><span class=\"toc-item-num\">2.2.3&nbsp;&nbsp;</span>Normalization (scaling)</a></span></li><li><span><a href=\"#Select-only-those-rows/-columns-which-are-relevant\" data-toc-modified-id=\"Select-only-those-rows/-columns-which-are-relevant-2.2.4\"><span class=\"toc-item-num\">2.2.4&nbsp;&nbsp;</span>Select only those rows/ columns which are relevant</a></span></li></ul></li><li><span><a href=\"#S-52-Gross-early-childhood-education-enrolment-ratio-in-(a)-pre-primary-education-and-(b)-early-childhood-educational-development-(SDG-Indicator-4.2.4)\" data-toc-modified-id=\"S-52-Gross-early-childhood-education-enrolment-ratio-in-(a)-pre-primary-education-and-(b)-early-childhood-educational-development-(SDG-Indicator-4.2.4)-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>S-52 Gross early childhood education enrolment ratio in (a) pre-primary education and (b) early childhood educational development (SDG Indicator 4.2.4)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Extraction\" data-toc-modified-id=\"Extraction-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>Extraction</a></span></li><li><span><a href=\"#Cleansing\" data-toc-modified-id=\"Cleansing-2.3.2\"><span class=\"toc-item-num\">2.3.2&nbsp;&nbsp;</span>Cleansing</a></span></li><li><span><a href=\"#Normalization-(scaling)\" data-toc-modified-id=\"Normalization-(scaling)-2.3.3\"><span class=\"toc-item-num\">2.3.3&nbsp;&nbsp;</span>Normalization (scaling)</a></span></li><li><span><a href=\"#Select-only-those-rows/-columns-which-are-relevant\" data-toc-modified-id=\"Select-only-those-rows/-columns-which-are-relevant-2.3.4\"><span class=\"toc-item-num\">2.3.4&nbsp;&nbsp;</span>Select only those rows/ columns which are relevant</a></span></li></ul></li></ul></li><li><span><a href=\"#SDG-API-Sources:-Data-extraction---cleansing---normalization\" data-toc-modified-id=\"SDG-API-Sources:-Data-extraction---cleansing---normalization-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>SDG API Sources: Data extraction - cleansing - normalization</a></span><ul class=\"toc-item\"><li><span><a href=\"#S-24-(prev.-S-14)-SDG-Indicator-8.7.1-Proportion-of-children-aged-5-17-years-engaged-in-child-labour-SL_TLF_CHLDEA\" data-toc-modified-id=\"S-24-(prev.-S-14)-SDG-Indicator-8.7.1-Proportion-of-children-aged-5-17-years-engaged-in-child-labour-SL_TLF_CHLDEA-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>S-24 (prev. S-14) SDG Indicator 8.7.1 Proportion of children aged 5-17 years engaged in child labour SL_TLF_CHLDEA</a></span><ul class=\"toc-item\"><li><span><a href=\"#Extraction\" data-toc-modified-id=\"Extraction-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Extraction</a></span></li><li><span><a href=\"#Cleansing\" data-toc-modified-id=\"Cleansing-3.1.2\"><span class=\"toc-item-num\">3.1.2&nbsp;&nbsp;</span>Cleansing</a></span></li><li><span><a href=\"#Normalization-(scaling)\" data-toc-modified-id=\"Normalization-(scaling)-3.1.3\"><span class=\"toc-item-num\">3.1.3&nbsp;&nbsp;</span>Normalization (scaling)</a></span></li><li><span><a href=\"#Select-only-those-rows/-columns-which-are-relevant\" data-toc-modified-id=\"Select-only-those-rows/-columns-which-are-relevant-3.1.4\"><span class=\"toc-item-num\">3.1.4&nbsp;&nbsp;</span>Select only those rows/ columns which are relevant</a></span></li></ul></li><li><span><a href=\"#S-23-(prev.-S-17)-ILO-STAT-Informal-Employment-(%-of-total-non-agricultural-employment)-SL_ISV_IFEM\" data-toc-modified-id=\"S-23-(prev.-S-17)-ILO-STAT-Informal-Employment-(%-of-total-non-agricultural-employment)-SL_ISV_IFEM-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>S-23 (prev. S-17) ILO STAT Informal Employment (% of total non-agricultural employment) SL_ISV_IFEM</a></span><ul class=\"toc-item\"><li><span><a href=\"#Extraction\" data-toc-modified-id=\"Extraction-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Extraction</a></span></li><li><span><a href=\"#Cleansing\" data-toc-modified-id=\"Cleansing-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>Cleansing</a></span></li><li><span><a href=\"#Normalization-(scaling)\" data-toc-modified-id=\"Normalization-(scaling)-3.2.3\"><span class=\"toc-item-num\">3.2.3&nbsp;&nbsp;</span>Normalization (scaling)</a></span></li><li><span><a href=\"#Select-only-those-rows/-columns-which-are-relevant\" data-toc-modified-id=\"Select-only-those-rows/-columns-which-are-relevant-3.2.4\"><span class=\"toc-item-num\">3.2.4&nbsp;&nbsp;</span>Select only those rows/ columns which are relevant</a></span></li></ul></li><li><span><a href=\"#S-61-(prev-S-19)-SDG-Indicator-16.2.2-Detected-victims-of-human-trafficking,-by-age-and-sex-(number)--VC_HTF_DETV\" data-toc-modified-id=\"S-61-(prev-S-19)-SDG-Indicator-16.2.2-Detected-victims-of-human-trafficking,-by-age-and-sex-(number)--VC_HTF_DETV-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>S-61 (prev S-19) SDG Indicator 16.2.2 Detected victims of human trafficking, by age and sex (number)  VC_HTF_DETV</a></span><ul class=\"toc-item\"><li><span><a href=\"#Extraction\" data-toc-modified-id=\"Extraction-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>Extraction</a></span></li><li><span><a href=\"#Cleanse\" data-toc-modified-id=\"Cleanse-3.3.2\"><span class=\"toc-item-num\">3.3.2&nbsp;&nbsp;</span>Cleanse</a></span></li><li><span><a href=\"#Normalization-(scaling)\" data-toc-modified-id=\"Normalization-(scaling)-3.3.3\"><span class=\"toc-item-num\">3.3.3&nbsp;&nbsp;</span>Normalization (scaling)</a></span></li><li><span><a href=\"#Select-only-those-rows/-columns-which-are-relevant\" data-toc-modified-id=\"Select-only-those-rows/-columns-which-are-relevant-3.3.4\"><span class=\"toc-item-num\">3.3.4&nbsp;&nbsp;</span>Select only those rows/ columns which are relevant</a></span></li></ul></li><li><span><a href=\"#S-62-(prev-S-20)-SDG-Indicator-1.1.1.-Proportion-of-population-below-international-poverty-line-(%)-SI_POV_DAY1\" data-toc-modified-id=\"S-62-(prev-S-20)-SDG-Indicator-1.1.1.-Proportion-of-population-below-international-poverty-line-(%)-SI_POV_DAY1-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>S-62 (prev S-20) SDG Indicator 1.1.1. Proportion of population below international poverty line (%) SI_POV_DAY1</a></span><ul class=\"toc-item\"><li><span><a href=\"#Extraction\" data-toc-modified-id=\"Extraction-3.4.1\"><span class=\"toc-item-num\">3.4.1&nbsp;&nbsp;</span>Extraction</a></span></li><li><span><a href=\"#Cleanse\" data-toc-modified-id=\"Cleanse-3.4.2\"><span class=\"toc-item-num\">3.4.2&nbsp;&nbsp;</span>Cleanse</a></span></li><li><span><a href=\"#Normalization-(scaling)\" data-toc-modified-id=\"Normalization-(scaling)-3.4.3\"><span class=\"toc-item-num\">3.4.3&nbsp;&nbsp;</span>Normalization (scaling)</a></span></li><li><span><a href=\"#Select-only-those-rows/-columns-which-are-relevant\" data-toc-modified-id=\"Select-only-those-rows/-columns-which-are-relevant-3.4.4\"><span class=\"toc-item-num\">3.4.4&nbsp;&nbsp;</span>Select only those rows/ columns which are relevant</a></span></li></ul></li><li><span><a href=\"#S-203-(prev-S-47)-SDG-Indicator-8.5.1.-Average-hourly-earnings-of-employees-by-sex-and-occupation-(local-currency)-SL_EMP_AEARN\" data-toc-modified-id=\"S-203-(prev-S-47)-SDG-Indicator-8.5.1.-Average-hourly-earnings-of-employees-by-sex-and-occupation-(local-currency)-SL_EMP_AEARN-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>S-203 (prev S-47) SDG Indicator 8.5.1. Average hourly earnings of employees by sex and occupation (local currency) SL_EMP_AEARN</a></span><ul class=\"toc-item\"><li><span><a href=\"#Extraction\" data-toc-modified-id=\"Extraction-3.5.1\"><span class=\"toc-item-num\">3.5.1&nbsp;&nbsp;</span>Extraction</a></span></li><li><span><a href=\"#Cleanse\" data-toc-modified-id=\"Cleanse-3.5.2\"><span class=\"toc-item-num\">3.5.2&nbsp;&nbsp;</span>Cleanse</a></span></li><li><span><a href=\"#Normalization-(scaling)\" data-toc-modified-id=\"Normalization-(scaling)-3.5.3\"><span class=\"toc-item-num\">3.5.3&nbsp;&nbsp;</span>Normalization (scaling)</a></span></li><li><span><a href=\"#Select-only-those-rows/-columns-which-are-relevant\" data-toc-modified-id=\"Select-only-those-rows/-columns-which-are-relevant-3.5.4\"><span class=\"toc-item-num\">3.5.4&nbsp;&nbsp;</span>Select only those rows/ columns which are relevant</a></span></li></ul></li><li><span><a href=\"#S-204-(prev-S-48)-SDG-Indicator-1.1.1,-ILO-Stat:-Employed-population-below-international-poverty-line,-by-sex-and-age-(%)-SI_POV_EMP1\" data-toc-modified-id=\"S-204-(prev-S-48)-SDG-Indicator-1.1.1,-ILO-Stat:-Employed-population-below-international-poverty-line,-by-sex-and-age-(%)-SI_POV_EMP1-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>S-204 (prev S-48) SDG Indicator 1.1.1, ILO Stat: Employed population below international poverty line, by sex and age (%) SI_POV_EMP1</a></span><ul class=\"toc-item\"><li><span><a href=\"#Extraction\" data-toc-modified-id=\"Extraction-3.6.1\"><span class=\"toc-item-num\">3.6.1&nbsp;&nbsp;</span>Extraction</a></span></li><li><span><a href=\"#Cleanse\" data-toc-modified-id=\"Cleanse-3.6.2\"><span class=\"toc-item-num\">3.6.2&nbsp;&nbsp;</span>Cleanse</a></span></li><li><span><a href=\"#Normalization-(scaling)\" data-toc-modified-id=\"Normalization-(scaling)-3.6.3\"><span class=\"toc-item-num\">3.6.3&nbsp;&nbsp;</span>Normalization (scaling)</a></span></li></ul></li><li><span><a href=\"#S-71-SDG-Indicator-1.3.1-Proportion-of-mothers-with-newborns-receiving-maternity-cash-benefit.-SI_COV_MATNL\" data-toc-modified-id=\"S-71-SDG-Indicator-1.3.1-Proportion-of-mothers-with-newborns-receiving-maternity-cash-benefit.-SI_COV_MATNL-3.7\"><span class=\"toc-item-num\">3.7&nbsp;&nbsp;</span>S-71 SDG Indicator 1.3.1 Proportion of mothers with newborns receiving maternity cash benefit. SI_COV_MATNL</a></span><ul class=\"toc-item\"><li><span><a href=\"#Extraction\" data-toc-modified-id=\"Extraction-3.7.1\"><span class=\"toc-item-num\">3.7.1&nbsp;&nbsp;</span>Extraction</a></span></li><li><span><a href=\"#Cleanse\" data-toc-modified-id=\"Cleanse-3.7.2\"><span class=\"toc-item-num\">3.7.2&nbsp;&nbsp;</span>Cleanse</a></span></li><li><span><a href=\"#Normalization-(scaling)\" data-toc-modified-id=\"Normalization-(scaling)-3.7.3\"><span class=\"toc-item-num\">3.7.3&nbsp;&nbsp;</span>Normalization (scaling)</a></span></li><li><span><a href=\"#Select-only-those-rows/-columns-which-are-relevant\" data-toc-modified-id=\"Select-only-those-rows/-columns-which-are-relevant-3.7.4\"><span class=\"toc-item-num\">3.7.4&nbsp;&nbsp;</span>Select only those rows/ columns which are relevant</a></span></li></ul></li><li><span><a href=\"#S-78-SDG-Indicator-1.3.1-Proportion-of-population-covered-by-social-insurance-programs-SI_COV_SOCINS\" data-toc-modified-id=\"S-78-SDG-Indicator-1.3.1-Proportion-of-population-covered-by-social-insurance-programs-SI_COV_SOCINS-3.8\"><span class=\"toc-item-num\">3.8&nbsp;&nbsp;</span>S-78 SDG Indicator 1.3.1 Proportion of population covered by social insurance programs SI_COV_SOCINS</a></span><ul class=\"toc-item\"><li><span><a href=\"#Extraction\" data-toc-modified-id=\"Extraction-3.8.1\"><span class=\"toc-item-num\">3.8.1&nbsp;&nbsp;</span>Extraction</a></span></li><li><span><a href=\"#Cleanse\" data-toc-modified-id=\"Cleanse-3.8.2\"><span class=\"toc-item-num\">3.8.2&nbsp;&nbsp;</span>Cleanse</a></span></li><li><span><a href=\"#Normalization-(scaling)\" data-toc-modified-id=\"Normalization-(scaling)-3.8.3\"><span class=\"toc-item-num\">3.8.3&nbsp;&nbsp;</span>Normalization (scaling)</a></span></li><li><span><a href=\"#Select-only-those-rows/-columns-which-are-relevant\" data-toc-modified-id=\"Select-only-those-rows/-columns-which-are-relevant-3.8.4\"><span class=\"toc-item-num\">3.8.4&nbsp;&nbsp;</span>Select only those rows/ columns which are relevant</a></span></li></ul></li><li><span><a href=\"#S-80-SDG-Indicator-1.3.1-Proportion-of-population-covered-by-labour-market-programmes-SI_COV_LMKT\" data-toc-modified-id=\"S-80-SDG-Indicator-1.3.1-Proportion-of-population-covered-by-labour-market-programmes-SI_COV_LMKT-3.9\"><span class=\"toc-item-num\">3.9&nbsp;&nbsp;</span>S-80 SDG Indicator 1.3.1 Proportion of population covered by labour market programmes SI_COV_LMKT</a></span><ul class=\"toc-item\"><li><span><a href=\"#Extraction\" data-toc-modified-id=\"Extraction-3.9.1\"><span class=\"toc-item-num\">3.9.1&nbsp;&nbsp;</span>Extraction</a></span></li><li><span><a href=\"#Cleanse\" data-toc-modified-id=\"Cleanse-3.9.2\"><span class=\"toc-item-num\">3.9.2&nbsp;&nbsp;</span>Cleanse</a></span></li><li><span><a href=\"#Normalization-(scaling)\" data-toc-modified-id=\"Normalization-(scaling)-3.9.3\"><span class=\"toc-item-num\">3.9.3&nbsp;&nbsp;</span>Normalization (scaling)</a></span></li><li><span><a href=\"#Select-only-those-rows/-columns-which-are-relevant\" data-toc-modified-id=\"Select-only-those-rows/-columns-which-are-relevant-3.9.4\"><span class=\"toc-item-num\">3.9.4&nbsp;&nbsp;</span>Select only those rows/ columns which are relevant</a></span></li></ul></li></ul></li><li><span><a href=\"#ILO-API-Sources:-Data-extraction---cleansing---normalization\" data-toc-modified-id=\"ILO-API-Sources:-Data-extraction---cleansing---normalization-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>ILO API Sources: Data extraction - cleansing - normalization</a></span><ul class=\"toc-item\"><li><span><a href=\"#S-50-Gender-wage-gap-(NEW)\" data-toc-modified-id=\"S-50-Gender-wage-gap-(NEW)-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>S-50 Gender wage gap (NEW)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Extraction\" data-toc-modified-id=\"Extraction-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>Extraction</a></span></li><li><span><a href=\"#Cleansing\" data-toc-modified-id=\"Cleansing-4.1.2\"><span class=\"toc-item-num\">4.1.2&nbsp;&nbsp;</span>Cleansing</a></span></li><li><span><a href=\"#Normalization-(scaling)\" data-toc-modified-id=\"Normalization-(scaling)-4.1.3\"><span class=\"toc-item-num\">4.1.3&nbsp;&nbsp;</span>Normalization (scaling)</a></span></li></ul></li><li><span><a href=\"#S-51-Indicator-3.4.2-Average-working-hours\" data-toc-modified-id=\"S-51-Indicator-3.4.2-Average-working-hours-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>S-51 Indicator 3.4.2 Average working hours</a></span><ul class=\"toc-item\"><li><span><a href=\"#Extraction\" data-toc-modified-id=\"Extraction-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>Extraction</a></span></li><li><span><a href=\"#Cleansing\" data-toc-modified-id=\"Cleansing-4.2.2\"><span class=\"toc-item-num\">4.2.2&nbsp;&nbsp;</span>Cleansing</a></span></li><li><span><a href=\"#Normalization-(scaling)\" data-toc-modified-id=\"Normalization-(scaling)-4.2.3\"><span class=\"toc-item-num\">4.2.3&nbsp;&nbsp;</span>Normalization (scaling)</a></span></li></ul></li><li><span><a href=\"#S-53-Indicator-NEW--Women-in-management\" data-toc-modified-id=\"S-53-Indicator-NEW--Women-in-management-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>S-53 Indicator NEW  Women in management</a></span><ul class=\"toc-item\"><li><span><a href=\"#Extraction\" data-toc-modified-id=\"Extraction-4.3.1\"><span class=\"toc-item-num\">4.3.1&nbsp;&nbsp;</span>Extraction</a></span></li><li><span><a href=\"#Cleansing\" data-toc-modified-id=\"Cleansing-4.3.2\"><span class=\"toc-item-num\">4.3.2&nbsp;&nbsp;</span>Cleansing</a></span></li><li><span><a href=\"#Normalization-(scaling)\" data-toc-modified-id=\"Normalization-(scaling)-4.3.3\"><span class=\"toc-item-num\">4.3.3&nbsp;&nbsp;</span>Normalization (scaling)</a></span></li></ul></li></ul></li><li><span><a href=\"#Misc-sources:-Data-cleansing---normalization\" data-toc-modified-id=\"Misc-sources:-Data-cleansing---normalization-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Misc sources: Data cleansing - normalization</a></span><ul class=\"toc-item\"><li><span><a href=\"#S-11-Economist-Intelligence-Unit,-Out-of-the-Shadows-Index.-Legal-Framework-score-only\" data-toc-modified-id=\"S-11-Economist-Intelligence-Unit,-Out-of-the-Shadows-Index.-Legal-Framework-score-only-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>S-11 Economist Intelligence Unit, Out of the Shadows Index. Legal Framework score only</a></span></li><li><span><a href=\"#S-60-(prev-S-18)-Walk-Free-Foundation.Global-Slavery-Index.-Prevalence-of-Modern-Slavery.-Prevalence-score-only.\" data-toc-modified-id=\"S-60-(prev-S-18)-Walk-Free-Foundation.Global-Slavery-Index.-Prevalence-of-Modern-Slavery.-Prevalence-score-only.-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>S-60 (prev S-18) Walk Free Foundation.Global Slavery Index. Prevalence of Modern Slavery. Prevalence score only.</a></span><ul class=\"toc-item\"><li><span><a href=\"#Extraction\" data-toc-modified-id=\"Extraction-5.2.1\"><span class=\"toc-item-num\">5.2.1&nbsp;&nbsp;</span>Extraction</a></span></li><li><span><a href=\"#Transform\" data-toc-modified-id=\"Transform-5.2.2\"><span class=\"toc-item-num\">5.2.2&nbsp;&nbsp;</span>Transform</a></span></li><li><span><a href=\"#Normalization-(scaling)\" data-toc-modified-id=\"Normalization-(scaling)-5.2.3\"><span class=\"toc-item-num\">5.2.3&nbsp;&nbsp;</span>Normalization (scaling)</a></span></li><li><span><a href=\"#Left-join-to-counry-list-to-bring-it-into-target-format\" data-toc-modified-id=\"Left-join-to-counry-list-to-bring-it-into-target-format-5.2.4\"><span class=\"toc-item-num\">5.2.4&nbsp;&nbsp;</span>Left join to counry list to bring it into target format</a></span></li></ul></li></ul></li><li><span><a href=\"#---------Archive/-Trash/-dev-section-(Disregard-this-section)-----------\" data-toc-modified-id=\"---------Archive/-Trash/-dev-section-(Disregard-this-section)------------6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span><div align=\"center\"> - - - - Archive/ Trash/ dev section (Disregard this section) - - - - - </div></a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Left-join-to-counry-list-to-bring-it-into-target-format\" data-toc-modified-id=\"Left-join-to-counry-list-to-bring-it-into-target-format-6.0.1\"><span class=\"toc-item-num\">6.0.1&nbsp;&nbsp;</span>Left join to counry list to bring it into target format</a></span></li><li><span><a href=\"#Left-join-to-counry-list-to-bring-it-into-target-format\" data-toc-modified-id=\"Left-join-to-counry-list-to-bring-it-into-target-format-6.0.2\"><span class=\"toc-item-num\">6.0.2&nbsp;&nbsp;</span>Left join to counry list to bring it into target format</a></span></li></ul></li><li><span><a href=\"#Define-function\" data-toc-modified-id=\"Define-function-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Define function</a></span><ul class=\"toc-item\"><li><span><a href=\"#Generic-functions\" data-toc-modified-id=\"Generic-functions-6.1.1\"><span class=\"toc-item-num\">6.1.1&nbsp;&nbsp;</span>Generic functions</a></span></li><li><span><a href=\"#UNESCO-API-specific-functions\" data-toc-modified-id=\"UNESCO-API-specific-functions-6.1.2\"><span class=\"toc-item-num\">6.1.2&nbsp;&nbsp;</span>UNESCO API specific functions</a></span></li><li><span><a href=\"#SDG-API-specific-functions\" data-toc-modified-id=\"SDG-API-specific-functions-6.1.3\"><span class=\"toc-item-num\">6.1.3&nbsp;&nbsp;</span>SDG API specific functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Transforming-ILO-data\" data-toc-modified-id=\"Transforming-ILO-data-6.1.3.1\"><span class=\"toc-item-num\">6.1.3.1&nbsp;&nbsp;</span>Transforming ILO data</a></span></li><li><span><a href=\"#Extracting-data-through-ILO-API\" data-toc-modified-id=\"Extracting-data-through-ILO-API-6.1.3.2\"><span class=\"toc-item-num\">6.1.3.2&nbsp;&nbsp;</span>Extracting data through ILO API</a></span></li></ul></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data extraction CRBA\n",
    "\n",
    "* This is the main file, which must be executed to extract data from all soures for which the process has been automated\n",
    "* For a list of all soures along with the information whether its a manual or atutomated extraction please consult the follownig file: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backlog of tasks\n",
    "\n",
    " - [X] Write the functions to extract data from UNESCO API into one function which takes the indicator code as argument (rather than having two functions with the same functionatlity for two indicators | and check if that ingo is not also in the SDG API anyways\n",
    " - [ ] Finalize understanding the maplecroft logic for a quantaitive indicator and then for: qualitative indicator and also understand all the exceptions (.e.g inverted indicator etc.) --> Draw a decision tree of all if-then statements and check if this logic is desired by Alex and Tomas\n",
    " - [ ] Implement this logic in the python function started below\n",
    " - [ ] Check with Alex/ Tomas what the final result should look like (raw data + scaled data + aggregated data = final index score for each indicator + aggregrated index scores\n",
    " - [ ] Check with Daniele in what format these things should be in SDMX (should they include dimensions like sex, age, ... SHould they include units etc.?\n",
    " - [ ] Start implementing INdicator by indicator (rather than extracting data from all sources first before transforming anything)\n",
    " - [ ] For log file: \n",
    "    * Summary statistics of indicator\n",
    "    * Coverage (how many NA values) for indicator\n",
    "    * Data on average from which year \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import urllib\n",
    "import requests\n",
    "import os\n",
    "import numpy as np\n",
    "# Extractors\n",
    "from extract.unesco_extractor import extract_unesco_api_data\n",
    "from extract.ilo_extractor import extract_ilo_api_data\n",
    "from extract.sdg_extractor import extract_sdg_api_data\n",
    "from extract import save_raw_data\n",
    "# Cleansers\n",
    "from cleanse.unesco_cleanser import cleanse_unesco_api_data\n",
    "from cleanse.ilo_cleanser import cleanse_ilo_api_data\n",
    "from cleanse.sdg_cleanser import cleanse_sdg_api_data\n",
    "from cleanse.save_cleansed_data import save_cleansed_data\n",
    "# Normalizer\n",
    "from normalize import scaler\n",
    "from normalize import save_normalized_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import country list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the export path for all RAW data sources\n",
    "cwd = os.getcwd()\n",
    "\n",
    "data_sources_raw = cwd + \"\\data\\data_raw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the list of countries which contains all different variations of country names \n",
    "country_full_list = pd.read_excel(cwd + '\\\\all_countrynames_list.xlsx',\n",
    "                                 keep_default_na = False).drop_duplicates()\n",
    "\n",
    "# Create a version of the list with unique ISO2 and ISO3 codes\n",
    "country_iso_list = country_full_list.drop_duplicates(subset = 'CountryIso2')\n",
    "\n",
    "# Country CRBA list, this is the list of the countries that should be contained \n",
    "country_crba_list = pd.read_excel(cwd + '\\\\crba_country_list.xlsx',\n",
    "                                 header = None,\n",
    "                                 usecols = [0, 1], \n",
    "                                 names = ['COUNTRY_ISO_3', 'COUNTRY_NAME']).merge(\n",
    "                                        right = country_iso_list,\n",
    "                                        how = 'left',\n",
    "                                        left_on = 'COUNTRY_ISO_3',\n",
    "                                        right_on = 'CountryIso3',\n",
    "                                        validate = 'one_to_one')[\n",
    "            ['COUNTRY_ISO_3', 'COUNTRY_NAME', 'CountryIso2']].rename(columns = {'CountryIso2': \"COUNTRY_ISO_2\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNESCO Sources: Data extraction - cleansing - normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S-55 (prev S-15) Percentage of out-of-school adolescents of lower secondary school age. \n",
    "\n",
    "The data of this source is retrieved from the UNSECO API, for which you must have an API key to acces it. For that, you must sign up. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data\n",
    "s55_raw = extract_unesco_api_data(\n",
    "    api_call_url = 'https://api.uis.unesco.org/sdmx/data/UNESCO,SDG4,2.0/ROFST.PT.L2+L2_3+L3._T._T+F+M.SCH_AGE_GROUP._T.INST_T._Z._T._Z._Z._Z._T._T._Z._Z._Z.?startPeriod=2005&endPeriod=2018&format=csv-sdmx&locale=en&subscription-key=460ab272abdd43c892bb59c218c22c09'\n",
    ")\n",
    "\n",
    "# Save raw data\n",
    "save_raw_data.save_raw_data(dataframe = s55_raw,\n",
    "             filename = 'S_55_raw.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain latest available observation and discard countries that are not in the final CRBA country list \n",
    "s55_cleansed = cleanse_unesco_api_data(raw_data = s55_raw,\n",
    "                                   raw_data_iso_2_col = 'REF_AREA',\n",
    "                                   country_df = country_crba_list,\n",
    "                                   country_df_iso2_col = 'COUNTRY_ISO_2',\n",
    "                                   non_dim_cols = ['OBS_VALUE', 'TIME_PERIOD', 'OBS_STATUS']\n",
    "                                   ) # the variable OBS_STATUS varies, but is an attribute, not a dimension. It specifies how an OBS_VALUE was created (A = standard way, E = estimate)\n",
    "\n",
    "# save cleansed data\n",
    "save_cleansed_data(dataframe = s55_cleansed,\n",
    "             filename = 'S_55_cleansed.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization (scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Scale the raw data and bring the dataframe into long formmat\n",
    "s55_normalized = scaler.normalizer(cleansed_data = s55_cleansed,\n",
    "                        indicator_raw_value = 'OBS_VALUE',\n",
    "                        indicator_code = '3.1.3',\n",
    "                        indicator_name = 'Out-of-school adolescents (lower secondary)',\n",
    "                        cleansed_df_iso2_col = 'REF_AREA',\n",
    "                        crba_final_country_list = country_crba_list,\n",
    "                        crba_final_country_list_iso_col = 'COUNTRY_ISO_2',\n",
    "                        inverted = True,\n",
    "                        non_dim_cols = ['TIME_PERIOD', \n",
    "                                        'REF_AREA', \n",
    "                                        'OBS_VALUE', \n",
    "                                        'OBS_STATUS', \n",
    "                                        'COUNTRY_ISO_3', \n",
    "                                        'COUNTRY_NAME', \n",
    "                                        'COUNTRY_ISO_2', \n",
    "                                        '_merge']\n",
    "                       )\n",
    "\n",
    "# save normalized data\n",
    "save_normalized_data.save_normalized_data(dataframe = s55_normalized,\n",
    "             filename = 'S_55_normalized.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select only those rows/ columns which are relevant \n",
    "\n",
    "* < to do >\n",
    "* Right now, the normalized dataset contains data on ALL subsets (which are defined by the dimension values). Ultimately, there has to be a commitment to one Subset of an indicator \n",
    "* But this filtering to exactly one dimension (i.e. one row per country) will be done later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S-56 (prev S-16) Percentage of out-of-school youth of upper secondary school age\n",
    "\n",
    "The data of this source is retrieved from the UNSECO API, for which you must have an API key to acces it. For that, you must sign up. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data\n",
    "s56_raw = extract_unesco_api_data(\n",
    "    api_call_url = 'https://api.uis.unesco.org/sdmx/data/UNESCO,SDG4,2.0/ROFST.PT.L2+L2_3+L3._T._T+F+M.SCH_AGE_GROUP._T.INST_T._Z._T._Z._Z._Z._T._T._Z._Z._Z.?startPeriod=2005&endPeriod=2018&format=csv-sdmx&locale=en&subscription-key=460ab272abdd43c892bb59c218c22c09'\n",
    ")\n",
    "\n",
    "# Save raw data\n",
    "save_raw_data.save_raw_data(dataframe = s56_raw,\n",
    "             filename = 'S_56_raw.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain latest available observation and discard countries that are not in the final CRBA country list \n",
    "s56_cleansed = cleanse_unesco_api_data(raw_data = s56_raw,\n",
    "                                   raw_data_iso_2_col = 'REF_AREA',\n",
    "                                   country_df = country_crba_list,\n",
    "                                   country_df_iso2_col = 'COUNTRY_ISO_2',\n",
    "                                   non_dim_cols = ['OBS_VALUE', \n",
    "                                                   'TIME_PERIOD', \n",
    "                                                   'OBS_STATUS']\n",
    "                                   ) # the variable OBS_STATUS varies, but is an attribute, not a dimension. It specifies how an OBS_VALUE was created (A = standard way, E = estimate)\n",
    "\n",
    "# save cleansed data\n",
    "save_cleansed_data(dataframe = s56_cleansed,\n",
    "             filename = 'S_56_cleansed.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization (scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Scale the raw data and bring the dataframe into long formmat\n",
    "s56_normalized = scaler.normalizer(cleansed_data = s56_cleansed,\n",
    "                        indicator_raw_value = 'OBS_VALUE',\n",
    "                        indicator_code = '3.1.4',\n",
    "                        indicator_name = 'Out-of-school adolescents (upper secondary)',\n",
    "                        cleansed_df_iso2_col = 'REF_AREA',\n",
    "                        crba_final_country_list = country_crba_list,\n",
    "                        crba_final_country_list_iso_col = 'COUNTRY_ISO_2',\n",
    "                        inverted = True,\n",
    "                        non_dim_cols = ['TIME_PERIOD',\n",
    "                                        'REF_AREA',\n",
    "                                        'OBS_VALUE',\n",
    "                                        'OBS_STATUS',\n",
    "                                        'COUNTRY_ISO_3',\n",
    "                                        'COUNTRY_NAME',\n",
    "                                        'COUNTRY_ISO_2',\n",
    "                                        '_merge']\n",
    "                       )\n",
    "\n",
    "# save normalized data\n",
    "save_normalized_data.save_normalized_data(dataframe = s56_normalized,\n",
    "             filename = 'S_56_normalized.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select only those rows/ columns which are relevant \n",
    "\n",
    "* < to do >\n",
    "* Right now, the normalized dataset contains data on ALL subsets (which are defined by the dimension values). Ultimately, there has to be a commitment to one Subset of an indicator \n",
    "* But this filtering to exactly one dimension (i.e. one row per country) will be done later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S-52 Gross early childhood education enrolment ratio in (a) pre-primary education and (b) early childhood educational development (SDG Indicator 4.2.4) \n",
    "\n",
    "The data of this source is retrieved from the UNSECO API, for which you must have an API key to acces it. For that, you must sign up. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data\n",
    "s52_raw = extract_unesco_api_data(\n",
    "    api_call_url = 'https://api.uis.unesco.org/sdmx/data/UNESCO,EDU_NON_FINANCE,3.0/GECER.PT.L01+L02._T._T+F+M._T._T.INST_T._Z._Z._T._T._T._Z._Z._Z._Z._Z.W00.W00._Z.?startPeriod=2010&endPeriod=2020&format=csv-sdmx&locale=en&subscription-key=460ab272abdd43c892bb59c218c22c09'\n",
    ")\n",
    "\n",
    "# Save raw data\n",
    "save_raw_data.save_raw_data(dataframe = s52_raw,\n",
    "             filename = 'S_52_raw.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain latest available observation and discard countries that are not in the final CRBA country list \n",
    "s52_cleansed = cleanse_unesco_api_data(raw_data = s52_raw,\n",
    "                                   raw_data_iso_2_col = 'REF_AREA',\n",
    "                                   country_df = country_crba_list,\n",
    "                                   country_df_iso2_col = 'COUNTRY_ISO_2',\n",
    "                                   non_dim_cols = ['OBS_VALUE', \n",
    "                                                   'TIME_PERIOD', \n",
    "                                                   'OBS_STATUS']\n",
    "                                   ) # the variable OBS_STATUS varies, but is an attribute, not a dimension. It specifies how an OBS_VALUE was created (A = standard way, E = estimate)\n",
    "\n",
    "# save cleansed data\n",
    "save_cleansed_data(dataframe = s52_cleansed,\n",
    "             filename = 'S_52_cleansed.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization (scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Scale the raw data and bring the dataframe into long formmat\n",
    "s52_normalized = scaler.normalizer(cleansed_data = s52_cleansed,\n",
    "                        indicator_raw_value = 'OBS_VALUE',\n",
    "                        indicator_code = '3.4.4',\n",
    "                        indicator_name = 'Access to pre-primary education.',\n",
    "                        cleansed_df_iso2_col = 'REF_AREA',\n",
    "                        crba_final_country_list = country_crba_list,\n",
    "                        crba_final_country_list_iso_col = 'COUNTRY_ISO_2',\n",
    "                        inverted = False,\n",
    "                        non_dim_cols = ['TIME_PERIOD',\n",
    "                                        'REF_AREA',\n",
    "                                        'OBS_VALUE',\n",
    "                                        'OBS_STATUS',\n",
    "                                        'COUNTRY_ISO_3',\n",
    "                                        'COUNTRY_NAME',\n",
    "                                        'COUNTRY_ISO_2',\n",
    "                                        '_merge']\n",
    "                       )\n",
    "\n",
    "# save normalized data\n",
    "save_normalized_data.save_normalized_data(dataframe = s52_normalized,\n",
    "             filename = 'S_52_normalized.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select only those rows/ columns which are relevant \n",
    "\n",
    "* < to do >\n",
    "* Right now, the normalized dataset contains data on ALL subsets (which are defined by the dimension values). Ultimately, there has to be a commitment to one Subset of an indicator \n",
    "* But this filtering to exactly one dimension (i.e. one row per country) will be done later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SDG API Sources: Data extraction - cleansing - normalization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S-24 (prev. S-14) SDG Indicator 8.7.1 Proportion of children aged 5-17 years engaged in child labour SL_TLF_CHLDEA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract data and directly flatten it out into a pandas dataframe\n",
    "s24_raw = extract_sdg_api_data(series_code = 'SL_TLF_CHLDEA')\n",
    "\n",
    "# Save data to raw data folder\n",
    "save_raw_data.save_raw_data(dataframe = s24_raw,\n",
    "             filename = 'S_24.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s24_cleansed = cleanse_sdg_api_data(raw_data = s24_raw,\n",
    "                                   country_list_full = country_full_list,\n",
    "                                   country_list_full_name_col = 'CountryDesc',\n",
    "                                   country_list_full_iso2_col = 'CountryIso2',\n",
    "                                   country_df = country_crba_list,\n",
    "                                   country_df_iso2_col = 'COUNTRY_ISO_2',\n",
    "                                   non_dim_cols = ['value',\n",
    "                                                  'source',\n",
    "                                                   'timePeriodStart'\n",
    "                                                  'footnotes',\n",
    "                                                  'Unnamed: 0'] \n",
    "                                   )\n",
    "\n",
    "# save cleansed data\n",
    "save_cleansed_data(dataframe = s24_cleansed,\n",
    "             filename = 'S_24_cleansed.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization (scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Scale the raw data and bring the dataframe into long formmat\n",
    "s24_normalized = scaler.normalizer(cleansed_data = s24_cleansed,\n",
    "                        indicator_raw_value = 'value',\n",
    "                        indicator_code = '3.1.1.',\n",
    "                        indicator_name = 'Child labour rate (5-17)',\n",
    "                        cleansed_df_iso2_col = 'CountryIso2',\n",
    "                        crba_final_country_list = country_crba_list,\n",
    "                        crba_final_country_list_iso_col = 'COUNTRY_ISO_2',\n",
    "                        inverted = True,\n",
    "                        non_dim_cols = ['geoAreaCode',\n",
    "                                        'geoAreaName',\n",
    "                                        'timePeriodStart',\n",
    "                                        'value',\n",
    "                                        'footnotes', \n",
    "                                        'Unnamed: 0', \n",
    "                                        'CountryDesc',\n",
    "                                        'CountryIso2',\n",
    "                                        'CountryIso3',\n",
    "                                        'COUNTRY_ISO_2',\n",
    "                                        'COUNTRY_ISO_3',\n",
    "                                        'COUNTRY_NAME',\n",
    "                                        '_merge']\n",
    "                       )\n",
    "\n",
    "# save normalized data\n",
    "save_normalized_data.save_normalized_data(dataframe = s24_normalized,\n",
    "             filename = 'S_24_normalized.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select only those rows/ columns which are relevant \n",
    "\n",
    "* < to do >\n",
    "* Right now, the normalized dataset contains data on ALL subsets (which are defined by the dimension values). Ultimately, there has to be a commitment to one Subset of an indicator \n",
    "* But this filtering to exactly one dimension (i.e. one row per country) will be done later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S-23 (prev. S-17) ILO STAT Informal Employment (% of total non-agricultural employment) SL_ISV_IFEM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction\n",
    "\n",
    "This data can be extracted from two different sources: \n",
    "    \n",
    "* ILO\n",
    "* SDG database\n",
    "\n",
    "After consultation with Alex and Tomás we have decided to extract the data from the SDG API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract data and directly flatten it out into a pandas dataframe\n",
    "s23_raw = extract_sdg_api_data(series_code = 'SL_ISV_IFEM')\n",
    "\n",
    "# Save data to raw data folder\n",
    "save_raw_data.save_raw_data(dataframe = s23_raw,\n",
    "             filename = 'S_23.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s23_cleansed = cleanse_sdg_api_data(raw_data = s23_raw,\n",
    "                                   country_list_full = country_full_list,\n",
    "                                   country_list_full_name_col = 'CountryDesc',\n",
    "                                   country_list_full_iso2_col = 'CountryIso2',\n",
    "                                   country_df = country_crba_list,\n",
    "                                   country_df_iso2_col = 'COUNTRY_ISO_2',\n",
    "                                   non_dim_cols = ['value',\n",
    "                                                  'source',\n",
    "                                                  'footnotes',\n",
    "                                                   'timePeriodStart',\n",
    "                                                  'Unnamed: 0'] \n",
    "                                   )\n",
    "\n",
    "# save cleansed data\n",
    "save_cleansed_data(dataframe = s23_cleansed,\n",
    "             filename = 'S_23_cleansed.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization (scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the raw data and bring the dataframe into long formmat\n",
    "s23_normalized = scaler.normalizer(cleansed_data = s23_cleansed,\n",
    "                        indicator_raw_value = 'value',\n",
    "                        indicator_code = '3.1.5',\n",
    "                        indicator_name = 'Informal employment.',\n",
    "                        cleansed_df_iso2_col = 'CountryIso2',\n",
    "                        crba_final_country_list = country_crba_list,\n",
    "                        crba_final_country_list_iso_col = 'COUNTRY_ISO_2',\n",
    "                        inverted = True,\n",
    "                        non_dim_cols = ['geoAreaCode',\n",
    "                                        'geoAreaName',\n",
    "                                        'timePeriodStart',\n",
    "                                        'source',\n",
    "                                        'value',\n",
    "                                        'footnotes', \n",
    "                                        'Unnamed: 0', \n",
    "                                        'CountryDesc',\n",
    "                                        'CountryIso2',\n",
    "                                        'CountryIso3',\n",
    "                                        'COUNTRY_ISO_2',\n",
    "                                        'COUNTRY_ISO_3',\n",
    "                                        'COUNTRY_NAME',\n",
    "                                        '_merge']\n",
    "                       )\n",
    "\n",
    "# save normalized data\n",
    "save_normalized_data.save_normalized_data(dataframe = s23_normalized,\n",
    "             filename = 'S_23_normalized.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select only those rows/ columns which are relevant \n",
    "\n",
    "* < to do >\n",
    "* Right now, the normalized dataset contains data on ALL subsets (which are defined by the dimension values). Ultimately, there has to be a commitment to one Subset of an indicator \n",
    "* But this filtering to exactly one dimension (i.e. one row per country) will be done later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S-61 (prev S-19) SDG Indicator 16.2.2 Detected victims of human trafficking, by age and sex (number)  VC_HTF_DETV \n",
    "### Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data\n",
    "s61_raw = extract_sdg_api_data(series_code = 'VC_HTF_DETV')\n",
    "\n",
    "# Save raw data\n",
    "save_raw_data.save_raw_data(dataframe = s61_raw,\n",
    "              filename = 'S_61.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s61_cleansed = cleanse_sdg_api_data(raw_data = s61_raw,\n",
    "                                   country_list_full = country_full_list,\n",
    "                                   country_list_full_name_col = 'CountryDesc',\n",
    "                                   country_list_full_iso2_col = 'CountryIso2',\n",
    "                                   country_df = country_crba_list,\n",
    "                                   country_df_iso2_col = 'COUNTRY_ISO_2',\n",
    "                                   non_dim_cols = ['value',\n",
    "                                                  'source',\n",
    "                                                  'footnotes',\n",
    "                                                   'timePeriodStart',\n",
    "                                                  'Unnamed: 0'] \n",
    "                                   )\n",
    "\n",
    "# save cleansed data\n",
    "save_cleansed_data(dataframe = s61_cleansed,\n",
    "             filename = 'S_61_cleansed.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization (scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the raw data and bring the dataframe into long formmat\n",
    "s61_normalized = scaler.normalizer(cleansed_data = s61_cleansed,\n",
    "                        indicator_raw_value = 'value',\n",
    "                        indicator_code = '3.2.2',\n",
    "                        indicator_name = 'Prevalence of human trafficking',\n",
    "                        cleansed_df_iso2_col = 'CountryIso2',\n",
    "                        crba_final_country_list = country_crba_list,\n",
    "                        crba_final_country_list_iso_col = 'COUNTRY_ISO_2',\n",
    "                        inverted = True,\n",
    "                        non_dim_cols = ['geoAreaCode',\n",
    "                                        'geoAreaName',\n",
    "                                        'timePeriodStart',\n",
    "                                        'source',\n",
    "                                        'value',\n",
    "                                        'footnotes', \n",
    "                                        'Unnamed: 0', \n",
    "                                        'CountryDesc',\n",
    "                                        'CountryIso2',\n",
    "                                        'CountryIso3',\n",
    "                                        'COUNTRY_ISO_2',\n",
    "                                        'COUNTRY_ISO_3',\n",
    "                                        'COUNTRY_NAME',\n",
    "                                        '_merge']\n",
    "                       )\n",
    "\n",
    "# save normalized data\n",
    "save_normalized_data.save_normalized_data(dataframe = s61_normalized,\n",
    "             filename = 'S_61_normalized.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select only those rows/ columns which are relevant \n",
    "\n",
    "* < to do >\n",
    "* Right now, the normalized dataset contains data on ALL subsets (which are defined by the dimension values). Ultimately, there has to be a commitment to one Subset of an indicator \n",
    "* But this filtering to exactly one dimension (i.e. one row per country) will be done later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S-62 (prev S-20) SDG Indicator 1.1.1. Proportion of population below international poverty line (%) SI_POV_DAY1 \n",
    "### Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data\n",
    "s62_raw = extract_sdg_api_data(series_code = 'SI_POV_DAY1')\n",
    "\n",
    "# Save raw data\n",
    "save_raw_data.save_raw_data(dataframe = s62_raw,\n",
    "              filename = 'S_62.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the raw data\n",
    "s62_cleansed = cleanse_sdg_api_data(raw_data = s62_raw,\n",
    "                                   country_list_full = country_full_list,\n",
    "                                   country_list_full_name_col = 'CountryDesc',\n",
    "                                   country_list_full_iso2_col = 'CountryIso2',\n",
    "                                   country_df = country_crba_list,\n",
    "                                   country_df_iso2_col = 'COUNTRY_ISO_2',\n",
    "                                   non_dim_cols = ['value',\n",
    "                                                  'source',\n",
    "                                                  'footnotes',\n",
    "                                                   'timePeriodStart',\n",
    "                                                  'Unnamed: 0'] \n",
    "                                   )\n",
    "\n",
    "# save cleansed data\n",
    "save_cleansed_data(dataframe = s62_cleansed,\n",
    "             filename = 'S_62_cleansed.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization (scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the raw data and bring the dataframe into long formmat\n",
    "s62_normalized = scaler.normalizer(cleansed_data = s62_cleansed,\n",
    "                        indicator_raw_value = 'value',\n",
    "                        indicator_code = '3.2.3.',\n",
    "                        indicator_name = 'Poverty rates',\n",
    "                        cleansed_df_iso2_col = 'CountryIso2',\n",
    "                        crba_final_country_list = country_crba_list,\n",
    "                        crba_final_country_list_iso_col = 'COUNTRY_ISO_2',\n",
    "                        inverted = True,\n",
    "                        non_dim_cols = ['geoAreaCode',\n",
    "                                        'geoAreaName',\n",
    "                                        'timePeriodStart',\n",
    "                                        'source',\n",
    "                                        'value',\n",
    "                                        'footnotes', \n",
    "                                        'Unnamed: 0', \n",
    "                                        'CountryDesc',\n",
    "                                        'CountryIso2',\n",
    "                                        'CountryIso3',\n",
    "                                        'COUNTRY_ISO_2',\n",
    "                                        'COUNTRY_ISO_3',\n",
    "                                        'COUNTRY_NAME',\n",
    "                                        '_merge']\n",
    "                       )\n",
    "\n",
    "# save normalized data\n",
    "save_normalized_data.save_normalized_data(dataframe = s62_normalized,\n",
    "             filename = 'S_62_normalized.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select only those rows/ columns which are relevant \n",
    "\n",
    "* < to do >\n",
    "* Right now, the normalized dataset contains data on ALL subsets (which are defined by the dimension values). Ultimately, there has to be a commitment to one Subset of an indicator \n",
    "* But this filtering to exactly one dimension (i.e. one row per country) will be done later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S-203 (prev S-47) SDG Indicator 8.5.1. Average hourly earnings of employees by sex and occupation (local currency) SL_EMP_AEARN\n",
    "### Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data\n",
    "s203_raw = extract_sdg_api_data(series_code = 'SL_EMP_AEARN')\n",
    "\n",
    "# Save raw data\n",
    "save_raw_data.save_raw_data(dataframe = s203_raw,\n",
    "              filename = 'S_203.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the raw data\n",
    "s203_cleansed = cleanse_sdg_api_data(raw_data = s203_raw,\n",
    "                                   country_list_full = country_full_list,\n",
    "                                   country_list_full_name_col = 'CountryDesc',\n",
    "                                   country_list_full_iso2_col = 'CountryIso2',\n",
    "                                   country_df = country_crba_list,\n",
    "                                   country_df_iso2_col = 'COUNTRY_ISO_2',\n",
    "                                   non_dim_cols = ['value',\n",
    "                                                  'source',\n",
    "                                                  'footnotes',\n",
    "                                                   'timePeriodStart',\n",
    "                                                  'Unnamed: 0'] \n",
    "                                   )\n",
    "\n",
    "# save cleansed data\n",
    "save_cleansed_data(dataframe = s203_cleansed,\n",
    "             filename = 'S_203_cleansed.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization (scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the raw data and bring the dataframe into long formmat\n",
    "s203_normalized = scaler.normalizer(cleansed_data = s203_cleansed,\n",
    "                        indicator_raw_value = 'value',\n",
    "                        indicator_code = '3.4.1',\n",
    "                        indicator_name = 'Average Earnings',\n",
    "                        cleansed_df_iso2_col = 'CountryIso2',\n",
    "                        crba_final_country_list = country_crba_list,\n",
    "                        crba_final_country_list_iso_col = 'COUNTRY_ISO_2',\n",
    "                        inverted = False,\n",
    "                        non_dim_cols = ['geoAreaCode',\n",
    "                                        'geoAreaName',\n",
    "                                        'timePeriodStart',\n",
    "                                        'source',\n",
    "                                        'value',\n",
    "                                        'footnotes', \n",
    "                                        'Unnamed: 0', \n",
    "                                        'CountryDesc',\n",
    "                                        'CountryIso2',\n",
    "                                        'CountryIso3',\n",
    "                                        'COUNTRY_ISO_2',\n",
    "                                        'COUNTRY_ISO_3',\n",
    "                                        'COUNTRY_NAME',\n",
    "                                        '_merge']\n",
    "                       )\n",
    "\n",
    "# save normalized data\n",
    "save_normalized_data.save_normalized_data(dataframe = s203_normalized,\n",
    "             filename = 'S_203_normalized.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select only those rows/ columns which are relevant \n",
    "\n",
    "* < to do >\n",
    "* Right now, the normalized dataset contains data on ALL subsets (which are defined by the dimension values). Ultimately, there has to be a commitment to one Subset of an indicator \n",
    "* But this filtering to exactly one dimension (i.e. one row per country) will be done later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S-204 (prev S-48) SDG Indicator 1.1.1, ILO Stat: Employed population below international poverty line, by sex and age (%) SI_POV_EMP1\n",
    "### Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data\n",
    "s204_raw = extract_sdg_api_data(series_code = 'SI_POV_EMP1')\n",
    "\n",
    "# Save raw data\n",
    "save_raw_data.save_raw_data(dataframe = s204_raw,\n",
    "              filename = 'S_204.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the raw data\n",
    "s204_cleansed = cleanse_sdg_api_data(raw_data = s204_raw,\n",
    "                                   country_list_full = country_full_list,\n",
    "                                   country_list_full_name_col = 'CountryDesc',\n",
    "                                   country_list_full_iso2_col = 'CountryIso2',\n",
    "                                   country_df = country_crba_list,\n",
    "                                   country_df_iso2_col = 'COUNTRY_ISO_2',\n",
    "                                   non_dim_cols = ['value',\n",
    "                                                  'source',\n",
    "                                                  'footnotes',\n",
    "                                                   'timePeriodStart',\n",
    "                                                  'Unnamed: 0'] \n",
    "                                   )\n",
    "\n",
    "# save cleansed data\n",
    "save_cleansed_data(dataframe = s204_cleansed,\n",
    "             filename = 'S_204_cleansed.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization (scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the raw data and bring the dataframe into long formmat\n",
    "s204_normalized = scaler.normalizer(cleansed_data = s204_cleansed,\n",
    "                        indicator_raw_value = 'value',\n",
    "                        indicator_code = 'NEW',\n",
    "                        indicator_name = 'Working poverty rate',\n",
    "                        cleansed_df_iso2_col = 'CountryIso2',\n",
    "                        crba_final_country_list = country_crba_list,\n",
    "                        crba_final_country_list_iso_col = 'COUNTRY_ISO_2',\n",
    "                        inverted = True,\n",
    "                        non_dim_cols = ['geoAreaCode',\n",
    "                                        'geoAreaName',\n",
    "                                        'timePeriodStart',\n",
    "                                        'source',\n",
    "                                        'value',\n",
    "                                        'footnotes', \n",
    "                                        'Unnamed: 0', \n",
    "                                        'CountryDesc',\n",
    "                                        'CountryIso2',\n",
    "                                        'CountryIso3',\n",
    "                                        'COUNTRY_ISO_2',\n",
    "                                        'COUNTRY_ISO_3',\n",
    "                                        'COUNTRY_NAME',\n",
    "                                        '_merge']\n",
    "                       )\n",
    "\n",
    "# save normalized data\n",
    "save_normalized_data.save_normalized_data(dataframe = s204_normalized,\n",
    "             filename = 'S_204_normalized.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S-71 SDG Indicator 1.3.1 Proportion of mothers with newborns receiving maternity cash benefit. SI_COV_MATNL \n",
    "### Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following columns are present in the datasets, and this is the number of unique values they have. \n",
      "The column goal has 1 unique values.\n",
      "The column target has 1 unique values.\n",
      "The column indicator has 1 unique values.\n",
      "The column series has 1 unique values.\n",
      "The column seriesDescription has 1 unique values.\n",
      "The column seriesCount has 1 unique values.\n",
      "The column geoAreaCode has 113 unique values.\n",
      "The column geoAreaName has 113 unique values.\n",
      "The column timePeriodStart has 4 unique values.\n",
      "The column value has 66 unique values.\n",
      "The column valueType has 1 unique values.\n",
      "The column time_detail has 1 unique values.\n",
      "The column timeCoverage has 1 unique values.\n",
      "The column upperBound has 1 unique values.\n",
      "The column lowerBound has 1 unique values.\n",
      "The column basePeriod has 1 unique values.\n",
      "The column source has 2 unique values.\n",
      "The column geoInfoUrl has 1 unique values.\n",
      "The column footnotes has 2 unique values.\n",
      "The column attributes.Nature has 1 unique values.\n",
      "The column attributes.Units has 1 unique values.\n",
      "The column dimensions.Sex has 1 unique values.\n",
      "The column dimensions.Reporting Type has 1 unique values.\n",
      "The raw data has been saved as .xlsx file in: D:\\Documents\\2020\\28_UNICEF\\10_working_repo\\data-etl\\data\\data_raw\\\n"
     ]
    }
   ],
   "source": [
    "# Extract data\n",
    "s71_raw = extract_sdg_api_data(series_code = 'SI_COV_MATNL')\n",
    "\n",
    "# Save raw data\n",
    "save_raw_data.save_raw_data(dataframe = s71_raw,\n",
    "              filename = 'S_71.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The raw data has been saved as .xlsx file in: D:\\Documents\\2020\\28_UNICEF\\10_working_repo\\data-etl\\data\\data_cleansed\\\n"
     ]
    }
   ],
   "source": [
    "# Transform the raw data\n",
    "s71_cleansed = cleanse_sdg_api_data(raw_data = s71_raw,\n",
    "                                   country_list_full = country_full_list,\n",
    "                                   country_list_full_name_col = 'CountryDesc',\n",
    "                                   country_list_full_iso2_col = 'CountryIso2',\n",
    "                                   country_df = country_crba_list,\n",
    "                                   country_df_iso2_col = 'COUNTRY_ISO_2',\n",
    "                                   non_dim_cols = ['value',\n",
    "                                                  'source',\n",
    "                                                  'footnotes',\n",
    "                                                   'attributes.Nature',\n",
    "                                                   'timePeriodStart',\n",
    "                                                  'Unnamed: 0'] \n",
    "                                   )\n",
    "\n",
    "# save cleansed data\n",
    "save_cleansed_data(dataframe = s71_cleansed,\n",
    "             filename = 'S_71_cleansed.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization (scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have a selected a few columns, which will not be regarded as dimensions. These are the remaining columns in the dataset, along with the number of values they take in the dataset.\n",
      "The column goal has 1 unique values.\n",
      "The column target has 1 unique values.\n",
      "The column indicator has 1 unique values.\n",
      "The column series has 1 unique values.\n",
      "The column seriesDescription has 1 unique values.\n",
      "The column seriesCount has 1 unique values.\n",
      "The column valueType has 1 unique values.\n",
      "The column time_detail has 1 unique values.\n",
      "The column timeCoverage has 1 unique values.\n",
      "The column upperBound has 1 unique values.\n",
      "The column lowerBound has 1 unique values.\n",
      "The column basePeriod has 1 unique values.\n",
      "The column geoInfoUrl has 1 unique values.\n",
      "The column attributes.Nature has 1 unique values.\n",
      "The column attributes.Units has 1 unique values.\n",
      "The column dimensions.Sex has 1 unique values.\n",
      "The column dimensions.Reporting Type has 1 unique values.\n",
      "The total number of subgroups in the dataset is therefore: 1\n",
      "\n",
      " - \n",
      "\n",
      "In the loop we are currently dealing with the subset #1, which has these defining values: \n",
      " \n",
      " Empty DataFrame\n",
      "Columns: [goal, target, indicator, series, seriesDescription, seriesCount, valueType, time_detail, timeCoverage, upperBound, lowerBound, basePeriod, geoInfoUrl, attributes.Nature, attributes.Units, dimensions.Sex, dimensions.Reporting Type]\n",
      "Index: []\n",
      "\n",
      " The shape of the subset in the cleansed dataset is: (0, 31) \n",
      " \n",
      " \n",
      "Dataframe is empty. There are no values to append.\n",
      " \n",
      " This is the end of loop #1. \n",
      " - \n",
      " \n",
      "\n",
      " - \n",
      "\n",
      "In the loop we are currently dealing with the subset #2, which has these defining values: \n",
      " \n",
      "     goal   target  indicator        series  \\\n",
      "1  ['1']  ['1.3']  ['1.3.1']  SI_COV_MATNL   \n",
      "\n",
      "                                   seriesDescription seriesCount valueType  \\\n",
      "1  [ILO] Proportion of mothers with newborns rece...         119     Float   \n",
      "\n",
      "  time_detail timeCoverage upperBound lowerBound basePeriod geoInfoUrl  \\\n",
      "1        None         None       None       None       None       None   \n",
      "\n",
      "  attributes.Nature attributes.Units dimensions.Sex dimensions.Reporting Type  \n",
      "1                 E          PERCENT         FEMALE                         G  \n",
      "\n",
      " The shape of the subset in the cleansed dataset is: (78, 31) \n",
      " \n",
      " \n",
      "The distribution of the raw data for this subgroup does not contain outliers or is too skewed on the upper end. The maximum value used for the normalisation is the maximum value in the dataset, which is 100.0. This value corresponds to country: 4            Austria\n",
      "7            Belgium\n",
      "10          Bulgaria\n",
      "14            Canada\n",
      "68       Switzerland\n",
      "17          Colombia\n",
      "19            Cyprus\n",
      "20    Czech Republic\n",
      "29           Germany\n",
      "21           Denmark\n",
      "25           Estonia\n",
      "23             Egypt\n",
      "64             Spain\n",
      "26           Finland\n",
      "27            France\n",
      "73    United Kingdom\n",
      "31            Greece\n",
      "18           Croatia\n",
      "33           Hungary\n",
      "37           Ireland\n",
      "34           Iceland\n",
      "38             Italy\n",
      "43     Liechtenstein\n",
      "65         Sri Lanka\n",
      "44         Lithuania\n",
      "45        Luxembourg\n",
      "42            Latvia\n",
      "48          Mongolia\n",
      "46             Malta\n",
      "51       Netherlands\n",
      "54            Norway\n",
      "52       New Zealand\n",
      "57            Poland\n",
      "58          Portugal\n",
      "59           Romania\n",
      "67            Sweden\n",
      "62          Slovakia\n",
      "72           Ukraine\n",
      "75           Uruguay\n",
      "Name: COUNTRY_NAME, dtype: object \n",
      "\n",
      "The distribution of the raw data for this subgroup does not contain outliers or is too skewed on the lower end. The minimum value used for the normalisation is the minimum value in the dataset, which is 0.1. This value corresponds to country: 53    Nigeria\n",
      "Name: COUNTRY_NAME, dtype: object \n",
      "\n",
      " \n",
      " This is the end of loop #2. \n",
      " - \n",
      " \n",
      "The number of rows of the final dataframe (before the conversion from wide to long format is) is divisible by 195. It is: (195, 39)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\2020\\28_UNICEF\\10_working_repo\\data-etl\\normalize\\scaler.py:234: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleansed_data_subset['SCALED'] = round(10 * (cleansed_data_subset[indicator_raw_value].astype('float') - min_val)/ tot_range, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The raw data has been saved as .xlsx file in: D:\\Documents\\2020\\28_UNICEF\\10_working_repo\\data-etl\\data\\data_normalized\\\n"
     ]
    }
   ],
   "source": [
    "# Scale the raw data and bring the dataframe into long formmat\n",
    "s71_normalized = scaler.normalizer(cleansed_data = s71_cleansed,\n",
    "                        indicator_raw_value = 'value',\n",
    "                        indicator_code = 'NEW',\n",
    "                        indicator_name = 'Mothers receiving maternity cash benefits',\n",
    "                        cleansed_df_iso2_col = 'CountryIso2',\n",
    "                        crba_final_country_list = country_crba_list,\n",
    "                        crba_final_country_list_iso_col = 'COUNTRY_ISO_2',\n",
    "                        inverted = False,\n",
    "                        non_dim_cols = ['geoAreaCode',\n",
    "                                        'geoAreaName',\n",
    "                                        'timePeriodStart',\n",
    "                                        'source',\n",
    "                                        'value',\n",
    "                                        'footnotes', \n",
    "                                        'Unnamed: 0', \n",
    "                                        'CountryDesc',\n",
    "                                        'CountryIso2',\n",
    "                                        'CountryIso3',\n",
    "                                        'COUNTRY_ISO_2',\n",
    "                                        'COUNTRY_ISO_3',\n",
    "                                        'COUNTRY_NAME',\n",
    "                                        '_merge']\n",
    "                       )\n",
    "\n",
    "# save normalized data\n",
    "save_normalized_data.save_normalized_data(dataframe = s71_normalized,\n",
    "             filename = 'S_71_normalized.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select only those rows/ columns which are relevant \n",
    "\n",
    "* < to do >\n",
    "* Right now, the normalized dataset contains data on ALL subsets (which are defined by the dimension values). Ultimately, there has to be a commitment to one Subset of an indicator \n",
    "* But this filtering to exactly one dimension (i.e. one row per country) will be done later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S-78 SDG Indicator 1.3.1 Proportion of population covered by social insurance programs SI_COV_SOCINS\n",
    "### Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following columns are present in the datasets, and this is the number of unique values they have. \n",
      "The column goal has 1 unique values.\n",
      "The column target has 1 unique values.\n",
      "The column indicator has 1 unique values.\n",
      "The column series has 1 unique values.\n",
      "The column seriesDescription has 1 unique values.\n",
      "The column seriesCount has 1 unique values.\n",
      "The column geoAreaCode has 141 unique values.\n",
      "The column geoAreaName has 141 unique values.\n",
      "The column timePeriodStart has 18 unique values.\n",
      "The column value has 631 unique values.\n",
      "The column valueType has 1 unique values.\n",
      "The column time_detail has 1 unique values.\n",
      "The column timeCoverage has 1 unique values.\n",
      "The column upperBound has 1 unique values.\n",
      "The column lowerBound has 1 unique values.\n",
      "The column basePeriod has 1 unique values.\n",
      "The column source has 150 unique values.\n",
      "The column geoInfoUrl has 1 unique values.\n",
      "The column footnotes has 157 unique values.\n",
      "The column attributes.Nature has 1 unique values.\n",
      "The column attributes.Units has 1 unique values.\n",
      "The column dimensions.Reporting Type has 1 unique values.\n",
      "The column dimensions.Quantile has 2 unique values.\n",
      "The raw data has been saved as .xlsx file in: D:\\Documents\\2020\\28_UNICEF\\10_working_repo\\data-etl\\data\\data_raw\\\n"
     ]
    }
   ],
   "source": [
    "# Extract data\n",
    "s78_raw = extract_sdg_api_data(series_code = 'SI_COV_SOCINS')\n",
    "\n",
    "# Save raw data\n",
    "save_raw_data.save_raw_data(dataframe = s78_raw,\n",
    "              filename = 'S_78.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The raw data has been saved as .xlsx file in: D:\\Documents\\2020\\28_UNICEF\\10_working_repo\\data-etl\\data\\data_cleansed\\\n"
     ]
    }
   ],
   "source": [
    "# Transform the raw data\n",
    "s78_cleansed = cleanse_sdg_api_data(raw_data = s78_raw,\n",
    "                                   country_list_full = country_full_list,\n",
    "                                   country_list_full_name_col = 'CountryDesc',\n",
    "                                   country_list_full_iso2_col = 'CountryIso2',\n",
    "                                   country_df = country_crba_list,\n",
    "                                   country_df_iso2_col = 'COUNTRY_ISO_2',\n",
    "                                   non_dim_cols = ['value',\n",
    "                                                  'source',\n",
    "                                                  'footnotes',\n",
    "                                                   'attributes.Nature',\n",
    "                                                   'timePeriodStart',\n",
    "                                                  'Unnamed: 0'] \n",
    "                                   )\n",
    "\n",
    "# save cleansed data\n",
    "save_cleansed_data(dataframe = s78_cleansed,\n",
    "             filename = 'S_78_cleansed.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization (scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have a selected a few columns, which will not be regarded as dimensions. These are the remaining columns in the dataset, along with the number of values they take in the dataset.\n",
      "The column goal has 1 unique values.\n",
      "The column target has 1 unique values.\n",
      "The column indicator has 1 unique values.\n",
      "The column series has 1 unique values.\n",
      "The column seriesDescription has 1 unique values.\n",
      "The column seriesCount has 1 unique values.\n",
      "The column valueType has 1 unique values.\n",
      "The column time_detail has 1 unique values.\n",
      "The column timeCoverage has 1 unique values.\n",
      "The column upperBound has 1 unique values.\n",
      "The column lowerBound has 1 unique values.\n",
      "The column basePeriod has 1 unique values.\n",
      "The column geoInfoUrl has 1 unique values.\n",
      "The column attributes.Nature has 1 unique values.\n",
      "The column attributes.Units has 1 unique values.\n",
      "The column dimensions.Reporting Type has 1 unique values.\n",
      "The column dimensions.Quantile has 2 unique values.\n",
      "The total number of subgroups in the dataset is therefore: 2\n",
      "\n",
      " - \n",
      "\n",
      "In the loop we are currently dealing with the subset #1, which has these defining values: \n",
      " \n",
      " Empty DataFrame\n",
      "Columns: [goal, target, indicator, series, seriesDescription, seriesCount, valueType, time_detail, timeCoverage, upperBound, lowerBound, basePeriod, geoInfoUrl, attributes.Nature, attributes.Units, dimensions.Reporting Type, dimensions.Quantile]\n",
      "Index: []\n",
      "\n",
      " The shape of the subset in the cleansed dataset is: (0, 31) \n",
      " \n",
      " \n",
      "Dataframe is empty. There are no values to append.\n",
      " \n",
      " This is the end of loop #1. \n",
      " - \n",
      " \n",
      "\n",
      " - \n",
      "\n",
      "In the loop we are currently dealing with the subset #2, which has these defining values: \n",
      " \n",
      "     goal   target  indicator         series  \\\n",
      "0  ['1']  ['1.3']  ['1.3.1']  SI_COV_SOCINS   \n",
      "\n",
      "                                   seriesDescription seriesCount valueType  \\\n",
      "0  [World Bank] Proportion of population covered ...         644     Float   \n",
      "\n",
      "  time_detail timeCoverage upperBound lowerBound basePeriod geoInfoUrl  \\\n",
      "0        None         None       None       None       None       None   \n",
      "\n",
      "  attributes.Nature attributes.Units dimensions.Reporting Type  \\\n",
      "0                CA          PERCENT                         G   \n",
      "\n",
      "  dimensions.Quantile  \n",
      "0                  Q1  \n",
      "\n",
      " The shape of the subset in the cleansed dataset is: (108, 31) \n",
      " \n",
      " \n",
      "The distribution of the raw data for this subgroup does not contain outliers or is too skewed on the upper end. The maximum value used for the normalisation is the maximum value in the dataset, which is 60.57968. This value corresponds to country: 75    Ghana\n",
      "Name: COUNTRY_NAME, dtype: object \n",
      "\n",
      "The distribution of the raw data for this subgroup does not contain outliers or is too skewed on the lower end. The minimum value used for the normalisation is the minimum value in the dataset, which is 0.0. This value corresponds to country: 81     Haiti\n",
      "192     Togo\n",
      "195    Tonga\n",
      "Name: COUNTRY_NAME, dtype: object \n",
      "\n",
      " \n",
      " This is the end of loop #2. \n",
      " - \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\2020\\28_UNICEF\\10_working_repo\\data-etl\\normalize\\scaler.py:234: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleansed_data_subset['SCALED'] = round(10 * (cleansed_data_subset[indicator_raw_value].astype('float') - min_val)/ tot_range, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " - \n",
      "\n",
      "In the loop we are currently dealing with the subset #3, which has these defining values: \n",
      " \n",
      "     goal   target  indicator         series  \\\n",
      "1  ['1']  ['1.3']  ['1.3.1']  SI_COV_SOCINS   \n",
      "\n",
      "                                   seriesDescription seriesCount valueType  \\\n",
      "1  [World Bank] Proportion of population covered ...         644     Float   \n",
      "\n",
      "  time_detail timeCoverage upperBound lowerBound basePeriod geoInfoUrl  \\\n",
      "1        None         None       None       None       None       None   \n",
      "\n",
      "  attributes.Nature attributes.Units dimensions.Reporting Type  \\\n",
      "1                CA          PERCENT                         G   \n",
      "\n",
      "  dimensions.Quantile  \n",
      "1                  _T  \n",
      "\n",
      " The shape of the subset in the cleansed dataset is: (108, 31) \n",
      " \n",
      " \n",
      "The distribution of the raw data for this subgroup does not contain outliers or is too skewed on the upper end. The maximum value used for the normalisation is the maximum value in the dataset, which is 59.52041. This value corresponds to country: 74    Ghana\n",
      "Name: COUNTRY_NAME, dtype: object \n",
      "\n",
      "The distribution of the raw data for this subgroup does not contain outliers or is too skewed on the lower end. The minimum value used for the normalisation is the minimum value in the dataset, which is 0.37097. This value corresponds to country: 80    Haiti\n",
      "Name: COUNTRY_NAME, dtype: object \n",
      "\n",
      " \n",
      " This is the end of loop #3. \n",
      " - \n",
      " \n",
      "The number of rows of the final dataframe (before the conversion from wide to long format is) is divisible by 195. It is: (390, 39)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\2020\\28_UNICEF\\10_working_repo\\data-etl\\normalize\\scaler.py:234: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleansed_data_subset['SCALED'] = round(10 * (cleansed_data_subset[indicator_raw_value].astype('float') - min_val)/ tot_range, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The raw data has been saved as .xlsx file in: D:\\Documents\\2020\\28_UNICEF\\10_working_repo\\data-etl\\data\\data_normalized\\\n"
     ]
    }
   ],
   "source": [
    "# Scale the raw data and bring the dataframe into long formmat\n",
    "s78_normalized = scaler.normalizer(cleansed_data = s78_cleansed,\n",
    "                        indicator_raw_value = 'value',\n",
    "                        indicator_code = '2.2.2.',\n",
    "                        indicator_name = 'Social insurance coverage',\n",
    "                        cleansed_df_iso2_col = 'CountryIso2',\n",
    "                        crba_final_country_list = country_crba_list,\n",
    "                        crba_final_country_list_iso_col = 'COUNTRY_ISO_2',\n",
    "                        inverted = False,\n",
    "                        non_dim_cols = ['geoAreaCode',\n",
    "                                        'geoAreaName',\n",
    "                                        'timePeriodStart',\n",
    "                                        'source',\n",
    "                                        'value',\n",
    "                                        'footnotes', \n",
    "                                        'Unnamed: 0', \n",
    "                                        'CountryDesc',\n",
    "                                        'CountryIso2',\n",
    "                                        'CountryIso3',\n",
    "                                        'COUNTRY_ISO_2',\n",
    "                                        'COUNTRY_ISO_3',\n",
    "                                        'COUNTRY_NAME',\n",
    "                                        '_merge']\n",
    "                       )\n",
    "\n",
    "# save normalized data\n",
    "save_normalized_data.save_normalized_data(dataframe = s78_normalized,\n",
    "             filename = 'S_78_normalized.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select only those rows/ columns which are relevant \n",
    "\n",
    "* < to do >\n",
    "* Right now, the normalized dataset contains data on ALL subsets (which are defined by the dimension values). Ultimately, there has to be a commitment to one Subset of an indicator \n",
    "* But this filtering to exactly one dimension (i.e. one row per country) will be done later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S-80 SDG Indicator 1.3.1 Proportion of population covered by labour market programmes SI_COV_LMKT \n",
    "### Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following columns are present in the datasets, and this is the number of unique values they have. \n",
      "The column goal has 1 unique values.\n",
      "The column target has 1 unique values.\n",
      "The column indicator has 1 unique values.\n",
      "The column series has 1 unique values.\n",
      "The column seriesDescription has 1 unique values.\n",
      "The column seriesCount has 1 unique values.\n",
      "The column geoAreaCode has 75 unique values.\n",
      "The column geoAreaName has 75 unique values.\n",
      "The column timePeriodStart has 14 unique values.\n",
      "The column value has 340 unique values.\n",
      "The column valueType has 1 unique values.\n",
      "The column time_detail has 1 unique values.\n",
      "The column timeCoverage has 1 unique values.\n",
      "The column upperBound has 1 unique values.\n",
      "The column lowerBound has 1 unique values.\n",
      "The column basePeriod has 1 unique values.\n",
      "The column source has 79 unique values.\n",
      "The column geoInfoUrl has 1 unique values.\n",
      "The column footnotes has 83 unique values.\n",
      "The column attributes.Nature has 1 unique values.\n",
      "The column attributes.Units has 1 unique values.\n",
      "The column dimensions.Reporting Type has 1 unique values.\n",
      "The column dimensions.Quantile has 2 unique values.\n",
      "The raw data has been saved as .xlsx file in: D:\\Documents\\2020\\28_UNICEF\\10_working_repo\\data-etl\\data\\data_raw\\\n"
     ]
    }
   ],
   "source": [
    "# Extract data\n",
    "s80_raw = extract_sdg_api_data(series_code = 'SI_COV_LMKT')\n",
    "\n",
    "# Save raw data\n",
    "save_raw_data.save_raw_data(dataframe = s80_raw,\n",
    "              filename = 'S_80.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The raw data has been saved as .xlsx file in: D:\\Documents\\2020\\28_UNICEF\\10_working_repo\\data-etl\\data\\data_cleansed\\\n"
     ]
    }
   ],
   "source": [
    "# Transform the raw data\n",
    "s80_cleansed = cleanse_sdg_api_data(raw_data = s80_raw,\n",
    "                                   country_list_full = country_full_list,\n",
    "                                   country_list_full_name_col = 'CountryDesc',\n",
    "                                   country_list_full_iso2_col = 'CountryIso2',\n",
    "                                   country_df = country_crba_list,\n",
    "                                   country_df_iso2_col = 'COUNTRY_ISO_2',\n",
    "                                   non_dim_cols = ['value',\n",
    "                                                  'source',\n",
    "                                                  'footnotes',\n",
    "                                                   'attributes.Nature',\n",
    "                                                   'timePeriodStart',\n",
    "                                                  'Unnamed: 0'] \n",
    "                                   )\n",
    "\n",
    "# save cleansed data\n",
    "save_cleansed_data(dataframe = s80_cleansed,\n",
    "             filename = 'S_80_cleansed.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization (scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have a selected a few columns, which will not be regarded as dimensions. These are the remaining columns in the dataset, along with the number of values they take in the dataset.\n",
      "The column goal has 1 unique values.\n",
      "The column target has 1 unique values.\n",
      "The column indicator has 1 unique values.\n",
      "The column series has 1 unique values.\n",
      "The column seriesDescription has 1 unique values.\n",
      "The column seriesCount has 1 unique values.\n",
      "The column valueType has 1 unique values.\n",
      "The column time_detail has 1 unique values.\n",
      "The column timeCoverage has 1 unique values.\n",
      "The column upperBound has 1 unique values.\n",
      "The column lowerBound has 1 unique values.\n",
      "The column basePeriod has 1 unique values.\n",
      "The column geoInfoUrl has 1 unique values.\n",
      "The column attributes.Nature has 1 unique values.\n",
      "The column attributes.Units has 1 unique values.\n",
      "The column dimensions.Reporting Type has 1 unique values.\n",
      "The column dimensions.Quantile has 2 unique values.\n",
      "The total number of subgroups in the dataset is therefore: 2\n",
      "\n",
      " - \n",
      "\n",
      "In the loop we are currently dealing with the subset #1, which has these defining values: \n",
      " \n",
      " Empty DataFrame\n",
      "Columns: [goal, target, indicator, series, seriesDescription, seriesCount, valueType, time_detail, timeCoverage, upperBound, lowerBound, basePeriod, geoInfoUrl, attributes.Nature, attributes.Units, dimensions.Reporting Type, dimensions.Quantile]\n",
      "Index: []\n",
      "\n",
      " The shape of the subset in the cleansed dataset is: (0, 31) \n",
      " \n",
      " \n",
      "Dataframe is empty. There are no values to append.\n",
      " \n",
      " This is the end of loop #1. \n",
      " - \n",
      " \n",
      "\n",
      " - \n",
      "\n",
      "In the loop we are currently dealing with the subset #2, which has these defining values: \n",
      " \n",
      "     goal   target  indicator       series  \\\n",
      "0  ['1']  ['1.3']  ['1.3.1']  SI_COV_LMKT   \n",
      "\n",
      "                                   seriesDescription seriesCount valueType  \\\n",
      "0  [World Bank] Proportion of population covered ...         350     Float   \n",
      "\n",
      "  time_detail timeCoverage upperBound lowerBound basePeriod geoInfoUrl  \\\n",
      "0        None         None       None       None       None       None   \n",
      "\n",
      "  attributes.Nature attributes.Units dimensions.Reporting Type  \\\n",
      "0                CA          PERCENT                         G   \n",
      "\n",
      "  dimensions.Quantile  \n",
      "0                  _T  \n",
      "\n",
      " The shape of the subset in the cleansed dataset is: (60, 31) \n",
      " \n",
      " \n",
      "The distribution of the raw data values this subgroup contains outliers or is too skewed on the upper end. The maximum value to be used for the normalisation is: 3rd quartile or distribution + 1.5 * IQR. It is: 12.870103749999998 \n",
      " See histogram printed below for info. \n",
      "\n",
      "The distribution of the raw data for this subgroup does not contain outliers or is too skewed on the lower end. The minimum value used for the normalisation is the minimum value in the dataset, which is 0.06606. This value corresponds to country: 118    Yemen\n",
      "Name: COUNTRY_NAME, dtype: object \n",
      "\n",
      "\n",
      " This is the distribution of the raw data of the indicator.\n",
      "AxesSubplot(0.125,0.125;0.775x0.755)\n",
      " \n",
      " This is the end of loop #2. \n",
      " - \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\2020\\28_UNICEF\\10_working_repo\\data-etl\\normalize\\scaler.py:234: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleansed_data_subset['SCALED'] = round(10 * (cleansed_data_subset[indicator_raw_value].astype('float') - min_val)/ tot_range, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " - \n",
      "\n",
      "In the loop we are currently dealing with the subset #3, which has these defining values: \n",
      " \n",
      "     goal   target  indicator       series  \\\n",
      "1  ['1']  ['1.3']  ['1.3.1']  SI_COV_LMKT   \n",
      "\n",
      "                                   seriesDescription seriesCount valueType  \\\n",
      "1  [World Bank] Proportion of population covered ...         350     Float   \n",
      "\n",
      "  time_detail timeCoverage upperBound lowerBound basePeriod geoInfoUrl  \\\n",
      "1        None         None       None       None       None       None   \n",
      "\n",
      "  attributes.Nature attributes.Units dimensions.Reporting Type  \\\n",
      "1                CA          PERCENT                         G   \n",
      "\n",
      "  dimensions.Quantile  \n",
      "1                  Q1  \n",
      "\n",
      " The shape of the subset in the cleansed dataset is: (60, 31) \n",
      " \n",
      " \n",
      "The distribution of the raw data values this subgroup contains outliers or is too skewed on the upper end. The maximum value to be used for the normalisation is: 3rd quartile or distribution + 1.5 * IQR. It is: 17.1705975 \n",
      " See histogram printed below for info. \n",
      "\n",
      "The distribution of the raw data for this subgroup does not contain outliers or is too skewed on the lower end. The minimum value used for the normalisation is the minimum value in the dataset, which is 0.05395. This value corresponds to country: 112    Tanzania\n",
      "Name: COUNTRY_NAME, dtype: object \n",
      "\n",
      "\n",
      " This is the distribution of the raw data of the indicator.\n",
      "AxesSubplot(0.125,0.125;0.775x0.755)\n",
      " \n",
      " This is the end of loop #3. \n",
      " - \n",
      " \n",
      "The number of rows of the final dataframe (before the conversion from wide to long format is) is divisible by 195. It is: (390, 39)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\2020\\28_UNICEF\\10_working_repo\\data-etl\\normalize\\scaler.py:234: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleansed_data_subset['SCALED'] = round(10 * (cleansed_data_subset[indicator_raw_value].astype('float') - min_val)/ tot_range, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The raw data has been saved as .xlsx file in: D:\\Documents\\2020\\28_UNICEF\\10_working_repo\\data-etl\\data\\data_normalized\\\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scale the raw data and bring the dataframe into long formmat\n",
    "s80_normalized = scaler.normalizer(cleansed_data = s80_cleansed,\n",
    "                        indicator_raw_value = 'value',\n",
    "                        indicator_code = 'NEW',\n",
    "                        indicator_name = 'Coverage of labour market programmes',\n",
    "                        cleansed_df_iso2_col = 'CountryIso2',\n",
    "                        crba_final_country_list = country_crba_list,\n",
    "                        crba_final_country_list_iso_col = 'COUNTRY_ISO_2',\n",
    "                        inverted = False,\n",
    "                        non_dim_cols = ['geoAreaCode',\n",
    "                                        'geoAreaName',\n",
    "                                        'timePeriodStart',\n",
    "                                        'source',\n",
    "                                        'value',\n",
    "                                        'footnotes', \n",
    "                                        'Unnamed: 0', \n",
    "                                        'CountryDesc',\n",
    "                                        'CountryIso2',\n",
    "                                        'CountryIso3',\n",
    "                                        'COUNTRY_ISO_2',\n",
    "                                        'COUNTRY_ISO_3',\n",
    "                                        'COUNTRY_NAME',\n",
    "                                        '_merge']\n",
    "                       )\n",
    "\n",
    "# save normalized data\n",
    "save_normalized_data.save_normalized_data(dataframe = s80_normalized,\n",
    "             filename = 'S_80_normalized.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select only those rows/ columns which are relevant \n",
    "\n",
    "* < to do >\n",
    "* Right now, the normalized dataset contains data on ALL subsets (which are defined by the dimension values). Ultimately, there has to be a commitment to one Subset of an indicator \n",
    "* But this filtering to exactly one dimension (i.e. one row per country) will be done later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ILO API Sources: Data extraction - cleansing - normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S-50 Gender wage gap (NEW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract raw data\n",
    "s50_raw = extract_ilo_api_data(\n",
    "    api_call_url = 'https://www.ilo.org/sdmx/rest/data/ILO,DF_YI_ALL_EAR_GGAP_OCU_RT/?format=csv&startPeriod=2010-01-01&endPeriod=2020-12-31'\n",
    ")\n",
    "\n",
    "\n",
    "# Save raw data\n",
    "save_raw_data.save_raw_data(dataframe = s50_raw,\n",
    "             filename = 'S_50_raw.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain latest available observation and discard countries that are not in the final CRBA country list \n",
    "s50_cleansed = cleanse_ilo_api_data(raw_data = s50_raw,\n",
    "                                   raw_data_iso_3_col = 'REF_AREA',\n",
    "                                   country_df = country_crba_list,\n",
    "                                   country_df_iso3_col = 'COUNTRY_ISO_3',\n",
    "                                   non_dim_cols = ['OBS_VALUE',\n",
    "                                                   'TIME_PERIOD',\n",
    "                                                   'SOURCE_NOTE',\n",
    "                                                   'INDICATOR_NOTE', \n",
    "                                                   'CURRENCY_NOTE',\n",
    "                                                   'INDICATOR_NOTE']\n",
    "                                   ) # the variable OBS_STATUS varies, but is an attribute, not a dimension. It specifies how an OBS_VALUE was created (A = standard way, E = estimate)\n",
    "\n",
    "# save cleansed data\n",
    "save_cleansed_data(dataframe = s50_cleansed,\n",
    "             filename = 'S_50_cleansed.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization (scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the raw data and bring the dataframe into long formmat\n",
    "s50_normalized = scaler.normalizer(cleansed_data = s50_cleansed,\n",
    "                        indicator_raw_value = 'OBS_VALUE',\n",
    "                        indicator_code = 'NEW',\n",
    "                        indicator_name = 'Gender wage pay gap',\n",
    "                        cleansed_df_iso2_col = 'COUNTRY_ISO_2',\n",
    "                        crba_final_country_list = country_crba_list,\n",
    "                        crba_final_country_list_iso_col = 'COUNTRY_ISO_2',\n",
    "                        inverted = True,\n",
    "                        non_dim_cols = ['TIME_PERIOD', \n",
    "                                        'REF_AREA', \n",
    "                                        'OBS_VALUE', \n",
    "                                        'OBS_STATUS', \n",
    "                                        'COUNTRY_ISO_3', \n",
    "                                        'COUNTRY_NAME', \n",
    "                                        'COUNTRY_ISO_2', \n",
    "                                        'SOURCE_NOTE',\n",
    "                                        'CURRENCY_NOTE',\n",
    "                                        'INDICATOR_NOTE', \n",
    "                                        '_merge']\n",
    "                       )\n",
    "\n",
    "# save normalized data\n",
    "save_normalized_data.save_normalized_data(dataframe = s50_normalized,\n",
    "             filename = 'S_50_normalized.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S-51 Indicator 3.4.2 Average working hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract raw data\n",
    "s51_raw = extract_ilo_api_data(\n",
    "    api_call_url = 'https://www.ilo.org/sdmx/rest/data/ILO,DF_YI_ALL_HOW_TEMP_SEX_ECO_GEO_NB/?format=csv&startPeriod=2010-01-01&endPeriod=2020-12-31'\n",
    ")\n",
    "\n",
    "# Save raw data\n",
    "save_raw_data.save_raw_data(dataframe = s51_raw,\n",
    "             filename = 'S_51_raw.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain latest available observation and discard countries that are not in the final CRBA country list \n",
    "s51_cleansed = cleanse_ilo_api_data(raw_data = s51_raw,\n",
    "                                   raw_data_iso_3_col = 'REF_AREA',\n",
    "                                   country_df = country_crba_list,\n",
    "                                   country_df_iso3_col = 'COUNTRY_ISO_3',\n",
    "                                   non_dim_cols = ['OBS_VALUE',\n",
    "                                                   'TIME_PERIOD',\n",
    "                                                   'SOURCE_NOTE',\n",
    "                                                   'OBS_STATUS',\n",
    "                                                   'SOURCE_NOTE', \n",
    "                                                   'INDICATOR_NOTE', \n",
    "                                                   'CURRENCY_NOTE',\n",
    "                                                   'INDICATOR_NOTE']\n",
    "                                   ) # the variable OBS_STATUS varies, but is an attribute, not a dimension. It specifies how an OBS_VALUE was created (A = standard way, E = estimate)\n",
    "\n",
    "# save cleansed data\n",
    "save_cleansed_data(dataframe = s51_cleansed,\n",
    "             filename = 'S_51_cleansed.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization (scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the raw data and bring the dataframe into long formmat\n",
    "s51_normalized = scaler.normalizer(cleansed_data = s51_cleansed,\n",
    "                        indicator_raw_value = 'OBS_VALUE',\n",
    "                        indicator_code = '3.4.2.',\n",
    "                        indicator_name = 'Average working hours',\n",
    "                        cleansed_df_iso2_col = 'COUNTRY_ISO_2',\n",
    "                        crba_final_country_list = country_crba_list,\n",
    "                        crba_final_country_list_iso_col = 'COUNTRY_ISO_2',\n",
    "                        inverted = True,\n",
    "                        non_dim_cols = ['TIME_PERIOD', \n",
    "                                        'REF_AREA', \n",
    "                                        'OBS_VALUE', \n",
    "                                        'OBS_STATUS', \n",
    "                                        'COUNTRY_ISO_3', \n",
    "                                        'COUNTRY_NAME', \n",
    "                                        'COUNTRY_ISO_2', \n",
    "                                        'SOURCE_NOTE',\n",
    "                                        'CURRENCY_NOTE',\n",
    "                                        'INDICATOR_NOTE', \n",
    "                                        '_merge']\n",
    "                       )\n",
    "\n",
    "# save normalized data\n",
    "save_normalized_data.save_normalized_data(dataframe = s51_normalized,\n",
    "             filename = 'S_51_normalized.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S-53 Indicator NEW  Women in management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract raw data\n",
    "s53_raw = extract_ilo_api_data(\n",
    "    api_call_url = 'https://www.ilo.org/sdmx/rest/data/ILO,DF_SDG_ALL_SDG_0552_OCU_RT/?format=csv&startPeriod=2010-01-01&endPeriod=2020-12-31'\n",
    ")\n",
    "\n",
    "# Save raw data\n",
    "save_raw_data.save_raw_data(dataframe = s53_raw,\n",
    "             filename = 'S_53_raw.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain latest available observation and discard countries that are not in the final CRBA country list \n",
    "s53_cleansed = cleanse_ilo_api_data(raw_data = s53_raw,\n",
    "                                   raw_data_iso_3_col = 'REF_AREA',\n",
    "                                   country_df = country_crba_list,\n",
    "                                   country_df_iso3_col = 'COUNTRY_ISO_3',\n",
    "                                   non_dim_cols = ['OBS_VALUE',\n",
    "                                                   'TIME_PERIOD',\n",
    "                                                   'SOURCE_NOTE',\n",
    "                                                   'OBS_STATUS',\n",
    "                                                   'SOURCE_NOTE', \n",
    "                                                   'INDICATOR_NOTE', \n",
    "                                                   'CURRENCY_NOTE',\n",
    "                                                   'INDICATOR_NOTE']\n",
    "                                   ) # the variable OBS_STATUS varies, but is an attribute, not a dimension. It specifies how an OBS_VALUE was created (A = standard way, E = estimate)\n",
    "\n",
    "# save cleansed data\n",
    "save_cleansed_data(dataframe = s53_cleansed,\n",
    "             filename = 'S_53_cleansed.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization (scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the raw data and bring the dataframe into long formmat\n",
    "s53_normalized = scaler.normalizer(cleansed_data = s53_cleansed,\n",
    "                        indicator_raw_value = 'OBS_VALUE',\n",
    "                        indicator_code = 'NEW',\n",
    "                        indicator_name = 'Women in management',\n",
    "                        cleansed_df_iso2_col = 'COUNTRY_ISO_2',\n",
    "                        crba_final_country_list = country_crba_list,\n",
    "                        crba_final_country_list_iso_col = 'COUNTRY_ISO_2',\n",
    "                        inverted = True,\n",
    "                        non_dim_cols = ['TIME_PERIOD', \n",
    "                                        'REF_AREA', \n",
    "                                        'OBS_VALUE', \n",
    "                                        'OBS_STATUS', \n",
    "                                        'COUNTRY_ISO_3', \n",
    "                                        'COUNTRY_NAME', \n",
    "                                        'COUNTRY_ISO_2', \n",
    "                                        'SOURCE_NOTE',\n",
    "                                        'CURRENCY_NOTE',\n",
    "                                        'INDICATOR_NOTE', \n",
    "                                        '_merge']\n",
    "                       )\n",
    "\n",
    "# save normalized data\n",
    "save_normalized_data.save_normalized_data(dataframe = s53_normalized,\n",
    "             filename = 'S_53_normalized.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s53_normalized.sort_values('value', ascending = False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(pd.DataFrame.sort_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc sources: Data cleansing - normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S-11 Economist Intelligence Unit, Out of the Shadows Index. Legal Framework score only\n",
    "\n",
    "I have decided to not automate this process, because the Excel is so nested that it is very difficult to do that. And it's faster and less erorr prone to retrieve the data manually. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S-60 (prev S-18) Walk Free Foundation.Global Slavery Index. Prevalence of Modern Slavery. Prevalence score only.\n",
    "### Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data\n",
    "url = 'http://downloads.globalslaveryindex.org/ephemeral/FINAL-GSI-2018-DATA-G20-AND-FISHING-1597151668.xlsx'\n",
    "\n",
    "s60_raw = pd.read_excel(io = url, \n",
    "                      sheet_name = 'Global prev, vuln, govt table',\n",
    "                       skiprows = 2)\n",
    "\n",
    "# Save data to raw data folder\n",
    "save_raw_data.save_raw_data(dataframe = s60_raw,\n",
    "             filename = 'S_60.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Filter relevant columns\n",
    "s60_raw = s60_raw[['Country ', # FYI: note the trailing space after Country\n",
    "        'Est. prevalence of population in modern slavery (victims per 1,000 population)']]\n",
    "\n",
    "# The data is from 2018, and is therefore assumed to be reflecting the prevalen in the year 2018\n",
    "s60_cleansed = s60_raw.assign(Year = 2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization (scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the raw data and \n",
    "s60_scaled = s60_cleansed.assign(\n",
    "    scaled = scaler(s60_cleansed['Est. prevalence of population in modern slavery (victims per 1,000 population)']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Left join to counry list to bring it into target format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate data to the\n",
    "s60_final = country_full_list.merge(right = s60_scaled,\n",
    "                 indicator = True,\n",
    "                 how = 'inner',\n",
    "                 left_on = 'CountryDesc',\n",
    "                 right_on = 'Country ',\n",
    "                 validate = 'many_to_one').assign(indicator ='I-18')\n",
    "\n",
    "# Check if some countries got lost during the join\n",
    "print('The number of rows in the raw data set was {}. The number of available datapoints in the final dataframe is {}. This difference is explicable because there were already missing values for the value in sxx_ scaled dataset'.format(\n",
    "    len(s60_scaled), \n",
    "    s60_final['Est. prevalence of population in modern slavery (victims per 1,000 population)'].notna().sum(), \n",
    "    len(s60_scaled) - s60_final['Est. prevalence of population in modern slavery (victims per 1,000 population)'].notna().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <div align=\"center\"> - - - - Archive/ Trash/ dev section (Disregard this section) - - - - - </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ilo_api_data(api_call_url):\n",
    "    \"\"\"Extract raw data from the ILO API\n",
    "\n",
    "    The API can be called without an API key. \n",
    "    The calls can be done with simple https requests, in which a few parameters must be specified. \n",
    "    To understand the structure and build the URL corresponding to your query, please \n",
    "    \n",
    "    * Visit the data explorer https://www.ilo.org/shinyapps/bulkexplorer28/\n",
    "    * There, you can browse the indicator your are looking for and by clicking on the \"i\"-button (top right corner), you can retrieve the indicator code\n",
    "    * With the indicator code, visit the query builder: https://ilostat.ilo.org/data/sdmx-query-builder/\n",
    "    * Insert the code, specify the other parameters and get the https URL\n",
    "\n",
    "    Parameters:\n",
    "    api_call_url (str): URL request for the ILO API. You can build a data query and retrieve the URL as decribed above.\n",
    "    \n",
    "    Returns:\n",
    "    obj: Returns pandas dataframe\n",
    "\n",
    "   \"\"\"\n",
    "    raw_data = pd.read_csv(filepath_or_buffer = api_call_url) \n",
    "    print('The extracted raw data contains the following columns: {}'.format(raw_data.columns))\n",
    "    return(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def cleanse_ilo_api_data(raw_data, raw_data_iso_3_col, country_df, country_df_iso3_col, non_dim_cols):\n",
    "    \"\"\"Cleanse raw data from ILO API\n",
    "\n",
    "    Retrieve only the latest observations for each country-dimension group. NB Dimensions make are required to uniquely identify a row of the dataset and therefore their values define groups, e.g. male vs femeale in a country.\n",
    "    Discard countries that aren't in the CRBA final list of countries\n",
    "\n",
    "    Parameters:\n",
    "    raw_data (obj): Return of function 'extract_sdg_api_data'. Should be a pandas dataframe.\n",
    "    raw_data_iso_2_col (str): Name of the column in 'raw data' frame that contains the ISO2 country codes.\n",
    "    country_df (obj): Dataframe containing the final CRBA list of countries\n",
    "    country_df_iso2_col (str): Column in 'country_df', which contains the country iso2 codes.\n",
    "    non_dim_cols (list): List of columns, which should be excluded from the group-by-statement (that is, if a column is not indicated here, then the maximum value for each of its values will be calculated)\n",
    "\n",
    "    Returns:\n",
    "    obj: Returns pandas dataframe, which only contains the countries meant to be in CRBA and the latest observed value for the indiator.\n",
    "\n",
    "   \"\"\"\n",
    "\n",
    "    # Group by to obtain the latest available value per group, where group is 'col_list_gb'\n",
    "        # Create list of all column to group by\n",
    "    col_list = raw_data.columns.to_list() # list of all columns in the dataframe\n",
    "    non_dim_cols_tuple = tuple(non_dim_cols) # parameters must be passed as list, but the following command requires a tuple\n",
    "    col_list_gb = [e for e in col_list if e not in non_dim_cols_tuple] # exclude timePeriodStart and value, because these one'saren't used for the groupby statement\n",
    "\n",
    "        # Some of the columns contain values which Python interprets as lists (e.g. [8.1]). These make the groupby statement malfunction. Convert them to string\n",
    "    raw_data[col_list_gb] =  raw_data[col_list_gb].astype(str)\n",
    "\n",
    "        # Retreive the latest available data for each group, where group is 'col_list_gb'\n",
    "    grouped_data = raw_data[raw_data['TIME_PERIOD'] == raw_data.groupby(col_list_gb)['TIME_PERIOD'].transform('max')]\n",
    "\n",
    "\n",
    "    # Discard countries that aren't part of the final CRBA master list\n",
    "    grouped_data_iso_filt = grouped_data.merge(\n",
    "        right = country_df,\n",
    "        how = 'right',\n",
    "        left_on = raw_data_iso_3_col,\n",
    "        right_on = country_df_iso3_col,\n",
    "        indicator = True,\n",
    "        validate = 'many_to_one')\n",
    "\n",
    "    # return result\n",
    "    return(grouped_data_iso_filt.sort_values(by = country_df_iso3_col, axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate data to the\n",
    "s62_final = country_full_list.merge(right = s62_scaled,\n",
    "                 indicator = True,\n",
    "                 how = 'outer',\n",
    "                 left_on = 'CountryDesc',\n",
    "                 right_on = 'geoAreaName', # validate = 'many_to_one'\n",
    "                 ).assign(indicator ='I-20')\n",
    "\n",
    "# Check if some countries got lost during the join\n",
    "print('The number of rows in the raw data set was {}. The number of available datapoints in the final dataframe is {}. Therefore, {} rows got lost during the join.'.format(\n",
    "    len(s62_scaled), \n",
    "    s61_final.value.notna().sum(), \n",
    "    len(s62_scaled) - s62_final.value.notna().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate data to the\n",
    "s61_final = country_full_list.merge(right = s61_scaled,\n",
    "                 indicator = True,\n",
    "                 how = 'outer',\n",
    "                 left_on = 'CountryDesc',\n",
    "                 right_on = 'geoAreaName',\n",
    "                 validate = 'many_to_one').assign(indicator ='I-19')\n",
    "\n",
    "# Check if some countries got lost during the join\n",
    "print('The number of rows in the raw data set was {}. The number of available datapoints in the final dataframe is {}. Therefore, {} rows got lost during the join.'.format(\n",
    "    len(s61_scaled), \n",
    "    s61_final.value.notna().sum(), \n",
    "    len(s61_scaled) - s61_final.value.notna().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_dim_cols = ['value',\n",
    "              'source',\n",
    "              'timePeriodStart',\n",
    "              'footnotes',\n",
    "              'Unnamed: 0']\n",
    "\n",
    "debug_s23 = s23_raw[s23_raw['geoAreaName'] == 'Albania'][['geoAreaName',\n",
    "                                                         'timePeriodStart',\n",
    "                                                         'dimensions.Sex',\n",
    "                                                         'dimensions.Activity']]\n",
    "\n",
    "\n",
    "debug_col_list = debug_s23.columns.to_list() # list of all columns in the dataframe\n",
    "debug_non_dim_cols_tuple = tuple(non_dim_cols) # parameters must be passed as list, but the following command requires a tuple\n",
    "debug_col_list_gb = [e for e in debug_col_list if e not in debug_non_dim_cols_tuple] # exclude timePeriodStart and value, because these one'saren't used for the groupby statement\n",
    "\n",
    "    # Some of the columns contain values which Python interprets as lists (e.g. [8.1]). These make the groupby statement malfunction. Convert them to string\n",
    "debug_s23[debug_col_list_gb] =  debug_s23[debug_col_list_gb].astype(str)\n",
    "\n",
    "    # It may be that the time period column is not of type numerical\n",
    "debug_s23 = debug_s23.astype({'timePeriodStart': float})\n",
    "\n",
    "# convert column \"a\" to int64 dtype and \"b\" to complex type\n",
    "# df = df.astype({\"a\": int, \"b\": complex})\n",
    "\n",
    "    # Retreive the latest available data for each group, where group is 'col_list_gb'\n",
    "grouped_data = debug_s23[debug_s23['timePeriodStart'] == debug_s23.groupby(debug_col_list_gb)['timePeriodStart'].transform('max')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def SOME_CLEANSER(raw_data, country_list_full, country_list_full_name_col, country_list_full_iso2_col, country_df, country_df_iso2_col, non_dim_cols):\n",
    "    \"\"\"Cleanse raw data from SDG API\n",
    "\n",
    "    Retrieve only the latest observations for each country-dimension group. NB Dimensions make are part of the composite primary key of the dataset and therefore their values define groups, e.g. male vs femeale in a country.\n",
    "    Discard countries that aren't in the CRBA final list of countries\n",
    "\n",
    "    Parameters:\n",
    "    raw_data (obj): Return of function 'extract_sdg_api_data'. Should be a pandas dataframe.\n",
    "    country_list_full (obj): Dataframe containing the full list of countries with all their country name variations, should be in long format\n",
    "    country_list_full_name_col (str): Column in 'country_list_full', which contains the country names (this column should be the primary key of the dataframe as well)\n",
    "    country_list_full_iso2_col (str): Column in 'country_list_full', which contains the country iso2 codes.\n",
    "    country_df (obj): Dataframe containing the final CRBA list of countries\n",
    "    country_df_iso2_col (str): Column in 'country_df', which contains the country iso2 codes.\n",
    "    non_dim_cols (list): List of columns, which should be excluded from the group by statement (that is, if a column is not indicated here, then the maximum value for each of its values will be calculated)\n",
    "\n",
    "    Returns:\n",
    "    obj: Returns pandas dataframe, which only contains the countries meant to be in CRBA and the latest observed value for the indiator.\n",
    "\n",
    "   \"\"\"\n",
    "\n",
    "    # Group by to obtain the latest available value per group, where group is 'col_list_gb'\n",
    "        # Create list of all column to group by\n",
    "    col_list = raw_data.columns.to_list() # list of all columns in the dataframe\n",
    "    non_dim_cols_tuple = tuple(non_dim_cols) # parameters must be passed as list, but the following command requires a tuple\n",
    "    col_list_gb = [e for e in col_list if e not in non_dim_cols_tuple] # exclude timePeriodStart and value, because these one'saren't used for the groupby statement\n",
    "\n",
    "        # Some of the columns contain values which Python interprets as lists (e.g. [8.1]). These make the groupby statement malfunction. Convert them to string\n",
    "    raw_data[col_list_gb] =  raw_data[col_list_gb].astype(str)\n",
    "\n",
    "        # It may be that the time period column is not of type numerical\n",
    "    raw_data['timePeriodStart'] = pd.to_numeric(raw_data['timePeriodStart'])\n",
    "\n",
    "        # Retreive the latest available data for each group, where group is 'col_list_gb'\n",
    "    grouped_data = raw_data[raw_data[\n",
    "        'timePeriodStart'] == raw_data.groupby(col_list_gb)['timePeriodStart'].transform('max')]\n",
    "\n",
    "    \n",
    "    #print(raw_data.groupby(col_list_gb)['timePeriodStart'].transform('max').head(15))\n",
    "    #print(raw_data['timePeriodStart'] == raw_data.groupby(col_list_gb)['timePeriodStart'].transform('max'))\n",
    "    print(col_list_gb)\n",
    "    # print(grouped_data[grouped_data['geoAreaName'] == 'Angola'].head(50))\n",
    "    \n",
    "    # Discard rows of countries that are not in the master country list\n",
    "        # The raw data only contains country names. Assign ISO codes to these country names\n",
    "    grouped_data_iso = grouped_data.merge(\n",
    "        right = country_list_full,\n",
    "        how = 'left',\n",
    "        left_on = 'geoAreaName',\n",
    "        right_on = country_list_full_name_col,\n",
    "        validate = 'many_to_one')\n",
    "\n",
    "        # Discard countries that aren't part of the final CRBA master list\n",
    "    grouped_data_iso_filt = grouped_data_iso.merge(\n",
    "        right = country_df,\n",
    "        how = 'right',\n",
    "        left_on = country_list_full_iso2_col,\n",
    "        right_on = country_df_iso2_col,\n",
    "        indicator = True,\n",
    "        validate = 'many_to_one')\n",
    "\n",
    "    # Note of Michael: \"Validation: Number of rows should be 195\" --> This is incorrect, the number can also diverge (because the left table may contain one, several or no values for a country-key)\n",
    "\n",
    "    # return result\n",
    "    return(grouped_data_iso_filt.sort_values(by = country_df_iso2_col, axis = 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_numeric(s23_normalized[\n",
    "    (s23_normalized['dimensions.Sex'] == 'BOTHSEX') &\n",
    "    (s23_normalized['dimensions.Activity'] == 'NONAGR') &\n",
    "    (s23_normalized['VALUE_TYPE'] == 'value') \n",
    "    \n",
    "]['value']).hist(bins = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tot_num_subsets = 1\n",
    "for col in s23_cleansed:\n",
    "    print('The column {} has {} unique values.'.format(col, s23_cleansed[col].nunique()))\n",
    "    tot_num_subsets *= s23_cleansed[col].nunique()\n",
    "print(tot_num_subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the raw data and \n",
    "s23_scaled = s23_cleansed.assign(\n",
    "    scaled = scaler(s23_cleansed.value))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are intested in sector = non-agricultural and want the data for both sexes. Filter the other rows out:\n",
    "temp = s23_raw[(s23_raw['dimensions.Sex'] == 'BOTHSEX') & (s23_raw['dimensions.Activity'] == 'NONAGR')]\n",
    "\n",
    "# transform data\n",
    "s23_cleansed = transform_sdg_api_data(raw_data = temp)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate data to the\n",
    "s24_final = country_full_list.merge(right = s24_scaled,\n",
    "                 indicator = True,\n",
    "                 how = 'left',\n",
    "                 left_on = 'CountryDesc',\n",
    "                 right_on = 'geoAreaName',\n",
    "                 validate = 'many_to_one').assign(indicator ='I-14')\n",
    "\n",
    "# Check if some countries got lost during the join\n",
    "print('The number of rows in the raw data set was {}. The number of available datapoints in the final dataframe is {}. Therefore, {} rows got lost during the join.'.format(\n",
    "    len(s24_scaled), \n",
    "    s24_final.value.notna().sum(), \n",
    "    len(s24_scaled) - s24_final.value.notna().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "    \n",
    "def normalizer(cleansed_data, indicator_raw_value, cleansed_df_iso2_col, crba_final_country_list, crba_final_country_list_iso_col, cat_var = False, cat_scoring_type = None, inverted = False, non_dim_cols = None, whisker_factor = 1.5):\n",
    "    \"\"\" Transform cleansed datasets into scaled (normalized) data in long format\n",
    "    \n",
    "    Parameters:\n",
    "    cleansed_data (obj): Raw dataset, pandas dataframe\n",
    "    indicator_raw_value (str): Column containing the actual raw data values \n",
    "    cleansed_df_iso2_col (str): Column which holds the ISO2 code in 'cleansed_data'\n",
    "    crba_final_country_list (obj): Final CRBA country list, should contain country ISO2 (primary key), ISO3 and country name columns.\n",
    "    crba_final_country_list_iso_col (str): Column in crba_final_country_list which holds the ISO2 codes of the countries. \n",
    "    cat_var (bool): is variable of type categorical or numerical?\n",
    "    cat_scoring_type (str): For categorical variables, specify the scoring type (i.e. what values the categorical variable can take)\n",
    "    inverted (bool): If we are dealing with a numerical variable, should the values be inverted?\n",
    "    whisker_factor (float): Whisker factor which is multiplied with the interquartile range (IQR) to define outliers. Default value is 1.5, as is standard in statistics.\n",
    "    non_dim_cols (list): List of columns who are required to uniquely identify a row in the dataset. The normalization will be done for each dimension-subset there is.  \n",
    "\n",
    "    Returns:\n",
    "    obj: Returns numpy array with transformed values\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if an indicator is numeric of categorical\n",
    "    if cat_var == True:\n",
    "        # This is the section dealing with categorical indicators\n",
    "        # There are various scoring_types a categorical variable can take. For each one, different labels apply to the values\n",
    "        # Type 2-1-0\n",
    "        if cat_scoring_type == 'Type 2-1-0':\n",
    "            # Specify the values the raw data can take\n",
    "            conditions = [\n",
    "                (cleansed_data[indicator_raw_value] == 0),\n",
    "                (cleansed_data[indicator_raw_value] == 1),\n",
    "                (cleansed_data[indicator_raw_value] == 2)\n",
    "                ]\n",
    "            \n",
    "            # Specify the normalized values\n",
    "            norm_values = ['No data', '0.00', '10.00']\n",
    "\n",
    "        # Type 2-1\n",
    "        elif cat_scoring_type == 'Type 2-1':\n",
    "            # Specify the values the raw data can take\n",
    "            conditions = [\n",
    "                (cleansed_data[indicator_raw_value] == 1),\n",
    "                (cleansed_data[indicator_raw_value] == 2)\n",
    "                ]\n",
    "            \n",
    "            # Specify the normalized values\n",
    "            norm_values = ['0.00', '10.00']\n",
    "\n",
    "        # Type 3-2-1\n",
    "        elif cat_scoring_type == 'Type 3-2-1':\n",
    "            # Specify the values the raw data can take\n",
    "            conditions = [\n",
    "                (cleansed_data[indicator_raw_value] == 1),\n",
    "                (cleansed_data[indicator_raw_value] == 2),\n",
    "                (cleansed_data[indicator_raw_value] == 3)\n",
    "                ]\n",
    "            \n",
    "            # Specify the normalized values\n",
    "            norm_values = ['0.00', '5.00', '10.00']\n",
    "\n",
    "        # Type 3-2-1-0\n",
    "        elif cat_scoring_type == 'Type 3-2-1-0':\n",
    "            # Specify the values the raw data can take\n",
    "            conditions = [\n",
    "                (cleansed_data[indicator_raw_value] == 0)\n",
    "                (cleansed_data[indicator_raw_value] == 1),\n",
    "                (cleansed_data[indicator_raw_value] == 2),\n",
    "                (cleansed_data[indicator_raw_value] == 3)\n",
    "                ]\n",
    "            \n",
    "            # Specify the normalized values\n",
    "            norm_values = ['No data', '0.00', '5.00', '10.00']            \n",
    "            \n",
    "        # Type 4-3-2-1\n",
    "        elif cat_scoring_type == 'Type 4-3-2-1':\n",
    "            # Specify the values the raw data can take\n",
    "            conditions = [\n",
    "                (cleansed_data[indicator_raw_value] == 1),\n",
    "                (cleansed_data[indicator_raw_value] == 2),\n",
    "                (cleansed_data[indicator_raw_value] == 3),\n",
    "                (cleansed_data[indicator_raw_value] == 4)\n",
    "                ]\n",
    "            \n",
    "            # Specify the normalized values\n",
    "            norm_values = ['0.00', '3.33', '6.67', '10.00']                \n",
    "            \n",
    "            \n",
    "            # create a new column and use np.select to assign values to it using our lists as arguments\n",
    "            cleansed_data['SCALED'] = np.select(conditions, values)\n",
    "\n",
    "        # Type 4-3-2-1-0\n",
    "        elif cat_scoring_type == 'Type 4-3-2-1-0':\n",
    "            # Specify the values the raw data can take\n",
    "            conditions = [\n",
    "                (cleansed_data[indicator_raw_value] == 0)\n",
    "                (cleansed_data[indicator_raw_value] == 1),\n",
    "                (cleansed_data[indicator_raw_value] == 2),\n",
    "                (cleansed_data[indicator_raw_value] == 3),\n",
    "                (cleansed_data[indicator_raw_value] == 4)\n",
    "                ]\n",
    "            \n",
    "            # Specify the normalized values\n",
    "            norm_values = ['No data', '0.00', '3.33', '6.67', '10.00'] \n",
    "\n",
    "\n",
    "        # Type 5-4-3-2-1-0\n",
    "        elif cat_scoring_type == 'Type 5-4-3-2-1-0':\n",
    "            # Specify the values the raw data can take\n",
    "            conditions = [\n",
    "                (cleansed_data[indicator_raw_value] == 0)\n",
    "                (cleansed_data[indicator_raw_value] == 1),\n",
    "                (cleansed_data[indicator_raw_value] == 2),\n",
    "                (cleansed_data[indicator_raw_value] == 3),\n",
    "                (cleansed_data[indicator_raw_value] == 4),\n",
    "                (cleansed_data[indicator_raw_value] == 5)\n",
    "                ]\n",
    "            \n",
    "            # Specify the normalized values\n",
    "            norm_values = ['No data', '0.00', '2.50', '5.00', '7.50', '10.00']\n",
    "            \n",
    "        # Type 7-6-5-4-3-2-1-0\n",
    "        elif cat_scoring_type == 'Type 7-6-5-4-3-2-1-0':\n",
    "            # Specify the values the raw data can take\n",
    "            conditions = [\n",
    "                (cleansed_data[indicator_raw_value] == 0)\n",
    "                (cleansed_data[indicator_raw_value] == 1),\n",
    "                (cleansed_data[indicator_raw_value] == 2),\n",
    "                (cleansed_data[indicator_raw_value] == 3),\n",
    "                (cleansed_data[indicator_raw_value] == 4),\n",
    "                (cleansed_data[indicator_raw_value] == 5),\n",
    "                (cleansed_data[indicator_raw_value] == 6),\n",
    "                (cleansed_data[indicator_raw_value] == 7)\n",
    "                ]\n",
    "            \n",
    "            # Specify the normalized values\n",
    "            norm_values = ['No data', '0.00', '1.67', '3.33', '5.00', '6.67', '8.33', '10.00'] \n",
    "        \n",
    "        else:\n",
    "            raise('The scoring type you have specified does not exist. Make sure your scoring type is listed in the documentation of this fuction.') \n",
    "        \n",
    "        # create a new column and use np.select to assign values to it using our lists as arguments\n",
    "        cleansed_data['SCALED'] = np.select(conditions, values)\n",
    "            \n",
    "    else:\n",
    "        # This is the section dealing with numerical indicators\n",
    "        # If there are dimensions in the dataset (e.g. GENDER), then the normalization of the indicator score \n",
    "        # must be done for all subgroups of all dimensions. For that, we must extract these subsets in the original dataset\n",
    "\n",
    "        # Extract the subsets (defined by the dimensions) of the data\n",
    "            # Exclude columns which are are not a dimension or should not be part in defining a subset (i.e. they aren't part of a unique identifier of a row)\n",
    "        col_list = cleansed_data.columns.to_list() # list of all columns in the dataframe\n",
    "        non_dim_cols_tuple = tuple(non_dim_cols) # parameters must be passed as list, but the following command requires a tuple\n",
    "        non_essential_col = [e for e in col_list if e not in non_dim_cols_tuple]\n",
    "        \n",
    "            # Define parameters to run a loop going through all subsets\n",
    "        length = cleansed_data[non_essential_col].drop_duplicates().shape[0]\n",
    "        width = cleansed_data[non_essential_col].drop_duplicates().shape[1]\n",
    "\n",
    "        # Create empty final dataframe before entering loop. The different subsets will be appended in this DF to obtain the full dataset incl. the scaled variable in the end\n",
    "        cleansed_data_full = pd.DataFrame(columns = cleansed_data.columns.tolist())\n",
    "        \n",
    "        # Loop: i) defining values of subsets to create ii) subsets , iii) calculate the scaled value for all of them and iv) append the subsets in one dataframe\n",
    "        for j in range(1, length):\n",
    "            # i) Create the defining values of the subset\n",
    "            subset = ''\n",
    "            for i in range(width):\n",
    "                subset += '(cleansed_data[cleansed_data[non_essential_col].columns[{}]] == cleansed_data[non_essential_col].drop_duplicates().iloc[{}, {}]) & '.format(i, j, i)\n",
    "            subset = subset.rstrip('& ')\n",
    "            \n",
    "            # ii) Define the subset of the entire dataframe \n",
    "            cleansed_data_subset = cleansed_data[eval(subset)]\n",
    "            \n",
    "            # Log: Inform user what subset the operations are being performed on\n",
    "            print('In the loop we are currently dealing with the subset #{}, which has these defining values: {}'.format(j, cleansed_data_subset[non_essential_col].drop_duplicates()))\n",
    "            print('\\n The shape of the subset is: {}'.format(cleansed_data_subset.shape)) # DELETE\n",
    "            \n",
    "            # Often, the subset will be empty dataframes, because they only consists of NaN values\n",
    "            try: \n",
    "                # iii) Determine basic descriptive statistics of the distribution that are required for the normalization\n",
    "                min_val = np.nanmin(cleansed_data_subset[indicator_raw_value].astype('float'))\n",
    "                max_val = np.nanmax(cleansed_data_subset[indicator_raw_value].astype('float'))\n",
    "                q1 =  cleansed_data_subset[indicator_raw_value].astype('float').quantile(q=0.25)\n",
    "                q2 =  cleansed_data_subset[indicator_raw_value].astype('float').quantile(q=0.50)\n",
    "                q3 =  cleansed_data_subset[indicator_raw_value].astype('float').quantile(q=0.75)                      \n",
    "                iqr = q3 - q1\n",
    "\n",
    "                # Define what max value to use for the normalization            \n",
    "                if max_val > q3 + whisker_factor * iqr:\n",
    "                    max_to_use = q3 + whisker_factor * iqr\n",
    "                    print('The distribution of the raw data values this subgroup contains outliers or is too skewed on the upper end. The maximum value to be used for the normalisation is: 3rd quartile or distribution + {} * IQR. It is: {} \\n See histogram printed below for info. \\n'.format(whisker_factor, max_to_use))\n",
    "                else:\n",
    "                    max_to_use = max_val\n",
    "                    print('The distribution of the raw data for this subgroup does not contain outliers or is too skewed on the upper end. The maximum value used for the normalisation is the maximum value in the dataset, which is {}. This value corresponds to country: {} \\n'.format(max_to_use, cleansed_data[cleansed_data[indicator_raw_value].astype('float') == max_val].COUNTRY_NAME))\n",
    "\n",
    "                # Define what min value to use for the normalization\n",
    "                if min_val < q1 - whisker_factor * iqr:\n",
    "                    min_to_use = q1 - whisker_factor * iqr\n",
    "                    print('The distribution of the raw data values for this subgroup contains outliers or is too skewed on the lower end. The minimum value to be used for the normalisation is 1st quartile or distribution - {} * IQR. It is: {} \\n See histogram printed below for info. \\n'.format(whisker_factor, min_to_use))\n",
    "                else:\n",
    "                    min_to_use = min_val\n",
    "                    print('The distribution of the raw data for this subgroup does not contain outliers or is too skewed on the lower end. The minimum value used for the normalisation is the minimum value in the dataset, which is {}. This value corresponds to country: {} \\n'.format(min_to_use, cleansed_data[cleansed_data[indicator_raw_value].astype('float') == min_val].COUNTRY_NAME))\n",
    "\n",
    "                # If there are outliers or a skewed distribution, print the distribution for the user.\n",
    "                if (min_val < q1 - whisker_factor * iqr) or (max_val > q3 + whisker_factor * iqr):\n",
    "                    print('\\n This is the distribution of the raw data of the indicator.')\n",
    "                    print(cleansed_data_subset[indicator_raw_value].hist(bins = 30))\n",
    "\n",
    "                # Define the value range that is used for the scaling (normalization)\n",
    "                tot_range = max_val - min_val \n",
    "\n",
    "                # Compute the normalized value of the raw data in the column \"SCALED\"\n",
    "                    # Distinguish between indicators, whose value must be inverted\n",
    "                if inverted == True:\n",
    "                    cleansed_data_subset['SCALED'] = round(10 - 10 * (cleansed_data_subset[indicator_raw_value].astype('float') - min_val)/ tot_range, 2)\n",
    "                else:\n",
    "                    cleansed_data_subset['SCALED'] = round(10 * (cleansed_data_subset[indicator_raw_value].astype('float') - min_val)/ tot_range, 2)  \n",
    "\n",
    "                # iv) Append the subset including its scaled value to the final returned dataframe\n",
    "                    # Right join to have all countries from the final crba master list\n",
    "                cleansed_data_subset_rj = cleansed_data_subset.merge(\n",
    "                       right = crba_final_country_list,\n",
    "                      how = 'right',\n",
    "                      left_on = cleansed_df_iso2_col,\n",
    "                      right_on = crba_final_country_list_iso_col,\n",
    "                      indicator = 'RJ_CRBA_FULL_LIST'\n",
    "                  )\n",
    "\n",
    "                    # Append the values\n",
    "                cleansed_data_full = cleansed_data_full.append(cleansed_data_subset_rj)\n",
    "\n",
    "            except:\n",
    "                print('Dataframe is empty. There are no values to append.')\n",
    "                    \n",
    "    # Bring the final dataframe with scaled (normalized) values from wide to long format\n",
    "        # Prepare the melting of the dataframe, by defining what columns remain untouched by the melt\n",
    "    kept_columns = [x for x in cleansed_data_full.columns.tolist() if x not in [indicator_raw_value, 'SCALED']]\n",
    "    \n",
    "        # Bring the dataframe from wide to long format and return it\n",
    "    return(\n",
    "        pd.melt(cleansed_data_full,\n",
    "           id_vars = kept_columns,\n",
    "           value_vars = [indicator_raw_value, 'SCALED'],\n",
    "           var_name = 'VALUE_TYPE')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #DEVELOPMENT; DELETE AFTER\n",
    "    # save_cleansed_data(dataframe = cleansed_data, # DELETE\n",
    "    #         filename = 'dev-cleansed.xlsx.xlsx') # DELETE\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s55_cleansed[eval(subset)]['OBS_VALUE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude columns which are value or should not be part of the composite key (i.e. the unique identifier of a row)\n",
    "col_list = s55_cleansed.columns.to_list() # list of all columns in the dataframe\n",
    "non_essential_col = [e for e in col_list if e not in ('TIME_PERIOD', 'REF_AREA', 'OBS_VALUE', 'COUNTRY_ISO_3', 'COUNTRY_NAME', 'COUNTRY_ISO_2')] \n",
    "\n",
    "# Perpare loop to identify the distint subsets in the datasets\n",
    "length = s55_cleansed[non_essential_col].drop_duplicates().shape[0]\n",
    "width = s55_cleansed[non_essential_col].drop_duplicates().shape[1]\n",
    "\n",
    "# Find the different subsets and their column values\n",
    "# Loop to identify the \"length\" number of subsets:\n",
    "for j in range(length):\n",
    "    # Loop to create the column values defining this subgroup\n",
    "    subset = ''\n",
    "    for i in range(width):\n",
    "        subset += '(s55_cleansed[s55_cleansed[non_essential_col].columns[{}]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[{}, {}]) & '.format(i, j, i)\n",
    "    subset = subset.rstrip('& ')\n",
    "    \n",
    "    # This is where the remaining code should start\n",
    "    print(s55_cleansed[eval(subset)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s55_cleansed = s55_cleansed.assign()\n",
    "\n",
    "# s55_cleansed.loc[s55_cleansed[eval(subset)], 'SCALED'] = 10 \n",
    "\n",
    "# s55_cleansed_subset_1 = s55_cleansed[some_condition.values]\n",
    "\n",
    "#s55_cleansed[some_condition.values]['SCALED'] = 10\n",
    "\n",
    "#s55_cleansed.reindex(columns = ['SCALED'])   \n",
    "\n",
    "# s55_cleansed.assign('SCALED' == )\n",
    "\n",
    "# s55_cleansed[eval(subset)]['SCALED']\n",
    "# s55_cleansed[s55_cleansed[some_condition]['SCALED'] = 10]\n",
    "# s55_cleansed.assign['SCALED' = ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            # Distinguish between indicators, whose value must be inverted\n",
    "            if inverted == True:\n",
    "                temp = cleansed_data.assign(SCALED = round(10 - 10 * (cleansed_data[indicator_raw_value].astype('float') - min_val)/ tot_range, 2))\n",
    "            else:\n",
    "                temp = cleansed_data.assign(SCALED = round(10 * (cleansed_data[indicator_raw_value].astype('float') - min_val)/ tot_range, 2))\n",
    "\n",
    "                \n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # Identify if there are subgroups along the dimensions\n",
    "            # Exclude columns year, country and value\n",
    "        col_list = cleansed_data.columns.to_list() # list of all columns in the dataframe\n",
    "        non_essential_col = [e for e in col_list if e not in (country_col, year_col, indicator_raw_value)] \n",
    "\n",
    "            # Take remaining columns to find out if there are subgroups\n",
    "        length = cleansed_data[non_essential_col].drop_duplicates().shape[0]\n",
    "        width = cleansed_data[non_essential_col].drop_duplicates().shape[1]\n",
    "\n",
    "        # Extract the different subsets in the dataset and do the normalization for each one of them\n",
    "        for j in range(length):\n",
    "            subset = ''\n",
    "            for i in range(width):\n",
    "                subset += '(s55_cleansed[s55_cleansed[non_essential_col].columns[{}]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[{}, {}]) & '.format(i, j, i)\n",
    "                subset = subset.rstrip('& ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s55_cleansed[non_essential_col].drop_duplicates().iloc[0, 22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s55_cleansed[non_essential_col][\n",
    "(s55_cleansed[s55_cleansed[non_essential_col].columns[0]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 0]) & \n",
    "(s55_cleansed[s55_cleansed[non_essential_col].columns[1]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 1]) & \n",
    "(s55_cleansed[s55_cleansed[non_essential_col].columns[2]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 2]) & \n",
    "(s55_cleansed[s55_cleansed[non_essential_col].columns[3]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 3]) & \n",
    "(s55_cleansed[s55_cleansed[non_essential_col].columns[4]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 4]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[5]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 5]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[6]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 6]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[7]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 7]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[8]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 8]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[9]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 9]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[10]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 10]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[11]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 11]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[12]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 12]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[13]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 13]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[14]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 14]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[15]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 15]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[16]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 16]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[17]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 17]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[18]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 18]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[19]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 19]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[20]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 20]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[21]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 21]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[22]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 22]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[23]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 23]) &\n",
    "(s55_cleansed[s55_cleansed[non_essential_col].columns[5]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 5])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list = s55_cleansed.columns.to_list() # list of all columns in the dataframe\n",
    "non_essential_col = [e for e in col_list if e not in ('TIME_PERIOD', 'REF_AREA', 'OBS_VALUE', 'COUNTRY_ISO_3', 'COUNTRY_NAME', 'COUNTRY_ISO_2')] \n",
    "\n",
    "s55_cleansed[non_essential_col].drop_duplicates().shape[1]\n",
    "\n",
    "length = s55_cleansed[non_essential_col].drop_duplicates().shape[0]\n",
    "width = s55_cleansed[non_essential_col].drop_duplicates().shape[1]\n",
    "\n",
    "s55_cleansed['STA_UNIT'] = s55_cleansed['STAT_UNIT'].astype(str)\n",
    "\n",
    "condition = \"s55_cleansed['Dataflow'] == 'UNESCO:SDG4(2.0)'\"\n",
    "\n",
    "condiction_rebuilt = \"s55_cleansed[non_essential_col].columns[0] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 0]\"\n",
    "\n",
    "print(s55_cleansed[non_essential_col].columns[0])\n",
    "\n",
    "# str(s55_cleansed[non_essential_col].columns[0])\n",
    "\n",
    " #s55_cleansed.str(s55_cleansed[non_essential_col].columns[0])\n",
    "\n",
    "# s55_cleansed[s55_cleansed.str(s55_cleansed[non_essential_col].columns[0]) == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 0]]\n",
    "\n",
    "\n",
    "#eval(condition)\n",
    "\n",
    "s55_cleansed[eval(condition)]\n",
    "\n",
    "# s55_cleansed[s55_cleansed[non_essential_col].columns[0] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1]\n",
    "\n",
    "# s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 0]\n",
    "\n",
    "s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 1]\n",
    "\n",
    "\n",
    "'''\n",
    "(s55_cleansed[s55_cleansed[non_essential_col].columns[0]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 0]) & \n",
    "(s55_cleansed[s55_cleansed[non_essential_col].columns[1]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 1]) & \n",
    "(s55_cleansed[s55_cleansed[non_essential_col].columns[2]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 2]) & \n",
    "(s55_cleansed[s55_cleansed[non_essential_col].columns[3]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 3]) & \n",
    "(s55_cleansed[s55_cleansed[non_essential_col].columns[4]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 4]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[5]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 5]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[6]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 6]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[7]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 7]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[8]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 8]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[9]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 9]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[10]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 10]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[11]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 11]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[12]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 12]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[13]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 13]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[14]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 14]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[15]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 15]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[16]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 16]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[17]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 17]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[18]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 18]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[19]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 19]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[20]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 20]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[21]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 21]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[22]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 22]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[23]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 23]) &\n",
    "(s55_cleansed[s55_cleansed[non_essential_col].columns[24]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 24])\n",
    "'''\n",
    "\n",
    "\n",
    "# Find the different subsets and their column values\n",
    "# Loop to identify the \"length\" number of subsets:\n",
    "# for j in range(length):\n",
    "\n",
    "# Loop to create the column values defining this subgroup\n",
    "subset = ''\n",
    "for i in range(width):\n",
    "    subset += '(s55_cleansed[s55_cleansed[non_essential_col].columns[{}]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, {}]) & '.format(i, i)\n",
    "subset = subset.rstrip('& ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in [: width]:\n",
    " #   subset = ''\n",
    " #   subset += '(s55_cleansed[s55_cleansed[non_essential_col].columns[{}]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, {}]'.format(i, i)\n",
    "  #  return(subset)\n",
    "    \n",
    "s55_cleansed[\n",
    "    (s55_cleansed[s55_cleansed[non_essential_col].columns[0]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[2, 0]) & \n",
    "    (s55_cleansed[s55_cleansed[non_essential_col].columns[1]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[2, 1]) & \n",
    "    (s55_cleansed[s55_cleansed[non_essential_col].columns[2]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[2, 2]) & \n",
    "    (s55_cleansed[s55_cleansed[non_essential_col].columns[3]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[2, 3])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Discard rows of countries that are not in the master country list\n",
    "        # The raw data only contains country names. Assign ISO codes to these country names\n",
    "    # grouped_data_iso = grouped_data.merge(\n",
    "    #     right = country_list_full,\n",
    "    #    how = 'left',\n",
    "    #    left_on = 'REF_AREA',\n",
    "    #    right_on = country_list_full_name_col,\n",
    "    #    validate = 'many_to_one')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# cleanse raw data\n",
    "s55_cleansed = cleanse_unesco_api_data(\n",
    "    uis_data = s55_raw,\n",
    "    country_df = country_crba_list,\n",
    "    country_df_iso2_col = 'COUNTRY_ISO_2',\n",
    "    columns = ['EDU_CAT', 'SEX', 'AGE'],\n",
    "    most_recent_only = True)\n",
    "\n",
    "# save cleansed data\n",
    "save_cleansed_data(dataframe = s55_cleansed,\n",
    "             filename = 'S_55_cleansed.xlsx')\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanse_sdg_api_data(raw_data):\n",
    "    \"\"\"Transform raw data from sdg API source to distil relevant data \n",
    "\n",
    "    Discard columns and aggregate data to retrieve the latest value for all countries in a given dataframe for a given SDG API series.\n",
    "\n",
    "    Parameters:\n",
    "    raw_data (obj): Return of function 'extract_sdg_api_data'. Should be a pandas dataframe \n",
    "    \n",
    "    Returns:\n",
    "    obj: Returns pandas dataframe\n",
    "\n",
    "   \"\"\"\n",
    "    # Extract relevant columns\n",
    "    stage_data = raw_data[['geoAreaCode',\n",
    "            'geoAreaName',\n",
    "            'timePeriodStart',\n",
    "            'value']]\n",
    "    \n",
    "    # Get the latest value for each country of the series\n",
    "    return(stage_data[stage_data['timePeriodStart'] == stage_data.groupby('geoAreaName')['timePeriodStart'].transform('max')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw data contains data on BOTHSEX, MALE AND FEMALE, limit ourseleves to BOTHSEXES\n",
    "temp = s24_raw[s24_raw['dimensions.Sex'] == 'BOTHSEX']\n",
    "\n",
    "# Transform the raw data\n",
    "s24_cleansed = cleanse_sdg_api_data(raw_data = temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s24_raw[s24_raw['geoAreaName'] == 'Burkina Faso']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #\n",
    "    #\n",
    "    #\n",
    "    '''\n",
    "    # Discard rows of countries that are not in the master country list\n",
    "        # The raw data only contains country names. Assign ISO codes to these country names    \n",
    "    raw_data_iso = raw_data.merge(\n",
    "        right = country_list_full,\n",
    "        how = 'left',\n",
    "        left_on = 'geoAreaName',\n",
    "        right_on = country_list_full_name_col,\n",
    "        validate = 'many_to_one')\n",
    "    \n",
    "        # Discard countries that aren't part of the final CRBA master list\n",
    "    contry_filt_data = raw_data_iso.merge(\n",
    "        right = country_df,\n",
    "        how = 'right', \n",
    "        left_on = country_list_full_iso2_col,\n",
    "        right_on = country_df_iso2_col,\n",
    "        indicator = True,\n",
    "        validate = 'many_to_one')\n",
    "         \n",
    "    # Get the latest value for each country of the series\n",
    "    col_list = contry_filt_data.columns.to_list() # list of all columns in the dataframe\n",
    "    col_list_gb = [e for e in col_list if e not in ('value', 'timePeriodStart')] # exclude timePeriodStart and value, because these one'saren't used for the groupby statement\n",
    "    \n",
    "    # Some of the columns contain values which Python interprets as lists (e.g. [8.1]). These make the groupby statement malfunction. Convert them to string\n",
    "    contry_filt_data[col_list_gb] =  contry_filt_data[col_list_gb].astype(str)\n",
    "    \n",
    "    # Retreive the latest available data for each group, where group is 'col_list_gb'\n",
    "    latest_year_data = contry_filt_data[contry_filt_data['timePeriodStart'] == contry_filt_data.groupby(col_list_gb)['timePeriodStart'].transform('max')]\n",
    "    \n",
    "    # return result\n",
    "    return(latest_year_data.sort_values(by = country_df_iso2_col, axis = 0))\n",
    "    ''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(latest_year_data.COUNTRY_ISO_2.describe())\n",
    "    print(contry_filt_data.COUNTRY_ISO_2.describe())\n",
    "    \n",
    "# Include a confirmation whether all of this went well\n",
    "    print(raw_data.shape)\n",
    "    print(country_df.shape)\n",
    "    print(raw_data_iso.shape)\n",
    "    print(contry_filt_data.shape)\n",
    "    print(contry_filt_data[contry_filt_data._merge == 'left_only'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s24_raw.geoAreaName.describe()\n",
    "country_full_list.CountryDesc.describe()\n",
    "\n",
    "col_list = s24_raw.columns.to_list() # list of all columns in the dataframe\n",
    "print(col_list)\n",
    "col_list_gb = [e for e in col_list if e not in ('value', 'timePeriodStart')]\n",
    "print(col_list_gb)\n",
    "\n",
    "# s24_raw['timePeriodStart'] = \n",
    "\n",
    "# s24_raw.groupby(by = 'geoAreaName')['timePeriodStart'].max()\n",
    "\n",
    "s24_raw[col_list_gb] = s24_raw[col_list_gb].astype(str)\n",
    "\n",
    "s24_raw[s24_raw['timePeriodStart'] == s24_raw.groupby(col_list_gb)['timePeriodStart'].transform('max')]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    ''' No this codeblock will probably be discarded\n",
    "    # Extract relevant columns\n",
    "    stage_data = raw_data[['geoAreaCode',\n",
    "            'geoAreaName',\n",
    "            'timePeriodStart',\n",
    "            'value']]\n",
    "    '''\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "THIS IS ONLY ABOUT EXTRACTING THE LATEST VALUE AND ABOUT FILTERING OUT COUNTRIES WHO SHOULDN'T BE IN THERE\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "\n",
    "    # Discard unnecessay columns\n",
    "    columns_kept = ['REF_AREA', 'TIME_PERIOD','OBS_VALUE'] + columns\n",
    "    uis_data = uis_data[columns_kept]\n",
    "    \n",
    "    if most_recent_only == True:\n",
    "        # Retrieve the most up-to-date number for each country\n",
    "        uis_data = uis_data[uis_data['TIME_PERIOD']==uis_data.groupby('REF_AREA')['TIME_PERIOD'].transform('max')]\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    # Discard rows of countries that are not in the master country list\n",
    "    cleansed_final = uis_data.merge(\n",
    "        right = country_df,\n",
    "        how = 'right', \n",
    "        left_on = 'REF_AREA',\n",
    "        right_on = country_df_iso2_col,\n",
    "        validate = 'one_to_one').assign(indicator = 'I-15')\n",
    "    \n",
    "    return(cleansed_final.sort_values(by = country_df_iso2_col, axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Left join to counry list to bring it into target format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate data to the\n",
    "s56_final = country_iso_list.merge(right = s56_scaled,\n",
    "                 how = 'left',\n",
    "                 left_on = 'CountryIso2',\n",
    "                 right_on = 'REF_AREA',\n",
    "                 validate = 'many_to_one').assign(indicator ='I-16')\n",
    "\n",
    "# Check if some countries got lost during the join\n",
    "print('The number of rows in the raw data set was {}. The number of available datapoints in the final dataframe is {}. Therefore, {} rows got lost during the join.'.format(\n",
    "    len(s56_scaled), \n",
    "    s56_final.OBS_VALUE.notna().sum(), \n",
    "    len(s56_scaled) - s56_final.OBS_VALUE.notna().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Left join to counry list to bring it into target format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left join data to the target format\n",
    "s55_final = country_iso_list.merge(right = s55_scaled,\n",
    "                 how = 'left',\n",
    "                 left_on = 'CountryIso2',\n",
    "                 right_on = 'REF_AREA',\n",
    "                 validate = 'one_to_many').assign(indicator = 'I-15')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generic functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_raw_data(dataframe, filename, output_path = data_sources_raw):\n",
    "    \"\"\"Save raw data \n",
    "\n",
    "    Parameters:\n",
    "    dataframe (obj): Pandas dataframe to be stored\n",
    "    filename (string): The way you would like o anem your file. Must include the extension (for example .xlsx)\n",
    "    output_path (string): Folder where data is stored\n",
    "\n",
    "    Returns:\n",
    "    obj: \n",
    "\n",
    "   \"\"\"\n",
    "    dataframe.to_excel(output_path + filename)\n",
    "    print('The raw data has been saved as .xlsx file in: ' + output_path)\n",
    "\n",
    "    \n",
    "def scaler(raw_dataframe, indicator_raw_value, cat_var = False, cat_scoring_type = None, inverted = False, whisker_factor = 1.5):\n",
    "    \"\"\" Transform raw data into scaled (normalized) data in long format\n",
    "    \n",
    "    Parameters:\n",
    "    raw_dataframe (obj): Raw dataset, pandas dataframe\n",
    "    indicator_raw_value (str): Column containing the actual raw data values \n",
    "    cat_var (bool): is variable of type categorical or numerical?\n",
    "    cat_scoring_type (str): For categorical variables, specify the scoring type (i.e. what values the categorical variable can take)\n",
    "    inverted (bool): If we are dealing with a numerical variable, should the values be inverted?\n",
    "    whisker_factor (float): \n",
    "\n",
    "    Returns:\n",
    "    obj: Returns numpy array with transformed values\n",
    "    \"\"\"\n",
    "    # Check if an indicator is numeric of categorical\n",
    "    if cat_var == True:\n",
    "        # This is the section dealing with categorical indicators\n",
    "        # There are various scoring_types a categorical variable can take. For each one, different labels apply to the values\n",
    "        # Type 2-1-0\n",
    "        if cat_scoring_type == 'Type 2-1-0':\n",
    "            # Specify the values the raw data can take\n",
    "            conditions = [\n",
    "                (raw_dataframe[indicator_raw_value] == 0),\n",
    "                (raw_dataframe[indicator_raw_value] == 1),\n",
    "                (raw_dataframe[indicator_raw_value] == 2)\n",
    "                ]\n",
    "            \n",
    "            # Specify the normalized values\n",
    "            norm_values = ['No data', '0.00', '10.00']\n",
    "\n",
    "        # Type 2-1\n",
    "        elif cat_scoring_type == 'Type 2-1':\n",
    "            # Specify the values the raw data can take\n",
    "            conditions = [\n",
    "                (raw_dataframe[indicator_raw_value] == 1),\n",
    "                (raw_dataframe[indicator_raw_value] == 2)\n",
    "                ]\n",
    "            \n",
    "            # Specify the normalized values\n",
    "            norm_values = ['0.00', '10.00']\n",
    "\n",
    "        # Type 3-2-1\n",
    "        elif cat_scoring_type == 'Type 3-2-1':\n",
    "            # Specify the values the raw data can take\n",
    "            conditions = [\n",
    "                (raw_dataframe[indicator_raw_value] == 1),\n",
    "                (raw_dataframe[indicator_raw_value] == 2),\n",
    "                (raw_dataframe[indicator_raw_value] == 3)\n",
    "                ]\n",
    "            \n",
    "            # Specify the normalized values\n",
    "            norm_values = ['0.00', '5.00', '10.00']\n",
    "\n",
    "        # Type 3-2-1-0\n",
    "        elif cat_scoring_type == 'Type 3-2-1-0':\n",
    "            # Specify the values the raw data can take\n",
    "            conditions = [\n",
    "                (raw_dataframe[indicator_raw_value] == 0)\n",
    "                (raw_dataframe[indicator_raw_value] == 1),\n",
    "                (raw_dataframe[indicator_raw_value] == 2),\n",
    "                (raw_dataframe[indicator_raw_value] == 3)\n",
    "                ]\n",
    "            \n",
    "            # Specify the normalized values\n",
    "            norm_values = ['No data', '0.00', '5.00', '10.00']            \n",
    "            \n",
    "        # Type 4-3-2-1\n",
    "        elif cat_scoring_type == 'Type 4-3-2-1':\n",
    "            # Specify the values the raw data can take\n",
    "            conditions = [\n",
    "                (raw_dataframe[indicator_raw_value] == 1),\n",
    "                (raw_dataframe[indicator_raw_value] == 2),\n",
    "                (raw_dataframe[indicator_raw_value] == 3),\n",
    "                (raw_dataframe[indicator_raw_value] == 4)\n",
    "                ]\n",
    "            \n",
    "            # Specify the normalized values\n",
    "            norm_values = ['0.00', '3.33', '6.67', '10.00']                \n",
    "            \n",
    "            \n",
    "            # create a new column and use np.select to assign values to it using our lists as arguments\n",
    "            raw_dataframe['SCALED'] = np.select(conditions, values)\n",
    "\n",
    "        # Type 4-3-2-1-0\n",
    "        elif cat_scoring_type == 'Type 4-3-2-1-0':\n",
    "            # Specify the values the raw data can take\n",
    "            conditions = [\n",
    "                (raw_dataframe[indicator_raw_value] == 0)\n",
    "                (raw_dataframe[indicator_raw_value] == 1),\n",
    "                (raw_dataframe[indicator_raw_value] == 2),\n",
    "                (raw_dataframe[indicator_raw_value] == 3),\n",
    "                (raw_dataframe[indicator_raw_value] == 4)\n",
    "                ]\n",
    "            \n",
    "            # Specify the normalized values\n",
    "            norm_values = ['No data', '0.00', '3.33', '6.67', '10.00'] \n",
    "\n",
    "\n",
    "        # Type 5-4-3-2-1-0\n",
    "        elif cat_scoring_type == 'Type 5-4-3-2-1-0':\n",
    "            # Specify the values the raw data can take\n",
    "            conditions = [\n",
    "                (raw_dataframe[indicator_raw_value] == 0)\n",
    "                (raw_dataframe[indicator_raw_value] == 1),\n",
    "                (raw_dataframe[indicator_raw_value] == 2),\n",
    "                (raw_dataframe[indicator_raw_value] == 3),\n",
    "                (raw_dataframe[indicator_raw_value] == 4),\n",
    "                (raw_dataframe[indicator_raw_value] == 5)\n",
    "                ]\n",
    "            \n",
    "            # Specify the normalized values\n",
    "            norm_values = ['No data', '0.00', '2.50', '5.00', '7.50', '10.00']\n",
    "            \n",
    "        # Type 7-6-5-4-3-2-1-0\n",
    "        elif cat_scoring_type == 'Type 7-6-5-4-3-2-1-0':\n",
    "            # Specify the values the raw data can take\n",
    "            conditions = [\n",
    "                (raw_dataframe[indicator_raw_value] == 0)\n",
    "                (raw_dataframe[indicator_raw_value] == 1),\n",
    "                (raw_dataframe[indicator_raw_value] == 2),\n",
    "                (raw_dataframe[indicator_raw_value] == 3),\n",
    "                (raw_dataframe[indicator_raw_value] == 4),\n",
    "                (raw_dataframe[indicator_raw_value] == 5),\n",
    "                (raw_dataframe[indicator_raw_value] == 6),\n",
    "                (raw_dataframe[indicator_raw_value] == 7)\n",
    "                ]\n",
    "            \n",
    "            # Specify the normalized values\n",
    "            norm_values = ['No data', '0.00', '1.67', '3.33', '5.00', '6.67', '8.33', '10.00'] \n",
    "        \n",
    "        else:\n",
    "            raise('The scoring type you have specified does not exist. Make sure your scoring type is listed in the documentation of this fuction.') \n",
    "        \n",
    "        # create a new column and use np.select to assign values to it using our lists as arguments\n",
    "        raw_dataframe['SCALED'] = np.select(conditions, values)\n",
    "            \n",
    "    else:\n",
    "        # This is the section dealing with numerical indicators\n",
    "\n",
    "        # Determine basic descriptive statistics of the distribution\n",
    "        min_val = min(raw_dataframe[indicator_raw_value].astype('float'))\n",
    "        max_val = max(raw_dataframe[indicator_raw_value].astype('float'))\n",
    "        q1 =  raw_dataframe[indicator_raw_value].astype('float').quantile(q=0.25)\n",
    "        q2 =  raw_dataframe[indicator_raw_value].astype('float').quantile(q=0.50)\n",
    "        q3 =  raw_dataframe[indicator_raw_value].astype('float').quantile(q=0.75)                      \n",
    "        iqr = q3 - q1\n",
    "        \n",
    "        # Define what max value to use for the normalization            \n",
    "        if max_val > q3 + whisker_factor * iqr:\n",
    "            max_to_use = q3 + whisker_factor * iqr\n",
    "            print('The distribution of the raw data values contains outliers or is too skewed on the upper end. The maximum value to be used for the normalisation is the the 3rd quartile added to IQR multiplied by the {}. It is: {} \\n See histogram printed below for info. \\n'.format(whisker_factor, max_to_use))\n",
    "        else:\n",
    "            max_to_use = max_val\n",
    "            print('The distribution of the raw data does not contain outliers on the upper end. The maximum value used for the normalisation is the maximum value in the dataset, which is {}. \\n'.format(max_to_use))\n",
    "                      \n",
    "        # Define what min value to use for the normalization\n",
    "        if min_val < q1 - whisker_factor * iqr:\n",
    "            min_to_use = q1 - whisker_factor * iqr\n",
    "            print('The distribution of the raw data values contains outliers or is too skewed on the lower end. The minimum value to be used for the normalisation is the the 3rd quartile added to IQR multiplied by the the {}. It is: {} \\n See histogram printed below for info. \\n'.format(whisker_factor, min_to_use))\n",
    "        else:\n",
    "            min_to_use = min_val\n",
    "            print('The distribution of the raw data does not contain outliers or is too skewed on the lower end. The minimum value used for the normalisation is the maximum value in the dataset, which is {}. \\n'.format(min_to_use))\n",
    "        \n",
    "        # If there are outliers or a skewed distribution, print the distribution for the user.\n",
    "        if (min_val < q1 - whisker_factor * iqr) or (max_val > q3 + whisker_factor * iqr):\n",
    "            print('\\n This is the distribution of the raw data of the indicator.')\n",
    "            print(raw_dataframe[indicator_raw_value].hist(bins = 30))\n",
    "                       \n",
    "        # Define the value range that is used for the scaling (normalization)\n",
    "        tot_range = max_val - min_val \n",
    "        \n",
    "        # Distinguish between indicators, whose value must be inverted\n",
    "        if inverted == True:\n",
    "            temp = raw_dataframe.assign(SCALED = round(10 - 10 * (raw_dataframe[indicator_raw_value].astype('float') - min_val)/ tot_range, 2))\n",
    "        else:\n",
    "            temp = raw_dataframe.assign(SCALED = round(10 * (raw_dataframe[indicator_raw_value].astype('float') - min_val)/ tot_range, 2))\n",
    "        \n",
    "        # bring dataframe from wide to long format\n",
    "        # Prepare the melting of the dataframe, by defining what columns remain untouched by the melt\n",
    "    kept_columns = [x for x in raw_dataframe.columns.tolist() if x not in [indicator_raw_value, 'SCALED']]\n",
    "\n",
    "    return(\n",
    "        pd.melt(temp,\n",
    "           id_vars = kept_columns,\n",
    "           value_vars = [indicator_raw_value, 'SCALED'],\n",
    "           var_name = 'VALUE_TYPE')\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNESCO API specific functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_unesco_api_data(stat_unit, unit, edu_level, subs_key, start_period = 2010, end_period = 2020):\n",
    "    \"\"\"Extract raw data from the UNESCO API\n",
    "\n",
    "    The API can be called with a API key, which you must generate by creating an account. \n",
    "    The calls can be done with simple https requests, in which many parameter must be specified (e.g. indicator code, sexes, age, ...). TO understand the structure, please check the query builder https://apiportal.uis.unesco.org/query-builder\n",
    "\n",
    "    Parameters:\n",
    "    stat_unit (string): Indicatiro code, I suggest you check the query builder https://apiportal.uis.unesco.org/query-builder to retrieve the right code\n",
    "    unit (string): Specify the unnit of measure, e.g. 'PT' for percentage total\n",
    "    edu_level (string): Choose educational level, L2 = lower secondary, L3 = upper secondary age\n",
    "    subs_key (string): Your API key. You must create one. Visit https://apiportal.uis.unesco.org/getting-started for more info\n",
    "    start_period (int): Year which is the beginning of series for which you want to collect data\n",
    "    end_period (ing): Year which is the end of series for which you want to collect data\n",
    "\n",
    "    Returns:\n",
    "    obj: Returns pandas dataframe\n",
    "\n",
    "   \"\"\"\n",
    "    url = 'https://api.uis.unesco.org/sdmx/data/SDG4/{sunit}.{umeasure}.{elevel}._T._T.SCH_AGE_GROUP._T.INST_T._Z._T._Z._Z._Z._T._T._Z._Z._Z.?startPeriod={syear}&endPeriod={eyear}&format=csv-sdmx&subscription-key={skey}'.format(\n",
    "        sunit = stat_unit,\n",
    "        umeasure = unit,\n",
    "        elevel = edu_level,\n",
    "        syear = start_period,\n",
    "        eyear = end_period,\n",
    "        skey = subs_key)\n",
    "    return(pd.read_csv(filepath_or_buffer = url, \n",
    "                      sep = ',',))\n",
    "\n",
    "def cleanse_unesco_api_data(uis_data, country_iso2_list, columns, most_recent_only = True):\n",
    "    \"\"\"< To do >\n",
    "\n",
    "    Parameters:\n",
    "    s15_raw (obj): Should be return of function s55_extract or s56_extract\n",
    "    country_iso2_list (array): A numpy array of 2-character country codes.\n",
    "    columns: List of the dimensions (i.e. columns) that are you want to keep for the indicator beyond the main necessary ones\n",
    "\n",
    "    Returns:\n",
    "    obj: Returns pandas dataframe with the relevant data of the indicator\n",
    "\n",
    "   \"\"\"\n",
    "    # Discard unnecessay columns\n",
    "    columns_kept = ['REF_AREA', 'TIME_PERIOD','OBS_VALUE'] + columns\n",
    "    uis_data = uis_data[columns_kept]\n",
    "    \n",
    "    # Discard rows of countries that are in the master country list\n",
    "    uis_data = uis_data[uis_data.REF_AREA.isin(country_iso2_list)]\n",
    "    \n",
    "    if most_recent_only == True:\n",
    "        # Retrieve the most up-to-date number for each country\n",
    "        uis_data = uis_data[uis_data['TIME_PERIOD']==uis_data.groupby('REF_AREA')['TIME_PERIOD'].transform('max')]\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return(uis_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SDG API specific functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sdg_api_data(series_code):\n",
    "    \"\"\"Extract raw data for a series \n",
    "\n",
    "    Extract data on the specifies series as json and flatten it out into a pandas dataframe\n",
    "    To retrieve the data from the API of the SDG indicators, you must proceed as follows: \n",
    "    \n",
    "    * Find the seriesCode of the indicator for which you want to retrieve data: I recommend visiting https://unstats.un.org/sdgs/indicators/database/ and browse the indicator you want and the series code will be indicated there\n",
    "    * Visit https://unstats.un.org/SDGAPI/swagger/#!/Series/V1SdgSeriesDataGet\n",
    "    * Expand the tab GET /v1/sdg/Series/Data\n",
    "    * Type in the seriesCode, this will provide you with the link to the JSON\n",
    "    * Important: The results will be on shwon on various pages if the dataset is too large. That is why it is a good idea to set a pageSize value large enough to accomodate all data in just one page to not have to iterate over various pages\n",
    "\n",
    "        \n",
    "    Parameters:\n",
    "    series (string): Code of the series, which can be found by browsing for the relevant series here:https://unstats.un.org/sdgs/indicators/database?indicator=16.2.2\n",
    "\n",
    "    Returns:\n",
    "    obj: Returns pandas dataframe\n",
    "\n",
    "   \"\"\"\n",
    "    url = 'https://unstats.un.org/SDGAPI/v1/sdg/Series/Data?seriesCode={}&pageSize=999999999'.format(series_code)\n",
    "    return(pd.json_normalize(requests.get(url).json()['data']))    \n",
    "\n",
    "\n",
    "def cleanse_sdg_api_data(raw_data):\n",
    "    \"\"\"Transform raw data from sdg API source to distil relevant data \n",
    "\n",
    "    Discard columns and aggregate data to retrieve the latest value for all countries in a given dataframe for a given SDG API series.\n",
    "\n",
    "    Parameters:\n",
    "    raw_data (obj): Return of function 'extract_sdg_api_data'. Should be a pandas dataframe \n",
    "    \n",
    "    Returns:\n",
    "    obj: Returns pandas dataframe\n",
    "\n",
    "   \"\"\"\n",
    "    # Extract relevant columns\n",
    "    stage_data = raw_data[['geoAreaCode',\n",
    "            'geoAreaName',\n",
    "            'timePeriodStart',\n",
    "            'value']]\n",
    "    # Get the latest value for each country of the series\n",
    "    return(stage_data[stage_data['timePeriodStart'] == stage_data.groupby('geoAreaName')['timePeriodStart'].transform('max')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def scaler(raw_dataframe, indicator_value, cat_var = False, inverted = False, up_lim = None, low_lim = None):\n",
    "    \"\"\" Transform raw data into scaled (normalized) data in long format\n",
    "    \n",
    "    Parameters:\n",
    "    raw_dataframe (obj): Raw dataset, pandas dataframe\n",
    "    indicator_value (str): Column containing the actual raw data values \n",
    "    cat_var (bool): is variable of type categorical or numerical?\n",
    "    inverted (bool): If we are dealing with a numerical variable, should the values be inverted?\n",
    "    up_lim_req (float): For numerical variables, is there an artificial upper limit \n",
    "    low_lim_req (bool): For numerical variables, is there an artificial upper limit\n",
    "\n",
    "    Returns:\n",
    "    obj: Returns numpy array with transformed values\n",
    "    \"\"\"\n",
    "    # Check if an indicator is numeric of categorical\n",
    "    if cat_var == True:\n",
    "        pass\n",
    "    else:\n",
    "        # This is the section dealing with numerical indicators\n",
    "        # Define the range of the indicator values (some take their max/min value, sometimes an upper/lower limit)\n",
    "        if up_lim == None:\n",
    "            max_val = max(raw_dataframe[indicator_value].astype('float'))\n",
    "        else:\n",
    "            max_val = up_lim\n",
    "        if low_lim == None:\n",
    "            min_val = min(raw_dataframe[indicator_value].astype('float'))\n",
    "        else:\n",
    "            min_val = low_lim \n",
    "        \n",
    "        # Define the value range that is used for the scaling (normalization)\n",
    "        tot_range = max_val - min_val \n",
    "        \n",
    "        # Distinguish between indicators, whose value must be inverted\n",
    "        if inverted == True:\n",
    "            temp = raw_dataframe.assign(SCALED = round(10 - 10 * (raw_dataframe[indicator_value].astype('float') - min_val)/ tot_range, 2))\n",
    "        else:\n",
    "            temp = raw_dataframe.assign(SCALED = round(10 * (raw_dataframe[indicator_value].astype('float') - min_val)/ tot_range, 2))\n",
    "        \n",
    "        # bring dataframe from wide to long format\n",
    "        # Prepare the melting of the dataframe, by defining what columns remain\n",
    "        kept_columns = [x for x in s55_cleansed.columns.tolist() if x not in [indicator_value, 'SCALED']]\n",
    "        \n",
    "        return(\n",
    "            pd.melt(temp,\n",
    "               id_vars = ['REF_AREA', 'TIME_PERIOD', 'EDU_CAT', 'SEX'],\n",
    "               value_vars = [indicator_value, 'SCALED'],\n",
    "               var_name = 'VALUE_TYPE')\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(s55_final) == len(s55_scaled) + len(country_iso_list) - len(s55_scaled)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaler(indicator, cat_var = False, inverted = False, up_lim = None, low_lim = None):\n",
    "    \"\"\" Transform raw data into scaled (normalized) data in long format\n",
    "    \n",
    "    Parameters:\n",
    "    indicator (obj): numpy array containing the indicator data\n",
    "    cat_var (bool): is variable of type categorical or numerical?\n",
    "    inverted (bool): If we are dealing with a numerical variable, should the values be inverted?\n",
    "    up_lim_req (float): For numerical variables, is there an artificial upper limit \n",
    "    low_lim_req (bool): For numerical variables, is there an artificial upper limit\n",
    "\n",
    "    Returns:\n",
    "    obj: Returns numpy array with transformed values\n",
    "    \"\"\"\n",
    "    # Check if an indicator is numeric of categorical\n",
    "    if cat_var == True:\n",
    "        pass\n",
    "    else:\n",
    "        # This is the section dealing with numerical indicators\n",
    "        # Define the range of the indicator values (some take their max/min value, sometimes an upper/lower limit)\n",
    "        if up_lim == None:\n",
    "            max_val = max(indicator.astype('float'))\n",
    "        else:\n",
    "            max_val = up_lim\n",
    "        if low_lim == None:\n",
    "            min_val = min(indicator.astype('float'))\n",
    "        else:\n",
    "            min_val = low_lim \n",
    "        \n",
    "        # Define the value range that is used for the scaling (normalization)\n",
    "        tot_range = max_val - min_val \n",
    "        \n",
    "        # Distinguish between indicators, whose value must be inverted\n",
    "        if inverted == False:\n",
    "            return(round(10 * (indicator.astype('float') - min_val)/ tot_range, 2))\n",
    "        else:\n",
    "            return(round(10 - 10 * (indicator.astype('float') - min_val)/ tot_range, 2))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_unesco_api_data(uis_data, country_code_list):\n",
    "    \"\"\"< To do >\n",
    "\n",
    "    Parameters:\n",
    "    s15_raw (obj): Should be return of function s55_extract or s56_extract\n",
    "    country_code_list (array): A numpy array of 2-character country codes.  \n",
    "\n",
    "    Returns:\n",
    "    obj: Returns pandas dataframe with the relevant data of the indicator\n",
    "\n",
    "   \"\"\"\n",
    "    # Discard unnecessay columns\n",
    "    uis_data = uis_data[['REF_AREA',\n",
    "         'TIME_PERIOD',\n",
    "         'OBS_VALUE']]\n",
    "    \n",
    "    # Discard unnecessary rows\n",
    "    uis_data = uis_data[uis_data.REF_AREA.isin(country_code_list)]\n",
    "    \n",
    "    # Retrieve the most up-to-date number for each country\n",
    "    uis_data = uis_data[uis_data['TIME_PERIOD']==uis_data.groupby('REF_AREA')['TIME_PERIOD'].transform('max')]\n",
    "    \n",
    "    return(uis_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to create the country list (which has then been manually modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the list which contains all variations of names a country can have. \n",
    "# NB: CountryDesc is the key, so there are duplicate ISO codes in this list \n",
    "\n",
    "# This is the list of James, which already contains the \"normal\" and the \"long\" name of countries\n",
    "daniele_list_1 = pd.read_csv(filepath_or_buffer = \"D:/Documents/2020/28_UNICEF/9_misc/compare_UNICEF_Transmonee.csv\", \n",
    "                      sep = ',',\n",
    "                      usecols = ['Country', 'Alpha2', 'Alpha3'])\n",
    "daniele_list_1.columns = ['CountryDesc', 'CountryIso2', 'CountryIso3']\n",
    "\n",
    "# This is the list of Daniele \n",
    "daniele_list_2 = pd.read_csv(filepath_or_buffer = \"D:/Documents/2020/28_UNICEF/9_misc/compare_UNICEF_Transmonee.csv\", \n",
    "                      sep = ',',\n",
    "                      usecols = ['CountryDesc', 'CountryIso2', 'CountryIso3'])\n",
    "\n",
    "# This is the list of me\n",
    "my_list = pd.read_excel(\n",
    "    io = \"D:/Documents/2020/28_UNICEF/9_misc/CountryMaster_v3.xlsx\", \n",
    "    )[[#'CountryID',\n",
    "   'CountryDesc',\n",
    "   'CountryIso2',\n",
    "   'CountryIso3']]\n",
    "\n",
    "# Create list with all possible variations of country names\n",
    "country_full_list = daniele_list_1.append(daniele_list_2).append(my_list).drop_duplicates()\n",
    "    \n",
    "# Create list which has countryIso as unique key\n",
    "country_iso_list = country_full_list.drop_duplicates(subset = 'CountryIso2')\n",
    "\n",
    "# Save files \n",
    "country_full_list.to_excel(cwd + 'all_countrynames_list.xlsx')\n",
    "country_full_list.to_csv(cwd + 'all_countrynames_list.csv',\n",
    "                        sep = \";\") # there were a few empty fields for ISO2 and ISO3 which I filled manually. \n",
    "# Furthermore I have deleted the following from the dataset: Channel Islands, Serbia and Montenegro, United nations, Unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the list which contains all variations of names a country can have. \n",
    "# NB: CountryDesc is the key, so there are duplicate ISO codes in this list \n",
    "\n",
    "# This is the list of James, which already contains the \"normal\" and the \"long\" name of countries\n",
    "daniele_list_1 = pd.read_csv(filepath_or_buffer = \"D:/Documents/2020/28_UNICEF/9_misc/compare_UNICEF_Transmonee.csv\", \n",
    "                      sep = ',',\n",
    "                      usecols = ['Country', 'Alpha2', 'Alpha3'])\n",
    "daniele_list_1.columns = ['CountryDesc', 'CountryIso2', 'CountryIso3']\n",
    "\n",
    "# This is the list of Daniele \n",
    "daniele_list_2 = pd.read_csv(filepath_or_buffer = \"D:/Documents/2020/28_UNICEF/9_misc/compare_UNICEF_Transmonee.csv\", \n",
    "                      sep = ',',\n",
    "                      usecols = ['CountryDesc', 'CountryIso2', 'CountryIso3'])\n",
    "\n",
    "# This is the list of me\n",
    "my_list = pd.read_excel(\n",
    "    io = \"D:/Documents/2020/28_UNICEF/9_misc/CountryMaster_v3.xlsx\", \n",
    "    )[[#'CountryID',\n",
    "   'CountryDesc',\n",
    "   'CountryIso2',\n",
    "   'CountryIso3']]\n",
    "\n",
    "# Create list with all possible variations of country names\n",
    "country_full_list = daniele_list_1.append(daniele_list_2).append(my_list).drop_duplicates()\n",
    "    \n",
    "# Create list which has countryIso as unique key\n",
    "country_iso_list = country_full_list.drop_duplicates(subset = 'CountryIso2')\n",
    "\n",
    "# Save files \n",
    "country_full_list.to_excel(cwd + 'all_countrynames_list.xlsx')\n",
    "country_full_list.to_csv(cwd + 'all_countrynames_list.csv',\n",
    "                        sep = \";\") # there were a few empty fields for ISO2 and ISO3 which I filled manually. \n",
    "# Furthermore I have deleted the following from the dataset: Channel Islands, Serbia and Montenegro, United nations, Unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "country_full_list = pd.read_excel(\n",
    "    io = \"D:/Documents/2020/28_UNICEF/9_misc/CountryMaster_v3.xlsx\", \n",
    "    )[[#'CountryID',\n",
    "   'CountryDesc',\n",
    "   'CountryIso2',\n",
    "   'CountryIso3']].drop_duplicates()\n",
    "\n",
    "country_iso_list = country_full_list.drop_duplicates(subset = 'CountryIso2')\n",
    "\n",
    "list_Daniele = \n",
    "\n",
    "temp = pd.read_csv(filepath_or_buffer = \"D:/Documents/2020/28_UNICEF/9_misc/compare_UNICEF_Transmonee.csv\", \n",
    "                      sep = ',',\n",
    "                      usecols = ['CountryDesc', 'CountryIso2', 'CountryIso3'])\n",
    "list_Daniele.columns = ['CountryDesc', 'CountryIso2', 'CountryIso3']\n",
    "\n",
    "list_Daniele = list_Daniele.append(temp)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the master country-reference lit\n",
    "'''\n",
    "url = 'https://pkgstore.datahub.io/core/country-list/data_csv/data/d7c9d7cfb42cb69f4422dec222dbbaa8/data_csv.csv'\n",
    "\n",
    "country_list = pd.read_csv(\n",
    "    filepath_or_buffer = url, \n",
    "    sep = ',',\n",
    "    )\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducing Maplecroft transformation (dev section)\n",
    "\n",
    "Using indicator 3.1.1. as a first example (quantitative variable): Child labour rate (5-17) \n",
    "\n",
    "The maplecroft sheet has each indicator undergo a set of tranformations: \n",
    "\n",
    "1. Indicator raw data = basis\n",
    "2. Normalize indicator raw data with certain transformation (e.g. 0.3.1.1. becomes 0.3.1.1_s)\n",
    "3. Aggregate various normalized indicator data into a cluster value (e.g. 0.3.1.1 and 0.3.1.2. become 0.3_s)\n",
    "\n",
    "In the following I am reproducing these steps, each of which contains many different nested if-then statements\n",
    "\n",
    "## 1 Indicator raw data\n",
    "\n",
    "s. above\n",
    "\n",
    "## 2 Normalize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_val = min(s24_transformed.value.astype('float'))\n",
    "max_val = max(s24_transformed.value.astype('float'))\n",
    "tot_range = max_val - min_val \n",
    "\n",
    "if indicator = \n",
    "\n",
    "# Do the actual normalization\n",
    "s24_transformed.assign(norm_val = (s24_transformed.value.astype('float') - min_val)/ tot_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "These if-statements must be included: \n",
    "\n",
    "* If categorical --> Something I still have to figure out\n",
    "    * Else: If upper limit required --> take upper limit value.\n",
    "        * Else: Take max value of series. And then: \n",
    "            * If raw data value > max to use\n",
    "            \n",
    "* Categorical vs numerical\n",
    "    * Inverted\n",
    "        * Is raw data point (for a country) available? --> \n",
    "            * Max to use as upper limit or max value\n",
    "                * Min to use as lower limit of min value\n",
    "                    *  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "sklearn.preprocessing.MinMaxScaler().fit_transform(s24_transformed.value)\n",
    "\n",
    "\n",
    "\n",
    "# Create x, where x the 'scores' column's values as floats\n",
    "x = s24_transformed.value.astype(float)\n",
    "\n",
    "temp = x.reshape(1, -1)\n",
    "\n",
    "# Create a minimum and maximum processor object\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "# Create an object to transform the data to fit minmax processor\n",
    "x_scaled = min_max_scaler.fit_transform(temp)\n",
    "\n",
    "# Run the normalizer on the dataframe\n",
    "df_normalized = pd.DataFrame(x_scaled)\n",
    "\n",
    "\n",
    "# s56_transformed.head()\n",
    "\n",
    "# The maplecroft sheet has each indicator undergo the following tran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the relevant columns\n",
    "temp = s24_raw[['geoAreaCode',\n",
    "            'geoAreaName',\n",
    "            'timePeriodStart',\n",
    "            'value']]\n",
    "\n",
    "# Extract the latest value for each country\n",
    "temp[temp['timePeriodStart'] == temp.groupby('geoAreaName')['timePeriodStart'].transform('max')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the raw data\n",
    "s61_transformed = transform_sdg_api_data(raw_data = s61_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data and directly flatten it out into a pandas dataframe\n",
    "s24_raw = extract_sdg_api_data(series = 'VC_HTF_DETV')\n",
    "\n",
    "# Save data to raw data folder\n",
    "save_raw_data(dataframe = s24_raw,\n",
    "             filename = 'S_24.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data and directly flatten it out into a pandas dataframe\n",
    "url = 'https://unstats.un.org/SDGAPI/v1/sdg/Series/Data?seriesCode=VC_HTF_DETV&pageSize=999999999' # choose page size large enough to make sure data all fits in one page\n",
    "s24_raw = pd.json_normalize(requests.get(url).json()['data'])\n",
    "\n",
    "# Save data to raw data folder\n",
    "save_raw_data(dataframe = s24_raw,\n",
    "             filename = 'S_24.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract raw data\n",
    "s56_raw = s56_extract(subs_key = '460ab272abdd43c892bb59c218c22c09',\n",
    "                      start_period = '2010',\n",
    "                      end_period = '2020')\n",
    "\n",
    "# Save raw data\n",
    "save_raw_data(dataframe = s56_raw,\n",
    "             filename = 'S_56.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s56_extract(subs_key, start_period = 2010, end_period = 2020):\n",
    "    \"\"\"Extract raw data of source S-15 from the UNESCO api UIS\n",
    "\n",
    "    Parameters:\n",
    "    subs_key (string): Your API key. You must create one. Visit https://apiportal.uis.unesco.org/getting-started for more info\n",
    "    start_period (int): Year which is the beginning of series for which you want to collect data\n",
    "    end_period (ing): Year which is the end of series for which you want to collect data\n",
    "\n",
    "    Returns:\n",
    "    obj: Returns pandas dataframe and saves raw data to specified output_path\n",
    "\n",
    "   \"\"\"\n",
    "    url = 'https://api.uis.unesco.org/sdmx/data/SDG4/ROFST.PT.L3._T._T.SCH_AGE_GROUP._T.INST_T._Z._T._Z._Z._Z._T._T._Z._Z._Z.?startPeriod={syear}&endPeriod={eyear}&format=csv-sdmx&subscription-key={skey}'.format(syear = start_period,\n",
    "        eyear = end_period,\n",
    "        skey = subs_key)\n",
    "    return(pd.read_csv(filepath_or_buffer = url, \n",
    "                      sep = ',',))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data\n",
    "s55_raw = s55_extract(subs_key = '460ab272abdd43c892bb59c218c22c09',\n",
    "                      start_period = '2010',\n",
    "                      end_period = '2020')\n",
    "\n",
    "# Save raw data\n",
    "save_raw_data(dataframe = s55_raw,\n",
    "             filename = 'S_55.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s55_extract(subs_key, start_period = 2010, end_period = 2020):\n",
    "    \"\"\"Extract raw data from the UNESCO API\n",
    "\n",
    "    The API can be called with a API key, which you must generate by creating an account. \n",
    "    The calls can be done with simple https requests, in which many parameter must be specified (e.g. indicator code, sexes, age, ...). TO understand the structure, please check the query builder https://apiportal.uis.unesco.org/query-builder\n",
    "\n",
    "    Parameters:\n",
    "    subs_key (string): Your API key. You must create one. Visit https://apiportal.uis.unesco.org/getting-started for more info\n",
    "    start_period (int): Year which is the beginning of series for which you want to collect data\n",
    "    end_period (ing): Year which is the end of series for which you want to collect data\n",
    "\n",
    "    Returns:\n",
    "    obj: Returns pandas dataframe and saves raw data to specified output_path\n",
    "\n",
    "   \"\"\"\n",
    "    url = 'https://api.uis.unesco.org/sdmx/data/SDG4/ROFST.PT.L2._T._T.SCH_AGE_GROUP._T.INST_T._Z._T._Z._Z._Z._T._T._Z._Z._Z.?startPeriod={syear}&endPeriod={eyear}&format=csv-sdmx&subscription-key={skey}'.format(syear = start_period,\n",
    "        eyear = end_period,\n",
    "        skey = subs_key)\n",
    "    return(pd.read_csv(filepath_or_buffer = url, \n",
    "                      sep = ',',))\n",
    "\n",
    "https://api.uis.unesco.org/sdmx/dataflow/all/all/latest?subscription-key=<your api key>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the relevant columns\n",
    "temp = s61_raw[['geoAreaCode',\n",
    "            'geoAreaName',\n",
    "            'timePeriodStart',\n",
    "            'value']]\n",
    "\n",
    "# Extract the latest value for each country\n",
    "temp = temp[temp['timePeriodStart'] == temp.groupby('geoAreaName')['timePeriodStart'].transform('max')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract the relevant columns\n",
    "temp = temp[['geoAreaCode',\n",
    "            'geoAreaName',\n",
    "            'timePeriodStart',\n",
    "            'value']]\n",
    "\n",
    "# Extract the latest value for each country\n",
    "temp = temp[temp['timePeriodStart'] == temp.groupby('geoAreaName')['timePeriodStart'].transform('max')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming ILO data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rows to get both sexes\n",
    "temp  = s23_raw_ilo[s23_raw_ilo['SEX'] == 'SEX_T']\n",
    "\n",
    "# Extract the relevant columns\n",
    "temp = temp[['REF_AREA',\n",
    "            'TIME_PERIOD',\n",
    "            'OBS_VALUE']]\n",
    "\n",
    "# Extract the latest value for each country\n",
    "temp = temp[temp['TIME_PERIOD'] == temp.groupby('REF_AREA')['TIME_PERIOD'].transform('max')]\n",
    "\n",
    "# Save file to examine it\n",
    "temp.to_excel(cwd + \"\\data_transformed\\S_23_SDG_ILO.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting data through ILO API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data from ILO API. The link is generated from this website: https://ilostat.ilo.org/data/sdmx-query-builder/ . \n",
    "# The code of the indicator I have from here: \n",
    "url = 'https://www.ilo.org/sdmx/rest/data/ILO,DF_SDG_ALL_SDG_A831_SEX_RT/?format=csv&startPeriod=2010-01-01&endPeriod=2020-12-31'\n",
    "\n",
    "s23_raw_ilo = pd.read_csv(filepath_or_buffer = url, \n",
    "                      sep = ',',)\n",
    "\n",
    "# Save data to raw data folder\n",
    "s23_raw_ilo.to_excel(dataframe = s23_raw,\n",
    "             filename = 'S_23.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s56_save_raw(s56_data, output_path = data_sources_raw + '\\S_56.xlsx'):\n",
    "    \"\"\"Save raw data of source s56 as .xlsx file\n",
    "\n",
    "    Parameters:\n",
    "    s15_data (obj): Raw data to be be saved. Must be ob class pandas.DataFrame\n",
    "    output_path (string): Output path of where to save the raw data if save_raw is set to true\n",
    "\n",
    "    Returns:\n",
    "    obj: Returns pandas dataframe and saves raw data to specified output_path\n",
    "\n",
    "   \"\"\"\n",
    "    s56_data.to_excel(output_path)\n",
    "    print('The raw data has been saved as .xlsx file in: ' + output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def s55_save_raw(s55_data, output_path = data_sources_raw + '\\S_55.xlsx'):\n",
    "    \"\"\"Save raw data of source s55 as .xlsx file\n",
    "\n",
    "    Parameters:\n",
    "    s15_data (obj): Raw data to be be saved. Must be ob class pandas.DataFrame\n",
    "    output_path (string): Output path of where to save the raw data if save_raw is set to true\n",
    "\n",
    "    Returns:\n",
    "    obj: Returns pandas dataframe and saves raw data to specified output_path\n",
    "\n",
    "   \"\"\"\n",
    "    s55_data.to_excel(output_path)\n",
    "    print('The raw data has been saved as .xlsx file in: ' + output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the most up-to-date number for each country\n",
    "s16_data[s16_data['TIME_PERIOD']==s16_data.groupby('REF_AREA')['TIME_PERIOD'].transform('max')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discard unnecessay columns\n",
    "s16_data = s16_data[['REF_AREA',\n",
    "         'TIME_PERIOD',\n",
    "         'OBS_VALUE']]\n",
    "\n",
    "# Discard unnecessary rows\n",
    "s16_data = s16_data[s16_data.REF_AREA.isin(counry_list.Code)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters of the API query\n",
    "subs_key = '460ab272abdd43c892bb59c218c22c09' # must generate an KEY on the UIS page\n",
    "start_period = '2010' # define from which year you wan to retrieve data \n",
    "end_period = '2020' # define until what you want to retreive data\n",
    "\n",
    "# Build URL query based on parameters defined\n",
    "url = 'https://api.uis.unesco.org/sdmx/data/SDG4/ROFST.PT.L3._T._T.SCH_AGE_GROUP._T.INST_T._Z._T._Z._Z._Z._T._T._Z._Z._Z.?startPeriod={syear}&endPeriod={eyear}&format=csv-sdmx&subscription-key={skey}'.format(syear = start_period,\n",
    "eyear = end_period,\n",
    "skey = subs_key)\n",
    "\n",
    "# Extract the data\n",
    "s16_data = pd.read_csv(filepath_or_buffer = url, \n",
    "                      sep = ',',\n",
    "                      )\n",
    "\n",
    "# Save data in raw version as excel file # TO do: Create a versioning\n",
    "s16_data.to_excel(data_sources_raw + '\\S_16.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the most up-to-date number for each country\n",
    "s15_data[s15_data['TIME_PERIOD']==s15_data.groupby('REF_AREA')['TIME_PERIOD'].transform('max')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discard unnecessay columns\n",
    "s15_data = s15_data[['REF_AREA',\n",
    "         'TIME_PERIOD',\n",
    "         'OBS_VALUE']]\n",
    "\n",
    "# Discard unnecessary rows\n",
    "s15_data = s15_data[s15_data.REF_AREA.isin(counry_list.Code)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters of the API query\n",
    "subs_key = '460ab272abdd43c892bb59c218c22c09' # must generate an KEY on the UIS page\n",
    "start_period = '2010' # define from which year you wan to retrieve data \n",
    "end_period = '2020' # define until what you want to retreive data\n",
    "\n",
    "# Build URL query based on parameters defined\n",
    "url = 'https://api.uis.unesco.org/sdmx/data/SDG4/ROFST.PT.L2._T._T.SCH_AGE_GROUP._T.INST_T._Z._T._Z._Z._Z._T._T._Z._Z._Z.?startPeriod={syear}&endPeriod={eyear}&format=csv-sdmx&subscription-key={skey}'.format(syear = start_period,\n",
    "eyear = end_period,\n",
    "skey = subs_key)\n",
    "\n",
    "# Extract the data\n",
    "s15_data = pd.read_csv(filepath_or_buffer = url, \n",
    "                      sep = ',',\n",
    "                      )\n",
    "\n",
    "# Save data in raw version as excel file # TO do: Create a versioning\n",
    "s15_data.to_excel(data_sources_raw + '\\S_15.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s15_data.REF_AREA.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see if ILO data can be retrieved from their SDMX data warehouse\n",
    "url = \"https://www.ilo.org/sdmx/rest/data/ILO,DF_YI_ALL_IFL_4IEM_SEX_ECO_IFL_RT/?format=jsondata&startPeriod=2010-01-01&endPeriod=2020-12-31\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "my_json = json.loads(response.text)\n",
    "\n",
    "type(my_json)\n",
    "\n",
    "# response = json.loads(response.text)\n",
    "\n",
    "print(my_json)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out the UN SDG Goals API\n",
    "\n",
    "url = \"https://unstats.un.org/SDGAPI/v1/sdg/Indicator/VC_HTF_DETV%20/Series/List\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "my_json = json.loads(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S-11\n",
    "url = 'https://outoftheshadows.eiu.com/wp-content/uploads/2019/05/OOSI_Out_of_the_shadows_index_60-countries_May2019.xlsm'\n",
    "\n",
    "data = pd.read_excel(url,\n",
    "                    sheet_name = 'Dataset',\n",
    "                    skiprows = 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s15_data.groupby(['REF_AREA'], sort = False)[['TIME_PERIOD', 'OBS_VALUE']].max()\n",
    "# s15_data.groupby(['REF_AREA'], sort = False).max()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "274.633px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
