{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TransMonEE Indicators - API (Helix and UIS) sources populated in Data Dictionary - LEGACY DATA ETL\n",
    "In this notebook, I will loop along these indicators for extraction and transformation.\n",
    "\n",
    "**Numbers (updated with dictionary v5):**\n",
    "* Total in WH: 396 indicators\n",
    "* Helix sources (34 indicators)\n",
    "* UIS sources (149 indicators)\n",
    "* WB sources (25 indicators)\n",
    "* UN-SDG sources (7 indicators)\n",
    "* WHO sources (4 indicators)\n",
    "* ILO source (1 indicator)\n",
    "* Legacy Excel file (323 indicators - 176 in SDMX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import (\n",
    "    get_API_code_address_etc,\n",
    "    api_request,\n",
    "    get_codelist_API_legacy,\n",
    "    get_units_codelist,\n",
    "    data_reader,\n",
    ")\n",
    "from sdmx.sdmx_struc import SdmxJsonStruct\n",
    "from extraction import legacy\n",
    "from extraction.wrap_api_address import wrap_api_address\n",
    "from transformation.destination import Destination\n",
    "from transformation.dataflow import Dataflow, define_maps\n",
    "from data_in.legacy_data.prepare_mapping import (\n",
    "    match_country_name,\n",
    "    match_indicator_names,\n",
    ")\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TransMonEE countries list - Country ISO codes\n",
    "##### Countries list is taken from dataflow TransMonEE in UNICEF Warehouse (requested by Eduard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNICEFâ€™s REST API data endpoint for TransMonEE Dataflow\n",
    "url_endpoint = (\n",
    "    \"https://sdmx.data.unicef.org/ws/public/sdmxapi/rest/data/ECARO,TRANSMONEE,1.0/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# address and parameters for dataflow structure request\n",
    "api_address = url_endpoint + \"all\"\n",
    "api_params = {\"format\": \"sdmx-json\", \"detail\": \"structureOnly\"}\n",
    "# API dataflow structure request\n",
    "d_flow_struc = api_request(api_address, api_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Country ISO-alpha codes (2, 3 letters) and M49 code (UNSD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate class SdmxJsonStruct with TMEE dataflow\n",
    "TmeeSdmxStruc = SdmxJsonStruct(d_flow_struc.json())\n",
    "# TransMonEE three-letters country codes are taken from its dataflow\n",
    "country_codes_3 = TmeeSdmxStruc.get_all_country_codes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# country codes equivalence from excel file in repo root\n",
    "country_codes_file = \"./data_in/all_countrynames_list.xlsx\"\n",
    "country_codes_df = pd.read_excel(country_codes_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# country M49 codes from UNSD (reference: https://unstats.un.org/unsd/methodology/m49/)\n",
    "m49_codes_file = \"./data_in/UNSD_Methodology.csv\"\n",
    "m49_codes_df = pd.read_csv(m49_codes_file, dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add country M49 codes to country_codes_df\n",
    "country_codes_m49_df = country_codes_df.merge(\n",
    "    m49_codes_df[[\"M49 Code\", \"ISO-alpha3 Code\"]].rename(\n",
    "        columns={\"ISO-alpha3 Code\": \"CountryIso3\"}\n",
    "    ),\n",
    "    on=\"CountryIso3\",\n",
    "    how=\"left\",\n",
    "    sort=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map TMEE country_codes (three-letters/two-letters equivalence)\n",
    "country_codes_2 = [\n",
    "    country_codes_df.CountryIso2[country_codes_df.CountryIso3 == elem].values\n",
    "    for elem in country_codes_3.values()\n",
    "]\n",
    "# country names are repeated in the list, and I want the uniques only\n",
    "# numpy unique sorts the array, I take an extra step to retrieve the original order\n",
    "uni_sort, sort_ind = np.unique(np.concatenate(country_codes_2), return_index=True)\n",
    "country_codes_2 = uni_sort[np.argsort(sort_ind)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map TMEE contry_codes (three-letters/M49 equivalence)\n",
    "country_codes_m49 = [\n",
    "    m49_codes_df[\"M49 Code\"][m49_codes_df[\"ISO-alpha3 Code\"] == elem].values\n",
    "    for elem in country_codes_3.values()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# country codes mapping dictionary (two-letters/three-letters)\n",
    "country_map = {k: v for k, v in zip(country_codes_2, country_codes_3.values())}\n",
    "# country codes mapping dictionary (M49, removing zeros to the left / ISO three-letters)\n",
    "country_map_m49 = {\n",
    "    k: v\n",
    "    for k, v in zip(\n",
    "        np.concatenate(country_codes_m49).astype(int).astype(str),\n",
    "        country_codes_3.values(),\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write dictionaries in py file to use it during transformations\n",
    "path_file = \"./transformation/country_map.py\"\n",
    "f = open(path_file, \"w\")\n",
    "f.write(\"country_map = \" + repr(country_map) + \"\\n\")\n",
    "f.write(\"country_map_49 = \" + repr(country_map_m49) + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Country names as defined in Legacy Data\n",
    "Required to identify rows with data by the Excel Legacy parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of countries as reported in legacy data\n",
    "legacy_country_list = [\n",
    "    \"albania\",\n",
    "    \"armenia\",\n",
    "    \"azerbaijan\",\n",
    "    \"belarus\",\n",
    "    \"bosnia and herzegovina\",\n",
    "    \"bulgaria\",\n",
    "    \"croatia\",\n",
    "    \"czech republic\",\n",
    "    \"estonia\",\n",
    "    \"georgia\",\n",
    "    \"hungary\",\n",
    "    \"kazakhstan\",\n",
    "    \"kyrgyzstan\",\n",
    "    \"latvia\",\n",
    "    \"lithuania\",\n",
    "    \"moldova\",\n",
    "    \"montenegro\",\n",
    "    \"poland\",\n",
    "    \"romania\",\n",
    "    \"russian federation\",\n",
    "    \"serbia\",\n",
    "    \"slovakia\",\n",
    "    \"slovenia\",\n",
    "    \"tajikistan\",\n",
    "    \"north macedonia\",\n",
    "    \"turkey\",\n",
    "    \"turkmenistan\",\n",
    "    \"ukraine\",\n",
    "    \"uzbekistan\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match country names (legacy data) with country names used in TMEE\n",
    "# build dictionary with country names (legacy data) and country codes in TMEE\n",
    "legacy_country_codes_3 = {}\n",
    "for name in legacy_country_list:\n",
    "    match = match_country_name(name, list(country_codes_3.keys()))\n",
    "    legacy_country_codes_3[name] = country_codes_3[match]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write legacy_country_codes_3 dictionary in py file to use during transformations\n",
    "path_file = \"./transformation/country_names_map.py\"\n",
    "f = open(path_file, \"w\")\n",
    "f.write(\"country_names_map = \" + repr(legacy_country_codes_3) + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map seasons (Education Legacy Data) to reference year\n",
    "Use first year as SDMX `TIME_PERIOD`, retain season as `COVERAGE_TIME` attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# year range\n",
    "year_ini = 1950\n",
    "year_end = 2050\n",
    "# seasons list\n",
    "season_str = [\n",
    "    str(a) + \"/\" + str(b)[2:]\n",
    "    for a, b in zip(range(year_ini, year_end + 1), range(year_ini + 1, year_end + 2))\n",
    "]\n",
    "# season to year mapping dictionary\n",
    "seasons_map = {k: str(v) for k, v in zip(season_str, range(year_ini, year_end + 1))}\n",
    "# write dictionary in py file to use it during transformations\n",
    "path_file = \"./transformation/seasons_map.py\"\n",
    "f = open(path_file, \"w\")\n",
    "f.write(\"seasons_map = \" + repr(seasons_map) + \"\\n\")\n",
    "f.write(\"season_str = \" + repr(season_str) + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Legacy data Extraction\n",
    "##### Source file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to legacy excel file\n",
    "source_path_nsi = \"./data_in/legacy_data/\"\n",
    "source_file = \"TM-2020-EN-December.xlsx\"\n",
    "full_path = source_path_nsi + source_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Raw data destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw data destination path\n",
    "raw_path = \"./data_out/data_raw/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parse all legacy indicators from Excel `source_file`\n",
    "There's one spreadsheet with contents and 6 spreadsheets containing data.\n",
    "\n",
    "The loop calls `parse_legacy` function for different spreadsheets.\n",
    "\n",
    "**Dev improvement**: `parse_legacy` could get the number of sheets directly from excel file and loop inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sheets = 6\n",
    "# Initialize legacy dataframe as None type\n",
    "legacy_df = None\n",
    "# legacy data filename to write\n",
    "legacy_file_write = \"legacy_data\"\n",
    "\n",
    "# Skip extraction if legacy already parsed and writen\n",
    "flag_parsed = os.path.exists(f\"{raw_path}{legacy_file_write}.csv\")\n",
    "\n",
    "if flag_parsed:\n",
    "    print(f\"Legacy data already parsed and writen\")\n",
    "else:\n",
    "    for i in range(1, n_sheets + 1):\n",
    "        print(f\"Parsing Spreadsheet: {i}\")\n",
    "        df = legacy.parse_legacy(full_path, i, legacy_country_list)\n",
    "        legacy_df = pd.concat([legacy_df, df])\n",
    "\n",
    "    # write legacy raw data (all indicators) to csv file\n",
    "    legacy_df.to_csv(f\"{raw_path}{legacy_file_write}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning Messages**: Education legacy indicators specify seasons instead of years, e.g: 2005/06\n",
    "\n",
    "SDMX accepts only a year as time dimension. Seasons are kept in `COVERAGE_TIME` attribute as suggested by *Daniele*.\n",
    "##### Transformation of legacy data into an SDMX structure\n",
    "It is performed on `legacy_df` dataframe, and placed in this [**Section**](#Transformation-of-Legacy-Indicators-into-an-SDMX-structure).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TransMonEE UIS API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uis_key = \"9d48382df9ad408ca538352a4186791b\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read and Query Data Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to excel data dictionary in repo\n",
    "data_dict_file = \"./data_in/data_dictionary/indicator_dictionary_TM_v5.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get indicators that are extracted by API (code, address and more in pandas dataframe)\n",
    "api_code_addr_df = get_API_code_address_etc(data_dict_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract and Transform Indicators from dataframe `api_code_addr_df`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### API Extraction: parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters: API-SDMX request dataflow from UN-SDG\n",
    "sdg_api_params = {\"startPeriod\": str(year_ini), \"endPeriod\": str(year_end)}\n",
    "# parameters: API-SDMX request dataflow from Helix\n",
    "helix_api_params = {**sdg_api_params, \"locale\": \"en\"}\n",
    "# parameters: API-SDMX request dataflow from UIS\n",
    "uis_api_params = {**helix_api_params, \"subscription-key\": uis_key}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters: API request from WHO (TM countries, all years available)\n",
    "country_call_who = \"COUNTRY:\" + \";COUNTRY:\".join(country_codes_3.values())\n",
    "year_filter_who = \";YEAR:*\"\n",
    "who_api_params = {\n",
    "    \"format\": \"csv\",\n",
    "    \"profile\": \"verbose\",\n",
    "    \"filter\": country_call_who + year_filter_who,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### API Extraction: headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API headers (desired format and compress response) for SDMX type API sources\n",
    "sdmx_headers = {\n",
    "    \"Accept\": \"application/vnd.sdmx.data+csv;version=1.0.0\",\n",
    "    \"Accept-Encoding\": \"gzip\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transformation: map raw data into dataflow TransMonEE in UNICEF Warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformed data destination path\n",
    "trans_path = \"./data_out/data_transformed/\"\n",
    "# name of dataflow TransMonEE in UNICEF warehouse\n",
    "dataflow_out = \"ECARO:TRANSMONEE(1.0)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TMEE DSD (data structure definition)\n",
    "dest_dsd = Destination(\"TMEE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loop on dataframe `api_code_addr_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "tags": []
   },
   "outputs": [],
   "source": [
    "# actual loop (EXTRACT AND TRANSFORM)\n",
    "for index, row in api_code_addr_df.iterrows():\n",
    "    \n",
    "    # sanity check on first four: strip strings leading and ending spaces\n",
    "    url_endpoint = row[\"Address\"].strip()\n",
    "    indicator_code = row[\"Code\"].strip()\n",
    "    indicator_source = row[\"Data_Source\"].strip()\n",
    "    # line below doesn't handle possible error: \"Units\" blank entry\n",
    "    indicator_units = row[\"Units\"].strip()\n",
    "    indicator_freq = row[\"Freq_Coll\"]\n",
    "    # ensure FREQ (years) astype int\n",
    "    if not np.isnan(indicator_freq):\n",
    "        indicator_freq = int(indicator_freq)\n",
    "    # indicator nature (e.g: all observations are estimated)\n",
    "    indicator_nature = row[\"Nature\"]\n",
    "    indicator_notes = row[\"Obs_Footnote\"]\n",
    "\n",
    "    # get source_key from indicator_source\n",
    "    pattern = \"(.*?):\"\n",
    "    source_key = re.findall(pattern, indicator_source)[0].strip()\n",
    "    # type of extraction response (json, sdmx, etc) from data dictionary\n",
    "    source_format = row[\"Content_type\"].strip()\n",
    "    \n",
    "    print(f\"Dealing with indicator: {indicator_code}\")\n",
    "        \n",
    "    # wrap API addresses\n",
    "    api_address = wrap_api_address(\n",
    "        source_key, url_endpoint, indicator_code, country_codes_3, country_codes_m49_df\n",
    "    )\n",
    "\n",
    "    # wrap API parameters & headers\n",
    "    api_headers = sdmx_headers\n",
    "    if source_key.lower() == \"helix\":\n",
    "        api_params = helix_api_params\n",
    "    elif source_key.lower() == \"uis\":\n",
    "        api_params = uis_api_params\n",
    "    elif source_key.lower() == \"sdg\" or source_key.lower() == \"ilo\":\n",
    "        # 'sdg' and 'ilo' rest sdmx use same parameters\n",
    "        api_params = sdg_api_params\n",
    "    else:\n",
    "        api_params = who_api_params\n",
    "        api_headers = None\n",
    "        \n",
    "    # Skip extraction if indicator already downloaded\n",
    "    flag_download = os.path.exists(f\"{raw_path}{indicator_code}.csv\")\n",
    "    # This skip would need extra info to be executed for update purposes!\n",
    "    # File names could include the year of execution?\n",
    "    if flag_download:\n",
    "        print(f\"Indicator {indicator_code} skipped (already downloaded)\")\n",
    "    elif source_format == \"pandas data reader\":\n",
    "        # raw data is extracted as a pandas df directly\n",
    "        data_raw, data_error = data_reader(\n",
    "            api_address, country_codes_3, year_ini, year_end\n",
    "        )\n",
    "        # if data_reader satisfactory\n",
    "        if not data_error:\n",
    "            # write data_raw to raw file\n",
    "            raw_file = f\"{raw_path}{indicator_code}.csv\"\n",
    "            data_raw.to_csv(raw_file, index=False)\n",
    "            print(f\"Indicator {indicator_code} succesfully downloaded\")\n",
    "            flag_download = True\n",
    "    else:\n",
    "        # request indicator raw data\n",
    "        indicator_raw = api_request(api_address, api_params, api_headers)\n",
    "        # if requests satisfactory\n",
    "        if indicator_raw.status_code == 200:\n",
    "            # write raw data to raw file\n",
    "            raw_file = f\"{raw_path}{indicator_code}.csv\"\n",
    "            with open(raw_file, \"wb\") as f:\n",
    "                f.write(indicator_raw.content)\n",
    "            print(f\"Indicator {indicator_code} succesfully downloaded\")\n",
    "            flag_download = True\n",
    "    \n",
    "    # Transform raw_data if it hasn't occured before\n",
    "    flag_transform = os.path.exists(f\"{trans_path}{indicator_code}.csv\")\n",
    "\n",
    "    if flag_transform:\n",
    "        print(f\"Transformation for {indicator_code} skipped (already done)\")\n",
    "    elif flag_download:\n",
    "        # build dataframe with indicator raw data\n",
    "        data_raw = pd.read_csv(f\"{raw_path}{indicator_code}.csv\", dtype=str)\n",
    "\n",
    "        # retain only codes from csv headers\n",
    "        raw_columns = data_raw.columns.values\n",
    "        rename_dict = {k: v.split(\":\")[0] for k, v in zip(raw_columns, raw_columns)}\n",
    "        data_raw.rename(columns=rename_dict, inplace=True)\n",
    "\n",
    "        # get dataflow from data raw anchor [0,0] if source_format is SDMX\n",
    "        if source_format.lower() == \"sdmx\":\n",
    "            text = data_raw.iloc[0, 0]\n",
    "            pattern = r\":(.+?)\\(\"\n",
    "            dataflow_key = re.findall(pattern, text)[0]\n",
    "        else:\n",
    "            # use source_format as dataflow_key (e.g: WHO api DSD change within indicator calls)\n",
    "            dataflow_key = source_format\n",
    "\n",
    "        print(f\"Transform indicator: {indicator_code}, from dataflow: {dataflow_key}\")\n",
    "\n",
    "        # instantiate dataflow class with the actual one\n",
    "        dflow_actual = Dataflow(dataflow_key)\n",
    "\n",
    "        # pre-view transformation duplicates\n",
    "        if dflow_actual.check_duplicates(data_raw):\n",
    "            print(f\"Indicator {indicator_code} will generate duplicates\")\n",
    "\n",
    "        if dflow_actual.cod_map:\n",
    "            # map the codes - normalization - works 'inplace'\n",
    "            dflow_actual.map_codes(data_raw)\n",
    "\n",
    "        # \"metadata\" from data dictionary: dataflow constants\n",
    "        # any of these below won't be used if they are dataflow columns\n",
    "        # Development NOTE: data dictionary info may be overwriten after\n",
    "        constants = {\n",
    "            \"INDICATOR\": indicator_code,\n",
    "            \"UNIT_MEASURE\": indicator_units,\n",
    "            \"OBS_FOOTNOTE\": indicator_notes,\n",
    "            \"FREQ\": indicator_freq,\n",
    "            \"DATA_SOURCE\": indicator_source,\n",
    "            \"OBS_STATUS\": indicator_nature,\n",
    "        }\n",
    "\n",
    "        # map the columns\n",
    "        data_map = dflow_actual.map_dataframe(data_raw, constants)\n",
    "\n",
    "        # save transformed indicator info independently (through pandas)\n",
    "        data_trans = pd.DataFrame(columns=dest_dsd.get_csv_columns(), dtype=str)\n",
    "        data_trans = data_trans.append(data_map)\n",
    "        # destination Dataflow: corresponding UNICEF Warehouse DSD name\n",
    "        data_trans[\"Dataflow\"] = dataflow_out\n",
    "\n",
    "        # good point to raise analysis on non-numerics (NaN, etc)\n",
    "        # e.g: drop nan values if present\n",
    "        data_trans.dropna(subset=[\"OBS_VALUE\"], inplace=True)\n",
    "        # check non-numeric data in observations\n",
    "        filter_non_num = pd.to_numeric(data_trans.OBS_VALUE, errors=\"coerce\").isnull()\n",
    "        # eliminate non-numeric observations if units not BINARY ('YES/NO' must be kept)\n",
    "        if (filter_non_num.sum() > 0) and (indicator_units != \"BINARY\"):\n",
    "            not_num_series = data_trans.OBS_VALUE[filter_non_num]\n",
    "            if not_num_series.str.contains(\"<1|>95\").all():\n",
    "                print(\n",
    "                    f\"Non-numeric observations accepted in {indicator_code}:\\n{not_num_series.unique()}\"\n",
    "                )\n",
    "            else:\n",
    "                print(\n",
    "                    f\"Non-numeric observations in {indicator_code} discarded:\\n{not_num_series.unique()}\"\n",
    "                )\n",
    "                data_trans.drop(data_trans[filter_non_num].index, inplace=True)\n",
    "\n",
    "        # save file\n",
    "        data_trans.to_csv(f\"{trans_path}{indicator_code}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformation of Legacy Indicators into an SDMX structure\n",
    "For this purpose we need some indicators *metadata* that allows the mappings.\n",
    "\n",
    "**Dev note**: data dictionary is not leveraged for legacy data so far. *Metadata* is prepared in a separated csv file `content_legacy_codes_v3`, located in `legacy_data` folder (and read by `define_maps.py` in `transformation` folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dataframe with legacy raw data\n",
    "data_raw = pd.read_csv(f\"{raw_path}{legacy_file_write}.csv\", dtype=str)\n",
    "\n",
    "# check indicator names match: legacy metadata file and raw data\n",
    "not_matching = match_indicator_names(\n",
    "    define_maps.legacy_meta_data.indicator, data_raw.indicator\n",
    ")\n",
    "\n",
    "if not_matching:\n",
    "    print(f\"Correct file '{define_maps.path_legacy}' using mappings: {not_matching}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform raw_data if it hasn't occured before\n",
    "flag_transform = os.path.exists(f\"{trans_path}{legacy_file_write}.csv\")\n",
    "\n",
    "if flag_transform:\n",
    "    print(f\"Transformation for legacy data skipped (already done)\")\n",
    "else:\n",
    "    # dataflow to process is legacy data\n",
    "    dataflow_key = \"LEGACY\"\n",
    "    # instantiate dataflow class with the actual key (LEGACY)\n",
    "    dflow_actual = Dataflow(dataflow_key)\n",
    "\n",
    "    # pre-view duplicates in legacy data\n",
    "    if dflow_actual.check_duplicates(data_raw):\n",
    "        print(f\"Legacy data contains duplicates\")\n",
    "    \n",
    "    # map the codes - normalization - from legacy dataframe\n",
    "    dflow_actual.map_codes(data_raw)\n",
    "\n",
    "    # initialize constants empty (no data from dictionary for legacy)\n",
    "    constants = {}\n",
    "    # map the columns\n",
    "    data_map = dflow_actual.map_dataframe(data_raw, constants)\n",
    "    \n",
    "    # save transformed indicator info independently (through pandas)\n",
    "    data_trans = pd.DataFrame(columns=dest_dsd.get_csv_columns(), dtype=str)\n",
    "    data_trans = data_trans.append(data_map)\n",
    "    # destination Dataflow: TMEE DSD in UNICEF Warehouse\n",
    "    data_trans[\"Dataflow\"] = dataflow_out\n",
    "\n",
    "    # drop nan values if present\n",
    "    data_trans.dropna(subset=[\"OBS_VALUE\"], inplace=True)\n",
    "    # check non-numerics in legacy data observations\n",
    "    filter_non_num = pd.to_numeric(data_trans.OBS_VALUE, errors=\"coerce\").isnull()\n",
    "    # eliminate non-numerics\n",
    "    if filter_non_num.sum() > 0:\n",
    "        not_num_array = data_trans.OBS_VALUE[filter_non_num].unique()\n",
    "        print(f\"Non-numeric observations discarded in legacy data:\\n{not_num_array}\")\n",
    "        data_trans.drop(data_trans[filter_non_num].index, inplace=True)\n",
    "\n",
    "    # save file\n",
    "    data_trans.to_csv(f\"{trans_path}{legacy_file_write}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data to Upload - Build only one CSV with all data transformed\n",
    "Could be done with Linux command `sed` for faster performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all csv files with data transformed (that are not TMEE out file)\n",
    "files_trans = [\n",
    "    file\n",
    "    for file in os.listdir(trans_path)\n",
    "    if (file.endswith(\".csv\") and file.find(\"TMEE\") < 0)\n",
    "]\n",
    "\n",
    "# pandas concat\n",
    "dest_dsd_df = pd.concat(\n",
    "    [pd.read_csv(f\"{trans_path}{f}\", dtype=str) for f in files_trans]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save file if not present to avoid re-writing\n",
    "etl_out_file = \"TMEE_ETL_out\"\n",
    "\n",
    "if f\"{etl_out_file}.csv\" not in [file for file in os.listdir(trans_path)]:\n",
    "    dest_dsd_df.to_csv(f\"{trans_path}{etl_out_file}.csv\", index=False)\n",
    "else:\n",
    "    print(f\"{etl_out_file} file not re-written, please first delete it to update.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File output ETL\n",
    "csv lines counted for the first SDMX upload: header + 235927\n",
    "\n",
    "csv lines counted for the second SDMX upload: header + 241548\n",
    "\n",
    "csv lines counted for the third SDMX upload: header + 285646\n",
    "\n",
    "csv lines counted for the fourth SDMX upload: header + 948659 (632664: UNPD indicator only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Work on output for Daniele codelist of indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# codelists destination path\n",
    "cl_path = \"./data_out/codelists/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to meta data used for legacy indicators transformation\n",
    "path_legacy = define_maps.path_legacy\n",
    "# data dictionary + legacy meta data: indicator codelists pre-print (some manual input for legacy still required)\n",
    "ind_codes_file = \"CL_TMEE_INDICATORS_pre_print\"\n",
    "get_codelist_API_legacy(api_code_addr_df, path_legacy).to_csv(\n",
    "    f\"{cl_path}{ind_codes_file}.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after some manual input (legacy indicator names), indicators codelist file renamed\n",
    "ind_codes_final_file = \"CL_TMEE_INDICATORS\"\n",
    "if f\"{ind_codes_final_file}.csv\" in [file for file in os.listdir(cl_path)]:\n",
    "    cl_indicators_df = pd.read_csv(f\"{cl_path}{ind_codes_final_file}.csv\", dtype=str)\n",
    "    # check there aren't empty entries in code and name\n",
    "    is_empty_code = cl_indicators_df.Code.isnull().any()\n",
    "    is_empty_name = cl_indicators_df.Indicator_name.isnull().any()\n",
    "    if is_empty_code or is_empty_name:\n",
    "        print(f\"Check empty codes or names in {ind_codes_final_file}.csv file.\")\n",
    "else:\n",
    "    print(f\"Please produce indicators codelist {ind_codes_final_file} file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce units codelist pre-print from etl_out_file\n",
    "units_codes_file = \"CL_TMEE_UNITS_pre_print\"\n",
    "cl_units_df, empty_flag = get_units_codelist(f\"{trans_path}{etl_out_file}.csv\")\n",
    "if not empty_flag:\n",
    "    cl_units_df.to_csv(f\"{cl_path}{units_codes_file}.csv\", index=False)\n",
    "else:\n",
    "    print(\n",
    "        f\"{units_codes_file} not written given observations without unit code specified.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spot-checks on codes (sex, residence, wealth, etc)\n",
    "print(f\"Codes for SEX: {dest_dsd_df['SEX'].unique()}\")\n",
    "print(f\"Codes for Residence: {dest_dsd_df['RESIDENCE'].unique()}\")\n",
    "print(f\"Codes for Wealth: {dest_dsd_df['WEALTH_QUINTILE'].unique()}\")\n",
    "print(f\"Codes for Freq: {dest_dsd_df['FREQ'].unique()}\")\n",
    "print(f\"Codes for Unit Mult: {dest_dsd_df['UNIT_MULTIPLIER'].unique()}\")\n",
    "print(f\"Codes for Obs Status: {dest_dsd_df['OBS_STATUS'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# codelist age groups is checked w.r.t CL_AGE in SDMX warehouse (too many to spot-check only)\n",
    "# UNICEFâ€™s REST API endpoint for codelists\n",
    "url_endpoint = \"https://sdmx.data.unicef.org/ws/public/sdmxapi/rest/codelist/\"\n",
    "codelist = \"UNICEF/CL_AGE\"\n",
    "# address and parameters for codelist request\n",
    "api_address = url_endpoint + codelist\n",
    "api_params = {\"format\": \"sdmx-json\"}\n",
    "# API codelist request\n",
    "age_cl_json = api_request(api_address, api_params).json()\n",
    "# Age Codelist from SDMX-JSON\n",
    "age_cl_dict = {\n",
    "    elem[\"id\"]: elem[\"name\"] for elem in age_cl_json[\"data\"][\"codelists\"][0][\"codes\"]\n",
    "}\n",
    "# Check if any age group in ETL output is NOT in age_cl_dict\n",
    "age_check_list = np.setdiff1d(dest_dsd_df[\"AGE\"], list(age_cl_dict.keys()))\n",
    "if len(age_check_list) > 0:\n",
    "    print(f\"Array of code/s not in {codelist}:\\n{age_check_list}\")\n",
    "else:\n",
    "    print(f\"Age-groups codelist {codelist} checked.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# age_groups codelist pre-print\n",
    "age_groups_file = \"CL_TMEE_AGE_pre_print\"\n",
    "# make dataframe column with unique codes in etl_out_file\n",
    "cl_age_df = pd.DataFrame(dest_dsd_df[\"AGE\"].unique(), columns=[\"code\"])\n",
    "# make list with description from UNICEF CL_AGE\n",
    "cl_age_label = [\n",
    "    age_cl_dict[code] if code in age_cl_dict else np.nan for code in cl_age_df.code\n",
    "]\n",
    "cl_age_df[\"label\"] = cl_age_label\n",
    "cl_age_df.to_csv(f\"{cl_path}{age_groups_file}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
