{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TransMonEE Indicators - API (Helix and UIS) sources populated in Data Dictionary - LEGACY DATA ETL\n",
    "In this notebook, I will loop along these indicators for extraction and transformation.\n",
    "\n",
    "**Numbers:**\n",
    "* Helix sources (28 indicators - 1 missing in dataflow)\n",
    "* UIS sources (129 indicators)\n",
    "* Legacy Excel file (322 indicators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_API_code_address_etc\n",
    "from fileUtils import fileDownload\n",
    "from sdmx import sdmx_struc\n",
    "from extraction import legacy\n",
    "from extraction.wrap_api_address import wrap_api_address\n",
    "from transformation.destination import Destination\n",
    "from transformation.dataflow import Dataflow\n",
    "from data_in.legacy_data.prepare_mapping import match_country_name\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TransMonEE countries list - Country ISO codes\n",
    "##### Countries list is taken from dataflow TransMonEE in UNICEF Warehouse (requested by Eduard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNICEFâ€™s REST API data endpoint for TransMonEE Dataflow\n",
    "url_endpoint = 'https://sdmx.data.unicef.org/ws/public/sdmxapi/rest/data/ECARO,TRANSMONEE,1.0/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# address and parameters for dataflow structure request\n",
    "api_address = url_endpoint + 'all'\n",
    "api_params = {'format':'sdmx-json', 'detail':'structureOnly'}\n",
    "# API dataflow structure request\n",
    "d_flow_struc = fileDownload.api_request(api_address,api_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Country ISO codes (2 and 3 letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TransMonEE three-letters country codes are taken from its dataflow\n",
    "country_codes_3 = sdmx_struc.get_all_country_codes(d_flow_struc.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# country codes equivalence from excel file in repo root\n",
    "country_codes_file = \"./data_in/all_countrynames_list.xlsx\"\n",
    "country_codes_df = pd.read_excel(country_codes_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map TMEE country_codes (three-letters/two-letters equivalence)\n",
    "country_codes_2 = [country_codes_df.CountryIso2[country_codes_df.CountryIso3 == elem].values\n",
    "                   for elem in country_codes_3.values()]\n",
    "# country names are repeated in the list, and I want the uniques only\n",
    "# numpy unique sorts the array, I take an extra step to retrieve the original order\n",
    "uni_sort, sort_ind = np.unique(np.concatenate(country_codes_2), return_index=True)\n",
    "country_codes_2 = uni_sort[np.argsort(sort_ind)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# country codes mapping dictionary (two-letters/three-letters)\n",
    "country_map = {k:v for k,v in zip(country_codes_2,country_codes_3.values())}\n",
    "# write dictionary in py file to use it during transformations\n",
    "path_file = \"./transformation/country_map.py\"\n",
    "f = open(path_file, 'w')\n",
    "f.write('country_map = ' + repr(country_map) + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Country names as defined in Legacy Data\n",
    "Required to identify rows with data by the Excel Legacy parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of countries as reported in legacy data\n",
    "legacy_country_list = [\"albania\", \"armenia\", \"azerbaijan\", \"belarus\", \"bosnia and herzegovina\", \"bulgaria\", \"croatia\",\n",
    "                 \"czech republic\", \"estonia\", \"georgia\", \"hungary\", \"kazakhstan\", \"kyrgyzstan\", \"latvia\", \"lithuania\",\n",
    "                 \"moldova\", \"montenegro\", \"poland\", \"romania\", \"russian federation\", \"serbia\", \"slovakia\", \"slovenia\",\n",
    "                 \"tajikistan\", \"the former yugoslav republic of macedonia\", \"turkmenistan\", \"ukraine\", \"uzbekistan\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match country names (legacy data) with country names used in TMEE\n",
    "# build dictionary with country names (legacy data) and country codes in TMEE\n",
    "legacy_country_codes_3 = {}\n",
    "for name in legacy_country_list:\n",
    "    match = match_country_name(name, list(country_codes_3.keys()))\n",
    "    legacy_country_codes_3[name] = country_codes_3[match]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write legacy_country_codes_3 dictionary in py file to use during transformations\n",
    "path_file = \"./transformation/country_names_map.py\"\n",
    "f = open(path_file, 'w')\n",
    "f.write('country_names_map = ' + repr(legacy_country_codes_3) + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Legacy data Extraction\n",
    "##### Source file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to legacy excel file\n",
    "source_path_nsi = './data_in/legacy_data/'\n",
    "source_file = 'TM-2019-EN-June.xlsx'\n",
    "full_path = source_path_nsi + source_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Raw data destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw data destination path\n",
    "raw_path = './data_out/data_raw/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parse all legacy indicators from Excel `source_file`\n",
    "There's one spreadsheet with contents and 6 spreadsheets containing data.\n",
    "\n",
    "The loop calls `parse_legacy` function for different spreadsheets.\n",
    "\n",
    "**Dev improvement**: `parse_legacy` could get the number of sheets directly from excel file and loop inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sheets = 6\n",
    "# Initialize legacy dataframe as None type\n",
    "legacy_df = None\n",
    "# legacy data filename to write\n",
    "legacy_file_write = 'legacy_data'\n",
    "\n",
    "# Skip extraction if legacy already parsed and writen\n",
    "flag_parsed = os.path.exists(f\"{raw_path}{legacy_file_write}.csv\")\n",
    "\n",
    "if flag_parsed:\n",
    "    print(f\"Legacy data already parsed and writen\")\n",
    "else:\n",
    "    for i in range(1, n_sheets+1):\n",
    "        print(f\"Parsing Spreadsheet: {i}\")\n",
    "        df = legacy.parse_legacy(full_path,i,legacy_country_list)\n",
    "        legacy_df = pd.concat([legacy_df,df])\n",
    "        \n",
    "    # write legacy raw data (all indicators) to csv file\n",
    "    legacy_df.to_csv(f\"{raw_path}{legacy_file_write}.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning Messages**: Education legacy indicators specify seasons instead of years, e.g: 2005/06\n",
    "\n",
    "SDMX accepts only a year as time dimension. *Daniele* suggested adding an attribute in Data Structure Definition to denote this.\n",
    "##### Transformation of legacy data into an SDMX structure\n",
    "It is performed on `legacy_df` dataframe, and placed in this [**Section**](#Transformation-of-Legacy-Indicators-into-an-SDMX-structure).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TransMonEE UIS API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uis_key = \"9d48382df9ad408ca538352a4186791b\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read and Query Data Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to excel data dictionary in repo\n",
    "data_dict_file = './data_in/data_dictionary/indicator_dictionary_TM_v3.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get indicators that are extracted by API (code, address and more in pandas dataframe)\n",
    "api_code_addr_df = get_API_code_address_etc(data_dict_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract and Transform Indicators from dataframe `api_code_addr_df`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### API Extraction: parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters: API request dataflow from Helix\n",
    "helix_api_params = {'startPeriod':'1950', 'endPeriod':'2050', 'locale':'en'}\n",
    "# parameters: API request dataflow form UIS\n",
    "uis_api_params = {**helix_api_params, 'subscription-key':uis_key}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### API Extraction: headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API headers (desired format and compress response)\n",
    "api_headers = {'Accept':'application/vnd.sdmx.data+csv;version=1.0.0', 'Accept-Encoding':'gzip'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transformation: map raw data into dataflow TransMonEE in UNICEF Warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformed data destination path\n",
    "trans_path = './data_out/data_transformed/'\n",
    "# name of dataflow TransMonEE in UNICEF warehouse\n",
    "dataflow_out = \"ECARO:TRANSMONEE(1.0)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TMEE DSD (data structure definition)\n",
    "dest_dsd = destination('TMEE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loop on dataframe `api_code_addr_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# actual loop (EXTRACT AND TRANSFORM)\n",
    "for index, row in api_code_addr_df.iterrows():\n",
    "    \n",
    "    # sanity check on strings: strip leading and ending spaces\n",
    "    url_endpoint = row['Address'].strip()\n",
    "    indicator_code = row['Code'].strip()\n",
    "    indicator_source = row['Data_Source'].strip()\n",
    "    # get source_key from indicator_source\n",
    "    pattern = \"(.*?):\"\n",
    "    source_key = re.findall(pattern, indicator_source)[0].strip()\n",
    "    indicator_notes = row['Obs_Footnote']\n",
    "    \n",
    "    print(f\"Dealing with indicator: {indicator_code}\")\n",
    "        \n",
    "    # wrap API addresses\n",
    "    api_address = wrap_api_address(source_key, url_endpoint, indicator_code, country_codes_3, country_codes_df)\n",
    "    \n",
    "    # wrap API parameters\n",
    "    if source_key.lower() == 'helix':\n",
    "        api_params = helix_api_params\n",
    "    else:\n",
    "        api_params = uis_api_params\n",
    "        \n",
    "    # Skip extraction if indicator already downloaded\n",
    "    flag_download = os.path.exists(f\"{raw_path}{indicator_code}.csv\")\n",
    "    # This skip would need extra info to be executed for update purposes!\n",
    "    # File names could include the year of execution?\n",
    "    if flag_download:\n",
    "        print(f\"Indicator {indicator_code} skipped (already downloaded)\")\n",
    "    else:\n",
    "        # request indicator raw data\n",
    "        indicator_raw = fileDownload.api_request(api_address,api_params,api_headers)\n",
    "        # if requests satisfactory\n",
    "        if indicator_raw.status_code == 200:\n",
    "            # write raw data to raw file\n",
    "            raw_file = f\"{raw_path}{indicator_code}.csv\"\n",
    "            with open(raw_file, 'wb') as f:\n",
    "                f.write(indicator_raw.content)\n",
    "            print(f\"Indicator {indicator_code} succesfully downloaded\")\n",
    "            flag_download = True\n",
    "    \n",
    "    # Transform raw_data if it hasn't occured before\n",
    "    flag_transform = os.path.exists(f\"{trans_path}{indicator_code}.csv\")\n",
    "    \n",
    "    if flag_transform:\n",
    "        print(f\"Transformation for {indicator_code} skipped (already done)\")\n",
    "    elif flag_download:        \n",
    "        # build dataframe with indicator raw data\n",
    "        data_raw = pd.read_csv(f\"{raw_path}{indicator_code}.csv\", dtype=str)\n",
    "\n",
    "        # retain only codes from csv headers\n",
    "        raw_columns = data_raw.columns.values\n",
    "        rename_dict = {k:v.split(':')[0] for k,v in zip(raw_columns,raw_columns)}\n",
    "        data_raw.rename(columns=rename_dict,inplace=True)\n",
    "\n",
    "        # get dataflow from data raw anchor [0,0]\n",
    "        text = data_raw.iloc[0,0]\n",
    "        pattern = ':(.+?)\\('\n",
    "        dataflow_key = re.findall(pattern, text)[0]\n",
    "\n",
    "        print(f\"Transform indicator: {indicator_code}, from dataflow: {dataflow_key}\")\n",
    "\n",
    "        # instantiate dataflow class with the actual one\n",
    "        dflow_actual = dataflow(dataflow_key)\n",
    "        if dflow_actual.cod_map:\n",
    "            # map the codes - normalization - works 'inplace'\n",
    "            dflow_actual.map_codes(data_raw)\n",
    "\n",
    "        # \"metadata\" from data dictionary: dataflow constants\n",
    "        # any of these below won't be used if they are dataflow columns\n",
    "        # Development NOTE: data dictionary info may be overwriten after\n",
    "        constants = {\n",
    "            'UNICEF_INDICATOR': indicator_code,\n",
    "            'DATA_SOURCE': indicator_source,\n",
    "            'OBS_FOOTNOTE': indicator_notes\n",
    "        }\n",
    "\n",
    "        # map the columns\n",
    "        data_map = dflow_actual.map_dataframe(data_raw, constants)\n",
    "\n",
    "        # save transformed indicator info independently (through pandas)\n",
    "        data_trans = pd.DataFrame(columns=dest_dsd.get_csv_columns(), dtype=str)\n",
    "        data_trans = data_trans.append(data_map)\n",
    "        # destination Dataflow: corresponding UNICEF Warehouse DSD name\n",
    "        data_trans['Dataflow'] = dataflow_out\n",
    "        # save file\n",
    "        data_trans.to_csv(f\"{trans_path}{indicator_code}.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformation of Legacy Indicators into an SDMX structure\n",
    "For this purpose we need some indicators *metadata* that allows the mappings.\n",
    "\n",
    "**Dev note**: data dictionary is not leveraged for legacy data so far. *Metadata* is prepared in a separated csv file `content_legacy_codes_v2`, located in `legacy_data` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform raw_data if it hasn't occured before\n",
    "flag_transform = os.path.exists(f\"{trans_path}{legacy_file_write}.csv\")\n",
    "\n",
    "if flag_transform:\n",
    "    print(f\"Transformation for legacy data skipped (already done)\")\n",
    "else:\n",
    "    # dataflow to process is legacy data\n",
    "    dataflow_key = \"LEGACY\"\n",
    "    # instantiate dataflow class with the actual key (LEGACY)\n",
    "    dflow_actual = dataflow(dataflow_key)\n",
    "    \n",
    "    # build dataframe with legacy raw data\n",
    "    data_raw = pd.read_csv(f\"{raw_path}{legacy_file_write}.csv\", dtype=str)\n",
    "    \n",
    "    # map the codes - normalization - from legacy dataframe\n",
    "    dflow_actual.map_codes(data_raw)\n",
    "\n",
    "    # initialize constants empty (no data from dictionary for legacy)\n",
    "    constants = {}\n",
    "    # map the columns\n",
    "    data_map = dflow_actual.map_dataframe(data_raw, constants)\n",
    "    \n",
    "    # save transformed indicator info independently (through pandas)\n",
    "    data_trans = pd.DataFrame(columns=dest_dsd.get_csv_columns(), dtype=str)\n",
    "    data_trans = data_trans.append(data_map)\n",
    "    # destination Dataflow: TMEE DSD in UNICEF Warehouse\n",
    "    data_trans['Dataflow'] = dataflow_out\n",
    "    # save file\n",
    "    data_trans.to_csv(f\"{trans_path}{legacy_file_write}.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data to Upload - Build only one CSV with all data transformed\n",
    "Could be done with Linux command `sed` for faster performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all csv files with data transformed\n",
    "files_trans = [file for file in os.listdir(trans_path) if file.endswith(\".csv\")]\n",
    "# pandas concat\n",
    "dest_dsd_df = pd.concat([pd.read_csv(f\"{trans_path}{f}\", dtype=str) for f in files_trans])\n",
    "\n",
    "# save file\n",
    "etl_out_file = 'TMEE_ETL_out'\n",
    "dest_dsd_df.to_csv(f\"{trans_path}{etl_out_file}.csv\",index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}