{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Required standard libraries\n",
    "import pandas as pd\n",
    "import json\n",
    "import urllib\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import bs4 as bs\n",
    "import selenium\n",
    "import html5lib\n",
    "#import nltk\n",
    "import datetime\n",
    "from selenium import webdriver\n",
    "\n",
    "# Extractors \n",
    "import extract\n",
    "\n",
    "# Cleansers (cluster specific)\n",
    "import cleanse\n",
    "\n",
    "# Normalizer (generalised across all clusters)\n",
    "from normalize import scaler\n",
    "\n",
    "# Utils\n",
    "from utils import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the export path for all data exports\n",
    "from pathlib import Path\n",
    "\n",
    "# Current working directory\n",
    "cwd = Path('.')\n",
    "\n",
    "# Folder with data-in artifacts, quired to run this script\n",
    "data_in = cwd / 'data_in'\n",
    "\n",
    "# Folder containing manually extracted raw data, ready to be put in the loop\n",
    "data_sources_staged_raw = cwd / 'data_out' / 'data_staged_raw'\n",
    "data_sources_staged_raw.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Folder to export raw data\n",
    "data_sources_raw = cwd / 'data_out' / 'data_raw'\n",
    "data_sources_raw.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Folder to export cleansed data\n",
    "data_sources_cleansed = cwd / 'data_out' / 'data_cleansed'\n",
    "data_sources_cleansed.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Folder to export normalized data\n",
    "data_sources_normalized = cwd / 'data_out' / 'data_normalized'\n",
    "data_sources_normalized.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load country list and mapping dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the list of countries which contains all different variations of country names \n",
    "country_full_list = pd.read_excel(\n",
    "    data_in / 'all_countrynames_list.xlsx',\n",
    "    keep_default_na = False).drop_duplicates()\n",
    "\n",
    "# Create a version of the list with unique ISO2 and ISO3 codes\n",
    "country_iso_list = country_full_list.drop_duplicates(subset = 'COUNTRY_ISO_2')\n",
    "\n",
    "# Country CRBA list, this is the list of the countries that should be in the final CRBA indicator list\n",
    "country_crba_list = pd.read_excel(\n",
    "    data_in / 'crba_country_list.xlsx',\n",
    "    header = None,\n",
    "    usecols = [0, 1], \n",
    "    names = ['COUNTRY_ISO_3', 'COUNTRY_NAME']).merge(\n",
    "        right = country_iso_list[['COUNTRY_ISO_2', 'COUNTRY_ISO_3']],\n",
    "        how = 'left',\n",
    "        on='COUNTRY_ISO_3',\n",
    "        validate = 'one_to_one')\n",
    "\n",
    "# Run the column mapper script to load the mapping dictionary\n",
    "with open(data_in / 'column_mapping.py') as file:\n",
    "    exec(file.read())\n",
    "\n",
    "# Run the column mapper script to load the mapping dictionary\n",
    "with open(data_in / 'value_mapping.py') as file:\n",
    "    exec(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sources sheet\n",
    "crba_data_dictionary_source = pd.read_excel(\n",
    "    data_in / 'indicator_dictionary_CRBA.xlsx',\n",
    "    sheet_name = \"Source\",\n",
    "    keep_default_na = False\n",
    ")\n",
    "\n",
    "# snapshot sheet\n",
    "crba_data_dictionary_snapshot = pd.read_excel(\n",
    "    data_in / 'indicator_dictionary_CRBA.xlsx',\n",
    "    sheet_name = \"Snapshot\",\n",
    "    keep_default_na = False\n",
    ")\n",
    "\n",
    "# indicator sheet\n",
    "crba_data_dictionary_indicator = pd.read_excel(\n",
    "    data_in / 'indicator_dictionary_CRBA.xlsx',\n",
    "    sheet_name = \"Indicator\",\n",
    "    keep_default_na = False\n",
    ")\n",
    "\n",
    "# Input lists\n",
    "crba_data_dictionary_input_list = pd.read_excel(\n",
    "    data_in / 'indicator_dictionary_CRBA.xlsx',\n",
    "    sheet_name = \"Input_Lists\",\n",
    "    keep_default_na = False\n",
    ")\n",
    "\n",
    "# Add 2-digit shortcodes of index, issue and category to indicators sheet\n",
    "crba_data_dictionary_indicator = crba_data_dictionary_indicator.merge(\n",
    "    right=crba_data_dictionary_input_list[['INDEX', 'INDEX_CODE']],\n",
    "    left_on='INDEX',\n",
    "    right_on='INDEX'\n",
    ").merge(\n",
    "    right=crba_data_dictionary_input_list[['ISSUE', 'ISSUE_CODE']],\n",
    "    left_on='ISSUE',\n",
    "    right_on='ISSUE'\n",
    ").merge(\n",
    "    right=crba_data_dictionary_input_list[['CATEGORY', 'CATEGORY_CODE']],\n",
    "    left_on='CATEGORY',\n",
    "    right_on='CATEGORY'\n",
    ")\n",
    "\n",
    "# Create indicator code prefix (INDEX-ISSUE_CAEGORY CODE)\n",
    "crba_data_dictionary_indicator = crba_data_dictionary_indicator.assign(\n",
    "    INDICATOR_CODE_PREFIX = crba_data_dictionary_indicator.INDEX_CODE +\n",
    "    \"_\" +\n",
    "    crba_data_dictionary_indicator.ISSUE_CODE+\n",
    "    \"_\"+\n",
    "    crba_data_dictionary_indicator.CATEGORY_CODE+\n",
    "    \"_\")\n",
    "\n",
    "# Create indicator code\n",
    "crba_data_dictionary_indicator = crba_data_dictionary_indicator.assign(\n",
    "    INDICATOR_CODE = crba_data_dictionary_indicator.INDICATOR_CODE_PREFIX + crba_data_dictionary_indicator.INDICATOR_NAME.apply(\n",
    "    lambda x: utils.create_ind_code(x)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib, inspect\n",
    "\n",
    "extractors = { \n",
    "    cls.type: cls for name, cls in inspect.getmembers(\n",
    "        importlib.import_module(\"extract\"), \n",
    "        inspect.isclass\n",
    "    ) if hasattr(cls, 'type')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Staging (pre-processing) to create exceptional indicatorsÂ´ raw data \n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The following columns are present in the datasets, and this is the number of unique values they have. \n",
      "The column goal has 3 unique values.\n",
      "The column target has 3 unique values.\n",
      "The column indicator has 3 unique values.\n",
      "The column series has 1 unique values.\n",
      "The column seriesDescription has 1 unique values.\n",
      "The column seriesCount has 1 unique values.\n",
      "The column geoAreaCode has 98 unique values.\n",
      "The column geoAreaName has 98 unique values.\n",
      "The column timePeriodStart has 15 unique values.\n",
      "The column value has 599 unique values.\n",
      "The column valueType has 1 unique values.\n",
      "The column time_detail has 1 unique values.\n",
      "The column timeCoverage has 1 unique values.\n",
      "The column upperBound has 1 unique values.\n",
      "The column lowerBound has 1 unique values.\n",
      "The column basePeriod has 1 unique values.\n",
      "The column source has 1 unique values.\n",
      "The column geoInfoUrl has 1 unique values.\n",
      "The column footnotes has 1 unique values.\n",
      "The column attributes.Nature has 1 unique values.\n",
      "The column attributes.Units has 1 unique values.\n",
      "The column dimensions.Reporting Type has 1 unique values.\n",
      "The following columns are present in the datasets, and this is the number of unique values they have. \n",
      "The column goal has 1 unique values.\n",
      "The column target has 1 unique values.\n",
      "The column indicator has 1 unique values.\n",
      "The column series has 1 unique values.\n",
      "The column seriesDescription has 1 unique values.\n",
      "The column seriesCount has 1 unique values.\n",
      "The column geoAreaCode has 95 unique values.\n",
      "The column geoAreaName has 95 unique values.\n",
      "The column timePeriodStart has 15 unique values.\n",
      "The column value has 84 unique values.\n",
      "The column valueType has 1 unique values.\n",
      "The column time_detail has 1 unique values.\n",
      "The column timeCoverage has 1 unique values.\n",
      "The column upperBound has 1 unique values.\n",
      "The column lowerBound has 1 unique values.\n",
      "The column basePeriod has 1 unique values.\n",
      "The column source has 1 unique values.\n",
      "The column geoInfoUrl has 1 unique values.\n",
      "The column footnotes has 1 unique values.\n",
      "The column attributes.Nature has 1 unique values.\n",
      "The column attributes.Units has 1 unique values.\n",
      "The column dimensions.Reporting Type has 1 unique values.\n",
      "The following columns are present in the datasets, and this is the number of unique values they have. \n",
      "The column goal has 1 unique values.\n",
      "The column target has 1 unique values.\n",
      "The column indicator has 1 unique values.\n",
      "The column series has 1 unique values.\n",
      "The column seriesDescription has 1 unique values.\n",
      "The column seriesCount has 1 unique values.\n",
      "The column geoAreaCode has 87 unique values.\n",
      "The column geoAreaName has 87 unique values.\n",
      "The column timePeriodStart has 15 unique values.\n",
      "The column value has 88 unique values.\n",
      "The column valueType has 1 unique values.\n",
      "The column time_detail has 1 unique values.\n",
      "The column timeCoverage has 1 unique values.\n",
      "The column upperBound has 1 unique values.\n",
      "The column lowerBound has 1 unique values.\n",
      "The column basePeriod has 1 unique values.\n",
      "The column source has 1 unique values.\n",
      "The column geoInfoUrl has 1 unique values.\n",
      "The column footnotes has 1 unique values.\n",
      "The column attributes.Nature has 1 unique values.\n",
      "The column attributes.Units has 1 unique values.\n",
      "The column dimensions.Reporting Type has 1 unique values.\n",
      "The following columns are present in the datasets, and this is the number of unique values they have. \n",
      "The column goal has 1 unique values.\n",
      "The column target has 1 unique values.\n",
      "The column indicator has 1 unique values.\n",
      "The column series has 1 unique values.\n",
      "The column seriesDescription has 1 unique values.\n",
      "The column seriesCount has 1 unique values.\n",
      "The column geoAreaCode has 110 unique values.\n",
      "The column geoAreaName has 110 unique values.\n",
      "The column timePeriodStart has 15 unique values.\n",
      "The column value has 79 unique values.\n",
      "The column valueType has 1 unique values.\n",
      "The column time_detail has 1 unique values.\n",
      "The column timeCoverage has 1 unique values.\n",
      "The column upperBound has 1 unique values.\n",
      "The column lowerBound has 1 unique values.\n",
      "The column basePeriod has 1 unique values.\n",
      "The column source has 1 unique values.\n",
      "The column geoInfoUrl has 1 unique values.\n",
      "The column footnotes has 1 unique values.\n",
      "The column attributes.Nature has 1 unique values.\n",
      "The column attributes.Units has 1 unique values.\n",
      "The column dimensions.Reporting Type has 1 unique values.\n"
     ]
    }
   ],
   "source": [
    "# Pre-processing of exceptional indicators, which require extra transformation\n",
    "# Important: File requires having filepaths from above defined and pandas already imported\n",
    "with open(data_in / 'staging_create_raw_data.py') as file:\n",
    "    exec(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract - Transform - Load Loop\n",
    "## API sources\n",
    "### CSV API sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "lumn PUBLISHSTATE has 1 unique values.\n",
      "The column YEAR has 4 unique values.\n",
      "The column REGION has 7 unique values.\n",
      "The column COUNTRY has 194 unique values.\n",
      "The column Display Value has 6 unique values.\n",
      "The column Numeric has 1 unique values.\n",
      "The column Low has 1 unique values.\n",
      "The column High has 1 unique values.\n",
      "The column Comments has 6 unique values.\n",
      "\n",
      " - - - - - \n",
      " Cleansing source S-137 \n",
      "\n",
      "\n",
      " Calling function 'extract_who_raw_data'...\n",
      "\n",
      " Calling function 'rename_and_discard_columns'...\n",
      "\n",
      " Calling function 'extract_year_from_timeperiod'...\n",
      "\n",
      " Calling function 'retrieve_latest_observation'...\n",
      "\n",
      " Calling function 'add_and_discard_countries'...\n",
      "\n",
      " Calling function 'add_cols_fill_cells'...\n",
      "\n",
      " Calling function 'map_values'...\n",
      "Values of column: DIM_SEX couldn't be mapped. If column DIM_SEX is present, there is an error with the code. \n",
      "Values of column: DIM_EDU_LEVEL couldn't be mapped. If column DIM_EDU_LEVEL is present, there is an error with the code. \n",
      "Values of column: DIM_AGE couldn't be mapped. If column DIM_AGE is present, there is an error with the code. \n",
      "Values of column: DIM_AGE_GROUP couldn't be mapped. If column DIM_AGE_GROUP is present, there is an error with the code. \n",
      "Values of column: DIM_MANAGEMENT_LEVEL couldn't be mapped. If column DIM_MANAGEMENT_LEVEL is present, there is an error with the code. \n",
      "Values of column: DIM_AREA_TYPE couldn't be mapped. If column DIM_AREA_TYPE is present, there is an error with the code. \n",
      "Values of column: DIM_QUANTILE couldn't be mapped. If column DIM_QUANTILE is present, there is an error with the code. \n",
      "Values of column: DIM_SDG_INDICATOR couldn't be mapped. If column DIM_SDG_INDICATOR is present, there is an error with the code. \n",
      "Values of column: DIM_OCU_TYPE couldn't be mapped. If column DIM_OCU_TYPE is present, there is an error with the code. \n",
      "Values of column: DIM_REP_TYPE couldn't be mapped. If column DIM_REP_TYPE is present, there is an error with the code. \n",
      "Values of column: DIM_SECTOR couldn't be mapped. If column DIM_SECTOR is present, there is an error with the code. \n",
      "Values of column: ATTR_UNIT_MEASURE couldn't be mapped. If column ATTR_UNIT_MEASURE is present, there is an error with the code. \n",
      "\n",
      " Calling function 'encode_categorical_variables'...\n",
      "Cleansing done. This is some basic information about the data: \n",
      " \n",
      " There are 195 rows in the dataframe and 0.0% have a NA-value in the column 'OBS_RAW_VALUE\n",
      "\n",
      " \n",
      " This is the summary of the column 'TIME_PERIOD': count     195.000000\n",
      "mean     2019.015385\n",
      "std         0.123394\n",
      "min      2019.000000\n",
      "25%      2019.000000\n",
      "50%      2019.000000\n",
      "75%      2019.000000\n",
      "max      2020.000000\n",
      "Name: TIME_PERIOD, dtype: float64\n",
      "\n",
      " - - - - - \n",
      " Extracting source S-157 \n",
      "\n",
      "The following columns are present in the datasets, and this is the number of unique values they have. \n",
      "The column GHO has 1 unique values.\n",
      "The column PUBLISHSTATE has 1 unique values.\n",
      "The column YEAR has 1 unique values.\n",
      "The column REGION has 6 unique values.\n",
      "The column COUNTRY has 183 unique values.\n",
      "The column SEX has 3 unique values.\n",
      "The column ENVCAUSE has 1 unique values.\n",
      "The column Display Value has 374 unique values.\n",
      "The column Numeric has 547 unique values.\n",
      "The column Low has 547 unique values.\n",
      "The column High has 547 unique values.\n",
      "The column Comments has 1 unique values.\n",
      "\n",
      " - - - - - \n",
      " Cleansing source S-157 \n",
      "\n",
      "\n",
      " Calling function 'extract_who_raw_data'...\n",
      "\n",
      " Calling function 'rename_and_discard_columns'...\n",
      "\n",
      " Calling function 'extract_year_from_timeperiod'...\n",
      "\n",
      " Calling function 'retrieve_latest_observation'...\n",
      "\n",
      " Calling function 'add_and_discard_countries'...\n",
      "\n",
      " Calling function 'add_cols_fill_cells'...\n",
      "\n",
      " Calling function 'map_values'...\n",
      "\n",
      " Successfully mapped values of column: DIM_SEX\n",
      "Values of column: DIM_EDU_LEVEL couldn't be mapped. If column DIM_EDU_LEVEL is present, there is an error with the code. \n",
      "Values of column: DIM_AGE couldn't be mapped. If column DIM_AGE is present, there is an error with the code. \n",
      "Values of column: DIM_AGE_GROUP couldn't be mapped. If column DIM_AGE_GROUP is present, there is an error with the code. \n",
      "Values of column: DIM_MANAGEMENT_LEVEL couldn't be mapped. If column DIM_MANAGEMENT_LEVEL is present, there is an error with the code. \n",
      "Values of column: DIM_AREA_TYPE couldn't be mapped. If column DIM_AREA_TYPE is present, there is an error with the code. \n",
      "Values of column: DIM_QUANTILE couldn't be mapped. If column DIM_QUANTILE is present, there is an error with the code. \n",
      "Values of column: DIM_SDG_INDICATOR couldn't be mapped. If column DIM_SDG_INDICATOR is present, there is an error with the code. \n",
      "Values of column: DIM_OCU_TYPE couldn't be mapped. If column DIM_OCU_TYPE is present, there is an error with the code. \n",
      "Values of column: DIM_REP_TYPE couldn't be mapped. If column DIM_REP_TYPE is present, there is an error with the code. \n",
      "Values of column: DIM_SECTOR couldn't be mapped. If column DIM_SECTOR is present, there is an error with the code. \n",
      "Values of column: ATTR_UNIT_MEASURE couldn't be mapped. If column ATTR_UNIT_MEASURE is present, there is an error with the code. \n",
      "\n",
      " Calling function 'encode_categorical_variables'...\n",
      "Cleansing done. This is some basic information about the data: \n",
      " \n",
      " There are 561 rows in the dataframe and 2.14% have a NA-value in the column 'OBS_RAW_VALUE\n",
      "\n",
      " \n",
      " This is the summary of the column 'TIME_PERIOD': count     561.000000\n",
      "mean     2016.085561\n",
      "std         0.579244\n",
      "min      2016.000000\n",
      "25%      2016.000000\n",
      "50%      2016.000000\n",
      "75%      2016.000000\n",
      "max      2020.000000\n",
      "Name: TIME_PERIOD, dtype: float64\n",
      "\n",
      " - - - - - \n",
      " Extracting source S-158 \n",
      "\n",
      "The following columns are present in the datasets, and this is the number of unique values they have. \n",
      "The column GHO has 1 unique values.\n",
      "The column PUBLISHSTATE has 1 unique values.\n",
      "The column YEAR has 1 unique values.\n",
      "The column REGION has 8 unique values.\n",
      "The column UNREGION has 26 unique values.\n",
      "The column COUNTRY has 195 unique values.\n",
      "The column RESIDENCEAREATYPE has 3 unique values.\n",
      "The column Display Value has 618 unique values.\n",
      "The column Numeric has 628 unique values.\n",
      "The column Low has 570 unique values.\n",
      "The column High has 570 unique values.\n",
      "The column Comments has 1 unique values.\n",
      "\n",
      " - - - - - \n",
      " Cleansing source S-158 \n",
      "\n",
      "\n",
      " Calling function 'extract_who_raw_data'...\n",
      "\n",
      " Calling function 'rename_and_discard_columns'...\n",
      "\n",
      " Calling function 'extract_year_from_timeperiod'...\n",
      "\n",
      " Calling function 'retrieve_latest_observation'...\n",
      "\n",
      " Calling function 'add_and_discard_countries'...\n",
      "\n",
      " Calling function 'add_cols_fill_cells'...\n",
      "\n",
      " Calling function 'map_values'...\n",
      "Values of column: DIM_SEX couldn't be mapped. If column DIM_SEX is present, there is an error with the code. \n",
      "Values of column: DIM_EDU_LEVEL couldn't be mapped. If column DIM_EDU_LEVEL is present, there is an error with the code. \n",
      "Values of column: DIM_AGE couldn't be mapped. If column DIM_AGE is present, there is an error with the code. \n",
      "Values of column: DIM_AGE_GROUP couldn't be mapped. If column DIM_AGE_GROUP is present, there is an error with the code. \n",
      "Values of column: DIM_MANAGEMENT_LEVEL couldn't be mapped. If column DIM_MANAGEMENT_LEVEL is present, there is an error with the code. \n",
      "\n",
      " Successfully mapped values of column: DIM_AREA_TYPE\n",
      "Values of column: DIM_QUANTILE couldn't be mapped. If column DIM_QUANTILE is present, there is an error with the code. \n",
      "Values of column: DIM_SDG_INDICATOR couldn't be mapped. If column DIM_SDG_INDICATOR is present, there is an error with the code. \n",
      "Values of column: DIM_OCU_TYPE couldn't be mapped. If column DIM_OCU_TYPE is present, there is an error with the code. \n",
      "Values of column: DIM_REP_TYPE couldn't be mapped. If column DIM_REP_TYPE is present, there is an error with the code. \n",
      "Values of column: DIM_SECTOR couldn't be mapped. If column DIM_SECTOR is present, there is an error with the code. \n",
      "Values of column: ATTR_UNIT_MEASURE couldn't be mapped. If column ATTR_UNIT_MEASURE is present, there is an error with the code. \n",
      "\n",
      " Calling function 'encode_categorical_variables'...\n",
      "Cleansing done. This is some basic information about the data: \n",
      " \n",
      " There are 577 rows in the dataframe and 0.52% have a NA-value in the column 'OBS_RAW_VALUE\n",
      "\n",
      " \n",
      " This is the summary of the column 'TIME_PERIOD': count     577.000000\n",
      "mean     2016.020797\n",
      "std         0.287924\n",
      "min      2016.000000\n",
      "25%      2016.000000\n",
      "50%      2016.000000\n",
      "75%      2016.000000\n",
      "max      2020.000000\n",
      "Name: TIME_PERIOD, dtype: float64\n",
      "\n",
      " - - - - - \n",
      " Extracting source S-199 \n",
      "\n",
      "The following columns are present in the datasets, and this is the number of unique values they have. \n",
      "The column GHO has 1 unique values.\n",
      "The column PUBLISHSTATE has 1 unique values.\n",
      "The column YEAR has 18 unique values.\n",
      "The column REGION has 8 unique values.\n",
      "The column WORLDBANKINCOMEGROUP has 6 unique values.\n",
      "The column COUNTRY has 191 unique values.\n",
      "The column Display Value has 3165 unique values.\n",
      "The column Numeric has 3563 unique values.\n",
      "The column Low has 1 unique values.\n",
      "The column High has 1 unique values.\n",
      "The column Comments has 1 unique values.\n",
      "\n",
      " - - - - - \n",
      " Cleansing source S-199 \n",
      "\n",
      "\n",
      " Calling function 'extract_who_raw_data'...\n",
      "\n",
      " Calling function 'rename_and_discard_columns'...\n",
      "\n",
      " Calling function 'extract_year_from_timeperiod'...\n",
      "\n",
      " Calling function 'retrieve_latest_observation'...\n",
      "\n",
      " Calling function 'add_and_discard_countries'...\n",
      "\n",
      " Calling function 'add_cols_fill_cells'...\n",
      "\n",
      " Calling function 'map_values'...\n",
      "Values of column: DIM_SEX couldn't be mapped. If column DIM_SEX is present, there is an error with the code. \n",
      "Values of column: DIM_EDU_LEVEL couldn't be mapped. If column DIM_EDU_LEVEL is present, there is an error with the code. \n",
      "Values of column: DIM_AGE couldn't be mapped. If column DIM_AGE is present, there is an error with the code. \n",
      "Values of column: DIM_AGE_GROUP couldn't be mapped. If column DIM_AGE_GROUP is present, there is an error with the code. \n",
      "Values of column: DIM_MANAGEMENT_LEVEL couldn't be mapped. If column DIM_MANAGEMENT_LEVEL is present, there is an error with the code. \n",
      "Values of column: DIM_AREA_TYPE couldn't be mapped. If column DIM_AREA_TYPE is present, there is an error with the code. \n",
      "Values of column: DIM_QUANTILE couldn't be mapped. If column DIM_QUANTILE is present, there is an error with the code. \n",
      "Values of column: DIM_SDG_INDICATOR couldn't be mapped. If column DIM_SDG_INDICATOR is present, there is an error with the code. \n",
      "Values of column: DIM_OCU_TYPE couldn't be mapped. If column DIM_OCU_TYPE is present, there is an error with the code. \n",
      "Values of column: DIM_REP_TYPE couldn't be mapped. If column DIM_REP_TYPE is present, there is an error with the code. \n",
      "Values of column: DIM_SECTOR couldn't be mapped. If column DIM_SECTOR is present, there is an error with the code. \n",
      "Values of column: ATTR_UNIT_MEASURE couldn't be mapped. If column ATTR_UNIT_MEASURE is present, there is an error with the code. \n",
      "\n",
      " Calling function 'encode_categorical_variables'...\n",
      "Cleansing done. This is some basic information about the data: \n",
      " \n",
      " There are 195 rows in the dataframe and 3.59% have a NA-value in the column 'OBS_RAW_VALUE\n",
      "\n",
      " \n",
      " This is the summary of the column 'TIME_PERIOD': count     195.000000\n",
      "mean     2017.025641\n",
      "std         0.827591\n",
      "min      2011.000000\n",
      "25%      2017.000000\n",
      "50%      2017.000000\n",
      "75%      2017.000000\n",
      "max      2020.000000\n",
      "Name: TIME_PERIOD, dtype: float64\n",
      "\n",
      " - - - - - \n",
      " Extracting source S-200 \n",
      "\n",
      "The following columns are present in the datasets, and this is the number of unique values they have. \n",
      "The column GHO has 1 unique values.\n",
      "The column PUBLISHSTATE has 1 unique values.\n",
      "The column YEAR has 1 unique values.\n",
      "The column REGION has 6 unique values.\n",
      "The column COUNTRY has 132 unique values.\n",
      "The column Display Value has 3 unique values.\n",
      "The column Numeric has 1 unique values.\n",
      "The column Low has 1 unique values.\n",
      "The column High has 1 unique values.\n",
      "The column Comments has 1 unique values.\n",
      "\n",
      " - - - - - \n",
      " Cleansing source S-200 \n",
      "\n",
      "\n",
      " Calling function 'extract_who_raw_data'...\n",
      "\n",
      " Calling function 'rename_and_discard_columns'...\n",
      "\n",
      " Calling function 'extract_year_from_timeperiod'...\n",
      "\n",
      " TIME_PERIOD column contained time periods (no atomic years). Successfully extrated year. \n",
      "\n",
      " Calling function 'retrieve_latest_observation'...\n",
      "\n",
      " Calling function 'add_and_discard_countries'...\n",
      "\n",
      " Calling function 'add_cols_fill_cells'...\n",
      "\n",
      " Calling function 'map_values'...\n",
      "Values of column: DIM_SEX couldn't be mapped. If column DIM_SEX is present, there is an error with the code. \n",
      "Values of column: DIM_EDU_LEVEL couldn't be mapped. If column DIM_EDU_LEVEL is present, there is an error with the code. \n",
      "Values of column: DIM_AGE couldn't be mapped. If column DIM_AGE is present, there is an error with the code. \n",
      "Values of column: DIM_AGE_GROUP couldn't be mapped. If column DIM_AGE_GROUP is present, there is an error with the code. \n",
      "Values of column: DIM_MANAGEMENT_LEVEL couldn't be mapped. If column DIM_MANAGEMENT_LEVEL is present, there is an error with the code. \n",
      "Values of column: DIM_AREA_TYPE couldn't be mapped. If column DIM_AREA_TYPE is present, there is an error with the code. \n",
      "Values of column: DIM_QUANTILE couldn't be mapped. If column DIM_QUANTILE is present, there is an error with the code. \n",
      "Values of column: DIM_SDG_INDICATOR couldn't be mapped. If column DIM_SDG_INDICATOR is present, there is an error with the code. \n",
      "Values of column: DIM_OCU_TYPE couldn't be mapped. If column DIM_OCU_TYPE is present, there is an error with the code. \n",
      "Values of column: DIM_REP_TYPE couldn't be mapped. If column DIM_REP_TYPE is present, there is an error with the code. \n",
      "Values of column: DIM_SECTOR couldn't be mapped. If column DIM_SECTOR is present, there is an error with the code. \n",
      "Values of column: ATTR_UNIT_MEASURE couldn't be mapped. If column ATTR_UNIT_MEASURE is present, there is an error with the code. \n",
      "\n",
      " Calling function 'encode_categorical_variables'...\n",
      "Cleansing done. This is some basic information about the data: \n",
      " \n",
      " There are 195 rows in the dataframe and 0.0% have a NA-value in the column 'OBS_RAW_VALUE\n",
      "\n",
      " \n",
      " This is the summary of the column 'TIME_PERIOD': count     195.000000\n",
      "mean     2015.297436\n",
      "std         3.295380\n",
      "min      2013.000000\n",
      "25%      2013.000000\n",
      "50%      2013.000000\n",
      "75%      2020.000000\n",
      "max      2020.000000\n",
      "Name: TIME_PERIOD, dtype: float64\n",
      "\n",
      " - - - - - \n",
      " Extracting source S-201 \n",
      "\n",
      "The following columns are present in the datasets, and this is the number of unique values they have. \n",
      "The column GHO has 1 unique values.\n",
      "The column PUBLISHSTATE has 1 unique values.\n",
      "The column YEAR has 1 unique values.\n",
      "The column REGION has 6 unique values.\n",
      "The column COUNTRY has 132 unique values.\n",
      "The column Display Value has 4 unique values.\n",
      "The column Numeric has 1 unique values.\n",
      "The column Low has 1 unique values.\n",
      "The column High has 1 unique values.\n",
      "The column Comments has 1 unique values.\n",
      "\n",
      " - - - - - \n",
      " Cleansing source S-201 \n",
      "\n",
      "\n",
      " Calling function 'extract_who_raw_data'...\n",
      "\n",
      " Calling function 'rename_and_discard_columns'...\n",
      "\n",
      " Calling function 'extract_year_from_timeperiod'...\n",
      "\n",
      " TIME_PERIOD column contained time periods (no atomic years). Successfully extrated year. \n",
      "\n",
      " Calling function 'retrieve_latest_observation'...\n",
      "\n",
      " Calling function 'add_and_discard_countries'...\n",
      "\n",
      " Calling function 'add_cols_fill_cells'...\n",
      "\n",
      " Calling function 'map_values'...\n",
      "Values of column: DIM_SEX couldn't be mapped. If column DIM_SEX is present, there is an error with the code. \n",
      "Values of column: DIM_EDU_LEVEL couldn't be mapped. If column DIM_EDU_LEVEL is present, there is an error with the code. \n",
      "Values of column: DIM_AGE couldn't be mapped. If column DIM_AGE is present, there is an error with the code. \n",
      "Values of column: DIM_AGE_GROUP couldn't be mapped. If column DIM_AGE_GROUP is present, there is an error with the code. \n",
      "Values of column: DIM_MANAGEMENT_LEVEL couldn't be mapped. If column DIM_MANAGEMENT_LEVEL is present, there is an error with the code. \n",
      "Values of column: DIM_AREA_TYPE couldn't be mapped. If column DIM_AREA_TYPE is present, there is an error with the code. \n",
      "Values of column: DIM_QUANTILE couldn't be mapped. If column DIM_QUANTILE is present, there is an error with the code. \n",
      "Values of column: DIM_SDG_INDICATOR couldn't be mapped. If column DIM_SDG_INDICATOR is present, there is an error with the code. \n",
      "Values of column: DIM_OCU_TYPE couldn't be mapped. If column DIM_OCU_TYPE is present, there is an error with the code. \n",
      "Values of column: DIM_REP_TYPE couldn't be mapped. If column DIM_REP_TYPE is present, there is an error with the code. \n",
      "Values of column: DIM_SECTOR couldn't be mapped. If column DIM_SECTOR is present, there is an error with the code. \n",
      "Values of column: ATTR_UNIT_MEASURE couldn't be mapped. If column ATTR_UNIT_MEASURE is present, there is an error with the code. \n",
      "\n",
      " Calling function 'encode_categorical_variables'...\n",
      "Cleansing done. This is some basic information about the data: \n",
      " \n",
      " There are 195 rows in the dataframe and 0.0% have a NA-value in the column 'OBS_RAW_VALUE\n",
      "\n",
      " \n",
      " This is the summary of the column 'TIME_PERIOD': count     195.000000\n",
      "mean     2015.297436\n",
      "std         3.295380\n",
      "min      2013.000000\n",
      "25%      2013.000000\n",
      "50%      2013.000000\n",
      "75%      2020.000000\n",
      "max      2020.000000\n",
      "Name: TIME_PERIOD, dtype: float64\n",
      "\n",
      " - - - - - \n",
      " Extracting source S-207 \n",
      "\n",
      "The following columns are present in the datasets, and this is the number of unique values they have. \n",
      "The column GHO has 1 unique values.\n",
      "The column PUBLISHSTATE has 1 unique values.\n",
      "The column YEAR has 1 unique values.\n",
      "The column REGION has 6 unique values.\n",
      "The column COUNTRY has 194 unique values.\n",
      "The column ALCOHOLTYPE has 3 unique values.\n",
      "The column Display Value has 14 unique values.\n",
      "The column Numeric has 1 unique values.\n",
      "The column Low has 1 unique values.\n",
      "The column High has 1 unique values.\n",
      "The column Comments has 1 unique values.\n",
      "\n",
      " - - - - - \n",
      " Cleansing source S-207 \n",
      "\n",
      "\n",
      " Calling function 'extract_who_raw_data'...\n",
      "\n",
      " Calling function 'rename_and_discard_columns'...\n",
      "\n",
      " Calling function 'extract_year_from_timeperiod'...\n",
      "\n",
      " Calling function 'retrieve_latest_observation'...\n",
      "\n",
      " Calling function 'add_and_discard_countries'...\n",
      "\n",
      " Calling function 'add_cols_fill_cells'...\n",
      "\n",
      " Calling function 'map_values'...\n",
      "Values of column: DIM_SEX couldn't be mapped. If column DIM_SEX is present, there is an error with the code. \n",
      "Values of column: DIM_EDU_LEVEL couldn't be mapped. If column DIM_EDU_LEVEL is present, there is an error with the code. \n",
      "Values of column: DIM_AGE couldn't be mapped. If column DIM_AGE is present, there is an error with the code. \n",
      "Values of column: DIM_AGE_GROUP couldn't be mapped. If column DIM_AGE_GROUP is present, there is an error with the code. \n",
      "Values of column: DIM_MANAGEMENT_LEVEL couldn't be mapped. If column DIM_MANAGEMENT_LEVEL is present, there is an error with the code. \n",
      "Values of column: DIM_AREA_TYPE couldn't be mapped. If column DIM_AREA_TYPE is present, there is an error with the code. \n",
      "Values of column: DIM_QUANTILE couldn't be mapped. If column DIM_QUANTILE is present, there is an error with the code. \n",
      "Values of column: DIM_SDG_INDICATOR couldn't be mapped. If column DIM_SDG_INDICATOR is present, there is an error with the code. \n",
      "Values of column: DIM_OCU_TYPE couldn't be mapped. If column DIM_OCU_TYPE is present, there is an error with the code. \n",
      "Values of column: DIM_REP_TYPE couldn't be mapped. If column DIM_REP_TYPE is present, there is an error with the code. \n",
      "Values of column: DIM_SECTOR couldn't be mapped. If column DIM_SECTOR is present, there is an error with the code. \n",
      "Values of column: ATTR_UNIT_MEASURE couldn't be mapped. If column ATTR_UNIT_MEASURE is present, there is an error with the code. \n",
      "\n",
      " Calling function 'encode_categorical_variables'...\n",
      "Cleansing done. This is some basic information about the data: \n",
      " \n",
      " There are 579 rows in the dataframe and 0.0% have a NA-value in the column 'OBS_RAW_VALUE\n",
      "\n",
      " \n",
      " This is the summary of the column 'TIME_PERIOD': count     579.000000\n",
      "mean     2016.020725\n",
      "std         0.287428\n",
      "min      2016.000000\n",
      "25%      2016.000000\n",
      "50%      2016.000000\n",
      "75%      2016.000000\n",
      "max      2020.000000\n",
      "Name: TIME_PERIOD, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# CSV sources\n",
    "api_sources = crba_data_dictionary_source[\n",
    "    (crba_data_dictionary_source[\"SOURCE_TYPE\"] == \"API (ILO)\") | \n",
    "    (crba_data_dictionary_source[\"SOURCE_TYPE\"] == \"API (UNESCO)\") | \n",
    "    (crba_data_dictionary_source[\"SOURCE_TYPE\"] == \"API (WHO)\") | \n",
    "    (crba_data_dictionary_source[\"SOURCE_TYPE\"] == \"API (UNICEF)\")\n",
    "].merge(\n",
    "    right = crba_data_dictionary_snapshot,\n",
    "    on = \"SOURCE_ID\"\n",
    ").merge(\n",
    "    right = crba_data_dictionary_indicator,\n",
    "    on = 'INDICATOR_ID'\n",
    ")\n",
    "\n",
    "# # # # # # # # # # # #\n",
    "# Delete again (only for temporary debugging 12.11.20)\n",
    "# # # # # # # # # # # # \n",
    "# api_sources = api_sources[api_sources[\"SOURCE_ID\"] == 'S-101']\n",
    "\n",
    "# define emty dataframe\n",
    "combined_cleansed_csv = pd.DataFrame()\n",
    "combined_normalized_csv = pd.DataFrame()\n",
    "\n",
    "# Loop to extract data from API sources\n",
    "for index, row in api_sources.iterrows():\n",
    "    # Log\n",
    "    print(\"\\n - - - - - \\n Extracting source {} \\n\".format(row[\"SOURCE_ID\"]))\n",
    "    \n",
    "    # Extraction section\n",
    "    try:\n",
    "        # Extract data\n",
    "        dataframe = extract.CSVExtractor.extract(url = row[\"ENDPOINT_URL\"])\n",
    "        \n",
    "        # Save raw data\n",
    "        dataframe.to_csv(\n",
    "            data_sources_raw / str(row[\"SOURCE_ID\"] + \"_raw.csv\"),\n",
    "            sep = \";\"\n",
    "            )\n",
    "    \n",
    "    except:\n",
    "       print(\"There was a problem with extraction of source {} \\n\".format(row[\"SOURCE_ID\"]))\n",
    "    \n",
    "    \n",
    "    # Log that we are entering cleasning\n",
    "    print(\"\\n - - - - - \\n Cleansing source {} \\n\".format(row[\"SOURCE_ID\"]))\n",
    "    \n",
    "    # Cleansing\n",
    "    dataframe = cleanse.Cleanser().extract_who_raw_data(\n",
    "        raw_data=dataframe,\n",
    "        variable_type = row[\"VALUE_LABELS\"],\n",
    "        display_value_col=\"Display Value\"\n",
    "    )\n",
    "    \n",
    "    dataframe = cleanse.Cleanser().rename_and_discard_columns(\n",
    "        raw_data=dataframe,\n",
    "        mapping_dictionary=mapping_dict,\n",
    "        final_sdmx_col_list=sdmx_df_columns_all\n",
    "    )\n",
    "\n",
    "    dataframe = cleanse.Cleanser().extract_year_from_timeperiod(\n",
    "        dataframe=dataframe,\n",
    "        year_col=\"TIME_PERIOD\",\n",
    "        time_cov_col=\"COVERAGE_TIME\"\n",
    "    )\n",
    "\n",
    "    dataframe = cleanse.Cleanser().retrieve_latest_observation(\n",
    "        renamed_data=dataframe,\n",
    "        dim_cols = sdmx_df_columns_dims,\n",
    "        country_cols = sdmx_df_columns_country,\n",
    "        time_cols = sdmx_df_columns_time,\n",
    "        attr_cols=sdmx_df_columns_attr,\n",
    "    )\n",
    "\n",
    "    # print(dataframe)\n",
    "\n",
    "    dataframe = cleanse.Cleanser().add_and_discard_countries(\n",
    "        grouped_data=dataframe,\n",
    "        crba_country_list=country_crba_list,\n",
    "        country_list_full = country_full_list\n",
    "    )\n",
    "\n",
    "    dataframe = cleanse.Cleanser().add_cols_fill_cells(\n",
    "        grouped_data_iso_filt=dataframe,\n",
    "        dim_cols=sdmx_df_columns_dims,\n",
    "        time_cols=sdmx_df_columns_time,\n",
    "        indicator_name_string=row[\"INDICATOR_NAME_x\"],\n",
    "        index_name_string=row[\"INDEX\"],\n",
    "        issue_name_string=row[\"ISSUE\"],\n",
    "        category_name_string=row[\"CATEGORY\"],\n",
    "        indicator_code_string=row[\"INDICATOR_CODE\"],\n",
    "        indicator_source_string=row[\"ADDRESS\"],\n",
    "        indicator_source_body_string=row[\"SOURCE_BODY\"],\n",
    "        indicator_description_string=row[\"INDICATOR_DESCRIPTION\"],\n",
    "        indicator_explanation_string=row[\"INDICATOR_EXPLANATION\"],\n",
    "        indicator_data_extraction_methodology_string=row[\"EXTRACTION_METHODOLOGY\"],\n",
    "        source_title_string=row[\"SOURCE_TITLE\"],\n",
    "        source_api_link_string=row[\"ENDPOINT_URL\"]\n",
    "    )\n",
    "\n",
    "    dataframe = cleanse.Cleanser().map_values(\n",
    "        cleansed_data = dataframe,\n",
    "        value_mapping_dict = value_mapper\n",
    "    )\n",
    "    \n",
    "    dataframe_cleansed = cleanse.Cleanser().encode_categorical_variables(\n",
    "        dataframe = dataframe,\n",
    "        encoding_string = row[\"VALUE_ENCODING\"],\n",
    "        encoding_labels = row[\"VALUE_LABELS\"]\n",
    "    )\n",
    "\n",
    "    cleanse.Cleanser().create_log_report(\n",
    "        cleansed_data=dataframe_cleansed\n",
    "    )\n",
    "\n",
    "    # Append dataframe to combined dataframe\n",
    "    combined_cleansed_csv = combined_cleansed_csv.append(\n",
    "        other = dataframe_cleansed\n",
    "    )\n",
    "\n",
    "    # Save cleansed data\n",
    "    dataframe_cleansed.to_csv(\n",
    "        data_sources_cleansed / str(row[\"SOURCE_ID\"] + \"_cleansed.csv\"),\n",
    "        sep = \";\")\n",
    "    \n",
    "    # Normalizing\n",
    "    dataframe_normalized = scaler.normalizer(\n",
    "        cleansed_data = dataframe_cleansed,\n",
    "        sql_subset_query_string=row[\"DIMENSION_VALUES_NORMALIZATION\"],\n",
    "        # dim_cols=sdmx_df_columns_dims,\n",
    "        variable_type = row[\"VALUE_LABELS\"],\n",
    "        is_inverted = row[\"INVERT_NORMALIZATION\"],\n",
    "        whisker_factor=1.5,\n",
    "        raw_data_col=\"RAW_OBS_VALUE\",\n",
    "        scaled_data_col_name=\"SCALED_OBS_VALUE\",\n",
    "        maximum_score=10,\n",
    "        )\n",
    "    \n",
    "    dataframe_normalized.to_csv(\n",
    "        data_sources_normalized / str(row[\"SOURCE_ID\"] + \"_normalized.csv\"),\n",
    "        sep = \";\")\n",
    "\n",
    "    # Append dataframe to combined dataframe\n",
    "    combined_normalized_csv = combined_normalized_csv.append(\n",
    "        other = dataframe_normalized\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON API sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "EASURE\n",
      "\n",
      " Calling function 'encode_categorical_variables'...\n",
      "Cleansing done. This is some basic information about the data: \n",
      " \n",
      " There are 391 rows in the dataframe and 24.81% have a NA-value in the column 'OBS_RAW_VALUE\n",
      "\n",
      " \n",
      " This is the summary of the column 'TIME_PERIOD': count     391.000000\n",
      "mean     2017.092072\n",
      "std         3.225971\n",
      "min      2007.000000\n",
      "25%      2016.000000\n",
      "50%      2018.000000\n",
      "75%      2019.000000\n",
      "max      2020.000000\n",
      "Name: TIME_PERIOD, dtype: float64\n",
      "\n",
      " - - - - - \n",
      " Extracting source S-186 \n",
      "\n",
      "The following columns are present in the datasets, and this is the number of unique values they have. \n",
      "The column goal has 1 unique values.\n",
      "The column target has 1 unique values.\n",
      "The column indicator has 1 unique values.\n",
      "The column series has 1 unique values.\n",
      "The column seriesDescription has 1 unique values.\n",
      "The column seriesCount has 1 unique values.\n",
      "The column geoAreaCode has 95 unique values.\n",
      "The column geoAreaName has 95 unique values.\n",
      "The column timePeriodStart has 15 unique values.\n",
      "The column value has 84 unique values.\n",
      "The column valueType has 1 unique values.\n",
      "The column time_detail has 1 unique values.\n",
      "The column timeCoverage has 1 unique values.\n",
      "The column upperBound has 1 unique values.\n",
      "The column lowerBound has 1 unique values.\n",
      "The column basePeriod has 1 unique values.\n",
      "The column source has 1 unique values.\n",
      "The column geoInfoUrl has 1 unique values.\n",
      "The column footnotes has 1 unique values.\n",
      "The column attributes.Nature has 1 unique values.\n",
      "The column attributes.Units has 1 unique values.\n",
      "The column dimensions.Reporting Type has 1 unique values.\n",
      "\n",
      " - - - - - \n",
      " Cleansing source S-186 \n",
      "\n",
      "\n",
      " Calling function 'extract_who_raw_data'...\n",
      "\n",
      " Calling function 'rename_and_discard_columns'...\n",
      "\n",
      " Calling function 'extract_year_from_timeperiod'...\n",
      "\n",
      " Calling function 'retrieve_latest_observation'...\n",
      "\n",
      " Calling function 'add_and_discard_countries'...\n",
      "\n",
      " Calling function 'add_cols_fill_cells'...\n",
      "\n",
      " Calling function 'map_values'...\n",
      "Values of column: DIM_SEX couldn't be mapped. If column DIM_SEX is present, there is an error with the code. \n",
      "Values of column: DIM_EDU_LEVEL couldn't be mapped. If column DIM_EDU_LEVEL is present, there is an error with the code. \n",
      "Values of column: DIM_AGE couldn't be mapped. If column DIM_AGE is present, there is an error with the code. \n",
      "Values of column: DIM_AGE_GROUP couldn't be mapped. If column DIM_AGE_GROUP is present, there is an error with the code. \n",
      "Values of column: DIM_MANAGEMENT_LEVEL couldn't be mapped. If column DIM_MANAGEMENT_LEVEL is present, there is an error with the code. \n",
      "Values of column: DIM_AREA_TYPE couldn't be mapped. If column DIM_AREA_TYPE is present, there is an error with the code. \n",
      "Values of column: DIM_QUANTILE couldn't be mapped. If column DIM_QUANTILE is present, there is an error with the code. \n",
      "\n",
      " Successfully mapped values of column: DIM_SDG_INDICATOR\n",
      "Values of column: DIM_OCU_TYPE couldn't be mapped. If column DIM_OCU_TYPE is present, there is an error with the code. \n",
      "\n",
      " Successfully mapped values of column: DIM_REP_TYPE\n",
      "Values of column: DIM_SECTOR couldn't be mapped. If column DIM_SECTOR is present, there is an error with the code. \n",
      "\n",
      " Successfully mapped values of column: ATTR_UNIT_MEASURE\n",
      "\n",
      " Calling function 'encode_categorical_variables'...\n",
      "Cleansing done. This is some basic information about the data: \n",
      " \n",
      " There are 195 rows in the dataframe and 51.79% have a NA-value in the column 'OBS_RAW_VALUE\n",
      "\n",
      " \n",
      " This is the summary of the column 'TIME_PERIOD': count     195.000000\n",
      "mean     2017.984615\n",
      "std         3.107153\n",
      "min      2005.000000\n",
      "25%      2017.000000\n",
      "50%      2020.000000\n",
      "75%      2020.000000\n",
      "max      2020.000000\n",
      "Name: TIME_PERIOD, dtype: float64\n",
      "\n",
      " - - - - - \n",
      " Extracting source S-187 \n",
      "\n",
      "The following columns are present in the datasets, and this is the number of unique values they have. \n",
      "The column goal has 1 unique values.\n",
      "The column target has 1 unique values.\n",
      "The column indicator has 1 unique values.\n",
      "The column series has 1 unique values.\n",
      "The column seriesDescription has 1 unique values.\n",
      "The column seriesCount has 1 unique values.\n",
      "The column geoAreaCode has 87 unique values.\n",
      "The column geoAreaName has 87 unique values.\n",
      "The column timePeriodStart has 15 unique values.\n",
      "The column value has 88 unique values.\n",
      "The column valueType has 1 unique values.\n",
      "The column time_detail has 1 unique values.\n",
      "The column timeCoverage has 1 unique values.\n",
      "The column upperBound has 1 unique values.\n",
      "The column lowerBound has 1 unique values.\n",
      "The column basePeriod has 1 unique values.\n",
      "The column source has 1 unique values.\n",
      "The column geoInfoUrl has 1 unique values.\n",
      "The column footnotes has 1 unique values.\n",
      "The column attributes.Nature has 1 unique values.\n",
      "The column attributes.Units has 1 unique values.\n",
      "The column dimensions.Reporting Type has 1 unique values.\n",
      "\n",
      " - - - - - \n",
      " Cleansing source S-187 \n",
      "\n",
      "\n",
      " Calling function 'extract_who_raw_data'...\n",
      "\n",
      " Calling function 'rename_and_discard_columns'...\n",
      "\n",
      " Calling function 'extract_year_from_timeperiod'...\n",
      "\n",
      " Calling function 'retrieve_latest_observation'...\n",
      "\n",
      " Calling function 'add_and_discard_countries'...\n",
      "\n",
      " Calling function 'add_cols_fill_cells'...\n",
      "\n",
      " Calling function 'map_values'...\n",
      "Values of column: DIM_SEX couldn't be mapped. If column DIM_SEX is present, there is an error with the code. \n",
      "Values of column: DIM_EDU_LEVEL couldn't be mapped. If column DIM_EDU_LEVEL is present, there is an error with the code. \n",
      "Values of column: DIM_AGE couldn't be mapped. If column DIM_AGE is present, there is an error with the code. \n",
      "Values of column: DIM_AGE_GROUP couldn't be mapped. If column DIM_AGE_GROUP is present, there is an error with the code. \n",
      "Values of column: DIM_MANAGEMENT_LEVEL couldn't be mapped. If column DIM_MANAGEMENT_LEVEL is present, there is an error with the code. \n",
      "Values of column: DIM_AREA_TYPE couldn't be mapped. If column DIM_AREA_TYPE is present, there is an error with the code. \n",
      "Values of column: DIM_QUANTILE couldn't be mapped. If column DIM_QUANTILE is present, there is an error with the code. \n",
      "\n",
      " Successfully mapped values of column: DIM_SDG_INDICATOR\n",
      "Values of column: DIM_OCU_TYPE couldn't be mapped. If column DIM_OCU_TYPE is present, there is an error with the code. \n",
      "\n",
      " Successfully mapped values of column: DIM_REP_TYPE\n",
      "Values of column: DIM_SECTOR couldn't be mapped. If column DIM_SECTOR is present, there is an error with the code. \n",
      "\n",
      " Successfully mapped values of column: ATTR_UNIT_MEASURE\n",
      "\n",
      " Calling function 'encode_categorical_variables'...\n",
      "Cleansing done. This is some basic information about the data: \n",
      " \n",
      " There are 195 rows in the dataframe and 55.9% have a NA-value in the column 'OBS_RAW_VALUE\n",
      "\n",
      " \n",
      " This is the summary of the column 'TIME_PERIOD': count     195.000000\n",
      "mean     2017.974359\n",
      "std         3.135161\n",
      "min      2007.000000\n",
      "25%      2017.000000\n",
      "50%      2020.000000\n",
      "75%      2020.000000\n",
      "max      2020.000000\n",
      "Name: TIME_PERIOD, dtype: float64\n",
      "\n",
      " - - - - - \n",
      " Extracting source S-188 \n",
      "\n",
      "The following columns are present in the datasets, and this is the number of unique values they have. \n",
      "The column goal has 1 unique values.\n",
      "The column target has 1 unique values.\n",
      "The column indicator has 1 unique values.\n",
      "The column series has 1 unique values.\n",
      "The column seriesDescription has 1 unique values.\n",
      "The column seriesCount has 1 unique values.\n",
      "The column geoAreaCode has 110 unique values.\n",
      "The column geoAreaName has 110 unique values.\n",
      "The column timePeriodStart has 15 unique values.\n",
      "The column value has 79 unique values.\n",
      "The column valueType has 1 unique values.\n",
      "The column time_detail has 1 unique values.\n",
      "The column timeCoverage has 1 unique values.\n",
      "The column upperBound has 1 unique values.\n",
      "The column lowerBound has 1 unique values.\n",
      "The column basePeriod has 1 unique values.\n",
      "The column source has 1 unique values.\n",
      "The column geoInfoUrl has 1 unique values.\n",
      "The column footnotes has 1 unique values.\n",
      "The column attributes.Nature has 1 unique values.\n",
      "The column attributes.Units has 1 unique values.\n",
      "The column dimensions.Reporting Type has 1 unique values.\n",
      "\n",
      " - - - - - \n",
      " Cleansing source S-188 \n",
      "\n",
      "\n",
      " Calling function 'extract_who_raw_data'...\n",
      "\n",
      " Calling function 'rename_and_discard_columns'...\n",
      "\n",
      " Calling function 'extract_year_from_timeperiod'...\n",
      "\n",
      " Calling function 'retrieve_latest_observation'...\n",
      "\n",
      " Calling function 'add_and_discard_countries'...\n",
      "\n",
      " Calling function 'add_cols_fill_cells'...\n",
      "\n",
      " Calling function 'map_values'...\n",
      "Values of column: DIM_SEX couldn't be mapped. If column DIM_SEX is present, there is an error with the code. \n",
      "Values of column: DIM_EDU_LEVEL couldn't be mapped. If column DIM_EDU_LEVEL is present, there is an error with the code. \n",
      "Values of column: DIM_AGE couldn't be mapped. If column DIM_AGE is present, there is an error with the code. \n",
      "Values of column: DIM_AGE_GROUP couldn't be mapped. If column DIM_AGE_GROUP is present, there is an error with the code. \n",
      "Values of column: DIM_MANAGEMENT_LEVEL couldn't be mapped. If column DIM_MANAGEMENT_LEVEL is present, there is an error with the code. \n",
      "Values of column: DIM_AREA_TYPE couldn't be mapped. If column DIM_AREA_TYPE is present, there is an error with the code. \n",
      "Values of column: DIM_QUANTILE couldn't be mapped. If column DIM_QUANTILE is present, there is an error with the code. \n",
      "\n",
      " Successfully mapped values of column: DIM_SDG_INDICATOR\n",
      "Values of column: DIM_OCU_TYPE couldn't be mapped. If column DIM_OCU_TYPE is present, there is an error with the code. \n",
      "\n",
      " Successfully mapped values of column: DIM_REP_TYPE\n",
      "Values of column: DIM_SECTOR couldn't be mapped. If column DIM_SECTOR is present, there is an error with the code. \n",
      "\n",
      " Successfully mapped values of column: ATTR_UNIT_MEASURE\n",
      "\n",
      " Calling function 'encode_categorical_variables'...\n",
      "Cleansing done. This is some basic information about the data: \n",
      " \n",
      " There are 195 rows in the dataframe and 43.59% have a NA-value in the column 'OBS_RAW_VALUE\n",
      "\n",
      " \n",
      " This is the summary of the column 'TIME_PERIOD': count     195.000000\n",
      "mean     2018.974359\n",
      "std         1.123488\n",
      "min      2015.000000\n",
      "25%      2018.000000\n",
      "50%      2019.000000\n",
      "75%      2020.000000\n",
      "max      2020.000000\n",
      "Name: TIME_PERIOD, dtype: float64\n",
      "\n",
      " - - - - - \n",
      " Extracting source S-198 \n",
      "\n",
      "The following columns are present in the datasets, and this is the number of unique values they have. \n",
      "The column goal has 1 unique values.\n",
      "The column target has 1 unique values.\n",
      "The column indicator has 1 unique values.\n",
      "The column series has 1 unique values.\n",
      "The column seriesDescription has 1 unique values.\n",
      "The column seriesCount has 1 unique values.\n",
      "The column geoAreaCode has 181 unique values.\n",
      "The column geoAreaName has 181 unique values.\n",
      "The column timePeriodStart has 20 unique values.\n",
      "The column value has 2025 unique values.\n",
      "The column valueType has 1 unique values.\n",
      "The column time_detail has 1 unique values.\n",
      "The column timeCoverage has 1 unique values.\n",
      "The column upperBound has 1 unique values.\n",
      "The column lowerBound has 1 unique values.\n",
      "The column basePeriod has 1 unique values.\n",
      "The column source has 1 unique values.\n",
      "The column geoInfoUrl has 1 unique values.\n",
      "The column footnotes has 1 unique values.\n",
      "The column attributes.Nature has 3 unique values.\n",
      "The column attributes.Units has 1 unique values.\n",
      "The column dimensions.Reporting Type has 1 unique values.\n",
      "\n",
      " - - - - - \n",
      " Cleansing source S-198 \n",
      "\n",
      "\n",
      " Calling function 'extract_who_raw_data'...\n",
      "\n",
      " Calling function 'rename_and_discard_columns'...\n",
      "\n",
      " Calling function 'extract_year_from_timeperiod'...\n",
      "\n",
      " Calling function 'retrieve_latest_observation'...\n",
      "\n",
      " Calling function 'add_and_discard_countries'...\n",
      "\n",
      " Calling function 'add_cols_fill_cells'...\n",
      "\n",
      " Calling function 'map_values'...\n",
      "Values of column: DIM_SEX couldn't be mapped. If column DIM_SEX is present, there is an error with the code. \n",
      "Values of column: DIM_EDU_LEVEL couldn't be mapped. If column DIM_EDU_LEVEL is present, there is an error with the code. \n",
      "Values of column: DIM_AGE couldn't be mapped. If column DIM_AGE is present, there is an error with the code. \n",
      "Values of column: DIM_AGE_GROUP couldn't be mapped. If column DIM_AGE_GROUP is present, there is an error with the code. \n",
      "Values of column: DIM_MANAGEMENT_LEVEL couldn't be mapped. If column DIM_MANAGEMENT_LEVEL is present, there is an error with the code. \n",
      "Values of column: DIM_AREA_TYPE couldn't be mapped. If column DIM_AREA_TYPE is present, there is an error with the code. \n",
      "Values of column: DIM_QUANTILE couldn't be mapped. If column DIM_QUANTILE is present, there is an error with the code. \n",
      "\n",
      " Successfully mapped values of column: DIM_SDG_INDICATOR\n",
      "Values of column: DIM_OCU_TYPE couldn't be mapped. If column DIM_OCU_TYPE is present, there is an error with the code. \n",
      "\n",
      " Successfully mapped values of column: DIM_REP_TYPE\n",
      "Values of column: DIM_SECTOR couldn't be mapped. If column DIM_SECTOR is present, there is an error with the code. \n",
      "\n",
      " Successfully mapped values of column: ATTR_UNIT_MEASURE\n",
      "\n",
      " Calling function 'encode_categorical_variables'...\n",
      "Cleansing done. This is some basic information about the data: \n",
      " \n",
      " There are 195 rows in the dataframe and 10.77% have a NA-value in the column 'OBS_RAW_VALUE\n",
      "\n",
      " \n",
      " This is the summary of the column 'TIME_PERIOD': count     195.000000\n",
      "mean     2015.589744\n",
      "std         4.092475\n",
      "min      2000.000000\n",
      "25%      2015.000000\n",
      "50%      2017.000000\n",
      "75%      2018.000000\n",
      "max      2020.000000\n",
      "Name: TIME_PERIOD, dtype: float64\n",
      "\n",
      " - - - - - \n",
      " Extracting source S-202 \n",
      "\n",
      "The following columns are present in the datasets, and this is the number of unique values they have. \n",
      "The column goal has 1 unique values.\n",
      "The column target has 1 unique values.\n",
      "The column indicator has 1 unique values.\n",
      "The column series has 1 unique values.\n",
      "The column seriesDescription has 1 unique values.\n",
      "The column seriesCount has 1 unique values.\n",
      "The column geoAreaCode has 141 unique values.\n",
      "The column geoAreaName has 141 unique values.\n",
      "The column timePeriodStart has 18 unique values.\n",
      "The column value has 631 unique values.\n",
      "The column valueType has 1 unique values.\n",
      "The column time_detail has 1 unique values.\n",
      "The column timeCoverage has 1 unique values.\n",
      "The column upperBound has 1 unique values.\n",
      "The column lowerBound has 1 unique values.\n",
      "The column basePeriod has 1 unique values.\n",
      "The column source has 150 unique values.\n",
      "The column geoInfoUrl has 1 unique values.\n",
      "The column footnotes has 157 unique values.\n",
      "The column attributes.Nature has 1 unique values.\n",
      "The column attributes.Units has 1 unique values.\n",
      "The column dimensions.Reporting Type has 1 unique values.\n",
      "The column dimensions.Quantile has 2 unique values.\n",
      "\n",
      " - - - - - \n",
      " Cleansing source S-202 \n",
      "\n",
      "\n",
      " Calling function 'extract_who_raw_data'...\n",
      "\n",
      " Calling function 'rename_and_discard_columns'...\n",
      "\n",
      " Calling function 'extract_year_from_timeperiod'...\n",
      "\n",
      " Calling function 'retrieve_latest_observation'...\n",
      "\n",
      " Calling function 'add_and_discard_countries'...\n",
      "\n",
      " Calling function 'add_cols_fill_cells'...\n",
      "\n",
      " Calling function 'map_values'...\n",
      "Values of column: DIM_SEX couldn't be mapped. If column DIM_SEX is present, there is an error with the code. \n",
      "Values of column: DIM_EDU_LEVEL couldn't be mapped. If column DIM_EDU_LEVEL is present, there is an error with the code. \n",
      "Values of column: DIM_AGE couldn't be mapped. If column DIM_AGE is present, there is an error with the code. \n",
      "Values of column: DIM_AGE_GROUP couldn't be mapped. If column DIM_AGE_GROUP is present, there is an error with the code. \n",
      "Values of column: DIM_MANAGEMENT_LEVEL couldn't be mapped. If column DIM_MANAGEMENT_LEVEL is present, there is an error with the code. \n",
      "Values of column: DIM_AREA_TYPE couldn't be mapped. If column DIM_AREA_TYPE is present, there is an error with the code. \n",
      "\n",
      " Successfully mapped values of column: DIM_QUANTILE\n",
      "\n",
      " Successfully mapped values of column: DIM_SDG_INDICATOR\n",
      "Values of column: DIM_OCU_TYPE couldn't be mapped. If column DIM_OCU_TYPE is present, there is an error with the code. \n",
      "\n",
      " Successfully mapped values of column: DIM_REP_TYPE\n",
      "Values of column: DIM_SECTOR couldn't be mapped. If column DIM_SECTOR is present, there is an error with the code. \n",
      "\n",
      " Successfully mapped values of column: ATTR_UNIT_MEASURE\n",
      "\n",
      " Calling function 'encode_categorical_variables'...\n",
      "Cleansing done. This is some basic information about the data: \n",
      " \n",
      " There are 303 rows in the dataframe and 28.71% have a NA-value in the column 'OBS_RAW_VALUE\n",
      "\n",
      " \n",
      " This is the summary of the column 'TIME_PERIOD': count     303.000000\n",
      "mean     2013.676568\n",
      "std         5.114679\n",
      "min      1999.000000\n",
      "25%      2010.000000\n",
      "50%      2014.000000\n",
      "75%      2020.000000\n",
      "max      2020.000000\n",
      "Name: TIME_PERIOD, dtype: float64\n",
      "\n",
      " - - - - - \n",
      " Extracting source S-203 \n",
      "\n",
      "The following columns are present in the datasets, and this is the number of unique values they have. \n",
      "The column goal has 1 unique values.\n",
      "The column target has 1 unique values.\n",
      "The column indicator has 1 unique values.\n",
      "The column series has 1 unique values.\n",
      "The column seriesDescription has 1 unique values.\n",
      "The column seriesCount has 1 unique values.\n",
      "The column geoAreaCode has 65 unique values.\n",
      "The column geoAreaName has 65 unique values.\n",
      "The column timePeriodStart has 19 unique values.\n",
      "The column value has 3381 unique values.\n",
      "The column valueType has 1 unique values.\n",
      "The column time_detail has 1 unique values.\n",
      "The column timeCoverage has 1 unique values.\n",
      "The column upperBound has 1 unique values.\n",
      "The column lowerBound has 1 unique values.\n",
      "The column basePeriod has 1 unique values.\n",
      "The column source has 46 unique values.\n",
      "The column geoInfoUrl has 1 unique values.\n",
      "The column footnotes has 128 unique values.\n",
      "The column attributes.Nature has 1 unique values.\n",
      "The column attributes.Units has 1 unique values.\n",
      "The column dimensions.Sex has 3 unique values.\n",
      "The column dimensions.Type of occupation has 24 unique values.\n",
      "The column dimensions.Reporting Type has 1 unique values.\n",
      "\n",
      " - - - - - \n",
      " Cleansing source S-203 \n",
      "\n",
      "\n",
      " Calling function 'extract_who_raw_data'...\n",
      "\n",
      " Calling function 'rename_and_discard_columns'...\n",
      "\n",
      " Calling function 'extract_year_from_timeperiod'...\n",
      "\n",
      " Calling function 'retrieve_latest_observation'...\n",
      "\n",
      " Calling function 'add_and_discard_countries'...\n",
      "\n",
      " Calling function 'add_cols_fill_cells'...\n",
      "\n",
      " Calling function 'map_values'...\n",
      "\n",
      " Successfully mapped values of column: DIM_SEX\n",
      "Values of column: DIM_EDU_LEVEL couldn't be mapped. If column DIM_EDU_LEVEL is present, there is an error with the code. \n",
      "Values of column: DIM_AGE couldn't be mapped. If column DIM_AGE is present, there is an error with the code. \n",
      "Values of column: DIM_AGE_GROUP couldn't be mapped. If column DIM_AGE_GROUP is present, there is an error with the code. \n",
      "Values of column: DIM_MANAGEMENT_LEVEL couldn't be mapped. If column DIM_MANAGEMENT_LEVEL is present, there is an error with the code. \n",
      "Values of column: DIM_AREA_TYPE couldn't be mapped. If column DIM_AREA_TYPE is present, there is an error with the code. \n",
      "Values of column: DIM_QUANTILE couldn't be mapped. If column DIM_QUANTILE is present, there is an error with the code. \n",
      "\n",
      " Successfully mapped values of column: DIM_SDG_INDICATOR\n",
      "\n",
      " Successfully mapped values of column: DIM_OCU_TYPE\n",
      "\n",
      " Successfully mapped values of column: DIM_REP_TYPE\n",
      "Values of column: DIM_SECTOR couldn't be mapped. If column DIM_SECTOR is present, there is an error with the code. \n",
      "\n",
      " Successfully mapped values of column: ATTR_UNIT_MEASURE\n",
      "\n",
      " Calling function 'encode_categorical_variables'...\n",
      "Cleansing done. This is some basic information about the data: \n",
      " \n",
      " There are 2294 rows in the dataframe and 5.71% have a NA-value in the column 'OBS_RAW_VALUE\n",
      "\n",
      " \n",
      " This is the summary of the column 'TIME_PERIOD': count    2294.000000\n",
      "mean     2013.275065\n",
      "std         4.840024\n",
      "min      2001.000000\n",
      "25%      2006.000000\n",
      "50%      2014.000000\n",
      "75%      2017.000000\n",
      "max      2020.000000\n",
      "Name: TIME_PERIOD, dtype: float64\n",
      "\n",
      " - - - - - \n",
      " Extracting source S-204 \n",
      "\n",
      "There was an issue with source S-204\n",
      "\n",
      " - - - - - \n",
      " Cleansing source S-204 \n",
      "\n",
      "\n",
      " Calling function 'extract_who_raw_data'...\n",
      "\n",
      " Calling function 'rename_and_discard_columns'...\n",
      "\n",
      " Calling function 'extract_year_from_timeperiod'...\n",
      "\n",
      " Calling function 'retrieve_latest_observation'...\n",
      "\n",
      " Calling function 'add_and_discard_countries'...\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "'COUNTRY_ISO_3'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-23fe382f1a5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m     )\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     dataframe = cleanse.Cleanser().add_and_discard_countries(\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mgrouped_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mcrba_country_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcountry_crba_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspaces/data-etl/cleanse/__init__.py\u001b[0m in \u001b[0;36madd_and_discard_countries\u001b[0;34m(cls, grouped_data, crba_country_list, country_list_full)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0;31m# Discard countries that aren't part of the final CRBA master list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m             grouped_data_iso_filt = grouped_data_iso.merge(\n\u001b[0m\u001b[1;32m    316\u001b[0m                 \u001b[0mright\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcrba_country_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"COUNTRY_ISO_3\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m                 \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"right\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m   7948\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7950\u001b[0;31m         return merge(\n\u001b[0m\u001b[1;32m   7951\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7952\u001b[0m             \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m ) -> \"DataFrame\":\n\u001b[0;32m---> 74\u001b[0;31m     op = _MergeOperation(\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    650\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 652\u001b[0;31m         ) = self._get_merge_keys()\n\u001b[0m\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0;31m# validate the merge keys dtypes. We may need to coerce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m_get_merge_keys\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1016\u001b[0m                         \u001b[0mright_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mlk\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1018\u001b[0;31m                         \u001b[0mleft_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1019\u001b[0m                         \u001b[0mjoin_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_label_or_level_values\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1561\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1562\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1563\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1565\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'COUNTRY_ISO_3'"
     ]
    }
   ],
   "source": [
    "# JSON sources\n",
    "api_sources = crba_data_dictionary_source[\n",
    "    (crba_data_dictionary_source[\"SOURCE_TYPE\"] == \"API (SDG)\")\n",
    "].merge(\n",
    "    right = crba_data_dictionary_snapshot,\n",
    "    on = \"SOURCE_ID\"\n",
    ").merge(\n",
    "    right = crba_data_dictionary_indicator,\n",
    "    on = 'INDICATOR_ID'\n",
    ")\n",
    "\n",
    "# Loop to extract data from API sources\n",
    "for index, row in api_sources.iterrows():\n",
    "    # Log\n",
    "    print(\"\\n - - - - - \\n Extracting source {} \\n\".format(row[\"SOURCE_ID\"]))\n",
    "    \n",
    "    # Exraction section\n",
    "    try:\n",
    "        # Extract data \n",
    "        dataframe = extract.JSONExtractor.extract(url = row[\"ENDPOINT_URL\"])\n",
    "        \n",
    "        # Save dataframe\n",
    "        dataframe.to_csv(\n",
    "            data_sources_raw / str(row[\"SOURCE_ID\"] + \"_raw.csv\"),\n",
    "            sep = \";\")\n",
    "    except:\n",
    "        print(\"There was an issue with source {}\".format(row[\"SOURCE_ID\"]))\n",
    "    \n",
    "    # Log that we are entering cleasning\n",
    "    print(\"\\n - - - - - \\n Cleansing source {} \\n\".format(row[\"SOURCE_ID\"]))\n",
    "    \n",
    "    # Cleansing in \n",
    "    dataframe = cleanse.Cleanser().extract_who_raw_data(\n",
    "        raw_data=dataframe,\n",
    "        variable_type = row[\"VALUE_LABELS\"],\n",
    "        display_value_col=\"Display Value\"\n",
    "    )\n",
    "    \n",
    "    dataframe = cleanse.Cleanser().rename_and_discard_columns(\n",
    "        raw_data=dataframe,\n",
    "        mapping_dictionary=mapping_dict,\n",
    "        final_sdmx_col_list=sdmx_df_columns_all\n",
    "    )\n",
    "\n",
    "    dataframe = cleanse.Cleanser().extract_year_from_timeperiod(\n",
    "        dataframe=dataframe,\n",
    "        year_col=\"TIME_PERIOD\",\n",
    "        time_cov_col=\"COVERAGE_TIME\"\n",
    "    )\n",
    "\n",
    "    dataframe = cleanse.Cleanser().retrieve_latest_observation(\n",
    "        renamed_data=dataframe,\n",
    "        dim_cols = sdmx_df_columns_dims,\n",
    "        country_cols = sdmx_df_columns_country,\n",
    "        time_cols = sdmx_df_columns_time,\n",
    "        attr_cols=sdmx_df_columns_attr,\n",
    "    )\n",
    "\n",
    "    dataframe = cleanse.Cleanser().add_and_discard_countries(\n",
    "        grouped_data=dataframe,\n",
    "        crba_country_list=country_crba_list,\n",
    "        country_list_full = country_full_list\n",
    "    )\n",
    "\n",
    "    dataframe = cleanse.Cleanser().add_cols_fill_cells(\n",
    "        grouped_data_iso_filt=dataframe,\n",
    "        dim_cols=sdmx_df_columns_dims,\n",
    "        time_cols=sdmx_df_columns_time,\n",
    "        indicator_name_string=row[\"INDICATOR_NAME_x\"],\n",
    "        index_name_string=row[\"INDEX\"],\n",
    "        issue_name_string=row[\"ISSUE\"],\n",
    "        category_name_string=row[\"CATEGORY\"],\n",
    "        indicator_code_string=row[\"INDICATOR_CODE\"],\n",
    "        indicator_source_string=row[\"ADDRESS\"],\n",
    "        indicator_source_body_string=row[\"SOURCE_BODY\"],\n",
    "        indicator_description_string=row[\"INDICATOR_DESCRIPTION\"],\n",
    "        source_title_string=row[\"SOURCE_TITLE\"],\n",
    "        indicator_explanation_string=row[\"INDICATOR_EXPLANATION\"],\n",
    "        indicator_data_extraction_methodology_string=row[\"EXTRACTION_METHODOLOGY\"],\n",
    "        source_api_link_string=row[\"ENDPOINT_URL\"]\n",
    "    )\n",
    "\n",
    "    dataframe = cleanse.Cleanser().map_values(\n",
    "        cleansed_data = dataframe,\n",
    "        value_mapping_dict = value_mapper\n",
    "    )\n",
    "    \n",
    "    dataframe_cleansed = cleanse.Cleanser().encode_categorical_variables(\n",
    "        dataframe = dataframe,\n",
    "        encoding_string = row[\"VALUE_ENCODING\"],\n",
    "        encoding_labels = row[\"VALUE_LABELS\"]\n",
    "    )\n",
    "\n",
    "    cleanse.Cleanser().create_log_report(\n",
    "        cleansed_data=dataframe_cleansed\n",
    "    )\n",
    "\n",
    "    # Append dataframe to combined dataframe\n",
    "    combined_cleansed_csv = combined_cleansed_csv.append(\n",
    "        other = dataframe_cleansed\n",
    "    )\n",
    "\n",
    "    # Save cleansed data\n",
    "    dataframe_cleansed.to_csv(\n",
    "        data_sources_cleansed / str(row[\"SOURCE_ID\"] + \"_cleansed.csv\"),\n",
    "        sep = \";\")\n",
    "    \n",
    "    # Normalizing section\n",
    "    dataframe_normalized = scaler.normalizer(\n",
    "        cleansed_data = dataframe_cleansed,\n",
    "        sql_subset_query_string=row[\"DIMENSION_VALUES_NORMALIZATION\"],\n",
    "        # dim_cols=sdmx_df_columns_dims,\n",
    "        variable_type = row[\"VALUE_LABELS\"],\n",
    "        is_inverted = row[\"INVERT_NORMALIZATION\"],\n",
    "        whisker_factor=1.5,\n",
    "        raw_data_col=\"RAW_OBS_VALUE\",\n",
    "        scaled_data_col_name=\"SCALED_OBS_VALUE\",\n",
    "        maximum_score=10,\n",
    "        )\n",
    "    \n",
    "    dataframe_normalized.to_csv(\n",
    "        data_sources_normalized / str(row[\"SOURCE_ID\"] + \"_normalized.csv\"),\n",
    "        sep = \";\")\n",
    "\n",
    "    # Append dataframe to combined dataframe\n",
    "    combined_normalized_csv = combined_normalized_csv.append(\n",
    "        other = dataframe_normalized\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML Sources\n",
    "### UN Treaty Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      " - - - - - \n",
      " Extracting source S-4 \n",
      "\n",
      "The following columns are present in the datasets, and this is the number of unique values they have. \n",
      "The column Participant has 180 unique values.\n",
      "The column Signature has 35 unique values.\n",
      "The column Ratification, Acceptance(A), Approval(AA), Accession(a), Succession(d) has 172 unique values.\n",
      "\n",
      " - - - - - \n",
      " Cleansing source S-4 \n",
      "\n",
      "\n",
      " Calling function 'rename_and_discard_columns'...\n",
      "\n",
      " Calling function 'add_and_discard_countries'...\n",
      "\n",
      " Calling function 'add_cols_fill_cells'...\n",
      "\n",
      " Calling function 'encode_ilo_un_treaty_data'...\n",
      "Cleansing done. This is some basic information about the data: \n",
      " \n",
      " There are 195 rows in the dataframe and 0.0% have a NA-value in the column 'OBS_RAW_VALUE\n",
      "\n",
      " \n",
      " This is the summary of the column 'TIME_PERIOD': count     195.0\n",
      "mean     2020.0\n",
      "std         0.0\n",
      "min      2020.0\n",
      "25%      2020.0\n",
      "50%      2020.0\n",
      "75%      2020.0\n",
      "max      2020.0\n",
      "Name: TIME_PERIOD, dtype: float64\n",
      "\n",
      " - - - - - \n",
      " Extracting source S-31 \n",
      "\n",
      "The following columns are present in the datasets, and this is the number of unique values they have. \n",
      "The column Participant has 68 unique values.\n",
      "The column Signature, Succession to signature(d) has 35 unique values.\n",
      "The column Ratification, Accession(a), Succession(d) has 55 unique values.\n",
      "\n",
      " - - - - - \n",
      " Cleansing source S-31 \n",
      "\n",
      "\n",
      " Calling function 'rename_and_discard_columns'...\n",
      "\n",
      " Calling function 'add_and_discard_countries'...\n",
      "\n",
      " Calling function 'add_cols_fill_cells'...\n",
      "\n",
      " Calling function 'encode_ilo_un_treaty_data'...\n",
      "Cleansing done. This is some basic information about the data: \n",
      " \n",
      " There are 195 rows in the dataframe and 0.0% have a NA-value in the column 'OBS_RAW_VALUE\n",
      "\n",
      " \n",
      " This is the summary of the column 'TIME_PERIOD': count     195.0\n",
      "mean     2020.0\n",
      "std         0.0\n",
      "min      2020.0\n",
      "25%      2020.0\n",
      "50%      2020.0\n",
      "75%      2020.0\n",
      "max      2020.0\n",
      "Name: TIME_PERIOD, dtype: float64\n",
      "\n",
      " - - - - - \n",
      " Extracting source S-84 \n",
      "\n",
      "The following columns are present in the datasets, and this is the number of unique values they have. \n",
      "The column Participant has 188 unique values.\n",
      "The column Signature has 80 unique values.\n",
      "The column Ratification, Acceptance(A), Approval(AA), Formal confirmation(c), Accession(a), Succession(d) has 158 unique values.\n",
      "\n",
      " - - - - - \n",
      " Cleansing source S-84 \n",
      "\n",
      "\n",
      " Calling function 'rename_and_discard_columns'...\n",
      "\n",
      " Calling function 'add_and_discard_countries'...\n",
      "\n",
      " Calling function 'add_cols_fill_cells'...\n",
      "\n",
      " Calling function 'encode_ilo_un_treaty_data'...\n",
      "Cleansing done. This is some basic information about the data: \n",
      " \n",
      " There are 195 rows in the dataframe and 0.0% have a NA-value in the column 'OBS_RAW_VALUE\n",
      "\n",
      " \n",
      " This is the summary of the column 'TIME_PERIOD': count     195.0\n",
      "mean     2020.0\n",
      "std         0.0\n",
      "min      2020.0\n",
      "25%      2020.0\n",
      "50%      2020.0\n",
      "75%      2020.0\n",
      "max      2020.0\n",
      "Name: TIME_PERIOD, dtype: float64\n",
      "\n",
      " - - - - - \n",
      " Extracting source S-105 \n",
      "\n",
      "The following columns are present in the datasets, and this is the number of unique values they have. \n",
      "The column Participant 2, 3, 4 has 193 unique values.\n",
      "The column Signature has 4 unique values.\n",
      "The column Definitive signature(s), Acceptance(A) has 177 unique values.\n",
      "\n",
      " - - - - - \n",
      " Cleansing source S-105 \n",
      "\n",
      "\n",
      " Calling function 'rename_and_discard_columns'...\n",
      "\n",
      " Calling function 'add_and_discard_countries'...\n",
      "\n",
      " Calling function 'add_cols_fill_cells'...\n",
      "\n",
      " Calling function 'encode_ilo_un_treaty_data'...\n",
      "Cleansing done. This is some basic information about the data: \n",
      " \n",
      " There are 195 rows in the dataframe and 0.0% have a NA-value in the column 'OBS_RAW_VALUE\n",
      "\n",
      " \n",
      " This is the summary of the column 'TIME_PERIOD': count     195.0\n",
      "mean     2020.0\n",
      "std         0.0\n",
      "min      2020.0\n",
      "25%      2020.0\n",
      "50%      2020.0\n",
      "75%      2020.0\n",
      "max      2020.0\n",
      "Name: TIME_PERIOD, dtype: float64\n",
      "\n",
      " - - - - - \n",
      " Extracting source S-115 \n",
      "\n",
      "The following columns are present in the datasets, and this is the number of unique values they have. \n",
      "The column Participant has 185 unique values.\n",
      "The column Signature has 59 unique values.\n",
      "The column Ratification, Accession(a), Succession(d) has 173 unique values.\n",
      "\n",
      " - - - - - \n",
      " Cleansing source S-115 \n",
      "\n",
      "\n",
      " Calling function 'rename_and_discard_columns'...\n",
      "\n",
      " Calling function 'add_and_discard_countries'...\n",
      "\n",
      " Calling function 'add_cols_fill_cells'...\n",
      "\n",
      " Calling function 'encode_ilo_un_treaty_data'...\n",
      "Cleansing done. This is some basic information about the data: \n",
      " \n",
      " There are 195 rows in the dataframe and 0.0% have a NA-value in the column 'OBS_RAW_VALUE\n",
      "\n",
      " \n",
      " This is the summary of the column 'TIME_PERIOD': count     195.0\n",
      "mean     2020.0\n",
      "std         0.0\n",
      "min      2020.0\n",
      "25%      2020.0\n",
      "50%      2020.0\n",
      "75%      2020.0\n",
      "max      2020.0\n",
      "Name: TIME_PERIOD, dtype: float64\n",
      "\n",
      " - - - - - \n",
      " Extracting source S-141 \n",
      "\n",
      "The following columns are present in the datasets, and this is the number of unique values they have. \n",
      "The column Participant has 197 unique values.\n",
      "The column Signature has 21 unique values.\n",
      "The column Approval(AA), Acceptance(A), Accession(a), Succession(d), Ratification has 180 unique values.\n",
      "\n",
      " - - - - - \n",
      " Cleansing source S-141 \n",
      "\n",
      "\n",
      " Calling function 'rename_and_discard_columns'...\n",
      "\n",
      " Calling function 'add_and_discard_countries'...\n",
      "\n",
      " Calling function 'add_cols_fill_cells'...\n",
      "\n",
      " Calling function 'encode_ilo_un_treaty_data'...\n",
      "Cleansing done. This is some basic information about the data: \n",
      " \n",
      " There are 195 rows in the dataframe and 0.0% have a NA-value in the column 'OBS_RAW_VALUE\n",
      "\n",
      " \n",
      " This is the summary of the column 'TIME_PERIOD': count     195.0\n",
      "mean     2020.0\n",
      "std         0.0\n",
      "min      2020.0\n",
      "25%      2020.0\n",
      "50%      2020.0\n",
      "75%      2020.0\n",
      "max      2020.0\n",
      "Name: TIME_PERIOD, dtype: float64\n",
      "\n",
      " - - - - - \n",
      " Extracting source S-142 \n",
      "\n",
      "The following columns are present in the datasets, and this is the number of unique values they have. \n",
      "The column Participant has 197 unique values.\n",
      "The column Signature has 17 unique values.\n",
      "The column Ratification, Acceptance(A), Approval(AA), Accession(a) has 115 unique values.\n",
      "\n",
      " - - - - - \n",
      " Cleansing source S-142 \n",
      "\n",
      "\n",
      " Calling function 'rename_and_discard_columns'...\n",
      "\n",
      " Calling function 'add_and_discard_countries'...\n",
      "\n",
      " Calling function 'add_cols_fill_cells'...\n",
      "\n",
      " Calling function 'encode_ilo_un_treaty_data'...\n",
      "Cleansing done. This is some basic information about the data: \n",
      " \n",
      " There are 195 rows in the dataframe and 0.0% have a NA-value in the column 'OBS_RAW_VALUE\n",
      "\n",
      " \n",
      " This is the summary of the column 'TIME_PERIOD': count     195.0\n",
      "mean     2020.0\n",
      "std         0.0\n",
      "min      2020.0\n",
      "25%      2020.0\n",
      "50%      2020.0\n",
      "75%      2020.0\n",
      "max      2020.0\n",
      "Name: TIME_PERIOD, dtype: float64\n",
      "\n",
      " - - - - - \n",
      " Extracting source S-143 \n",
      "\n",
      "The following columns are present in the datasets, and this is the number of unique values they have. \n",
      "The column Participant has 190 unique values.\n",
      "The column Signature has 13 unique values.\n",
      "The column Approval(AA), Formal confirmation(c), Acceptance(A), Accession(a), Succession(d), Ratification has 184 unique values.\n",
      "\n",
      " - - - - - \n",
      " Cleansing source S-143 \n",
      "\n",
      "\n",
      " Calling function 'rename_and_discard_columns'...\n",
      "\n",
      " Calling function 'add_and_discard_countries'...\n",
      "\n",
      " Calling function 'add_cols_fill_cells'...\n",
      "\n",
      " Calling function 'encode_ilo_un_treaty_data'...\n",
      "Cleansing done. This is some basic information about the data: \n",
      " \n",
      " There are 195 rows in the dataframe and 0.0% have a NA-value in the column 'OBS_RAW_VALUE\n",
      "\n",
      " \n",
      " This is the summary of the column 'TIME_PERIOD': count     195.0\n",
      "mean     2020.0\n",
      "std         0.0\n",
      "min      2020.0\n",
      "25%      2020.0\n",
      "50%      2020.0\n",
      "75%      2020.0\n",
      "max      2020.0\n",
      "Name: TIME_PERIOD, dtype: float64\n",
      "\n",
      " - - - - - \n",
      " Extracting source S-144 \n",
      "\n",
      "The following columns are present in the datasets, and this is the number of unique values they have. \n",
      "The column Participant has 190 unique values.\n",
      "The column Signature, Succession to signature(d) has 42 unique values.\n",
      "The column Ratification, Acceptance(A), Approval(AA), Accession(a) has 180 unique values.\n",
      "\n",
      " - - - - - \n",
      " Cleansing source S-144 \n",
      "\n",
      "\n",
      " Calling function 'rename_and_discard_columns'...\n",
      "\n",
      " Calling function 'add_and_discard_countries'...\n",
      "\n",
      " Calling function 'add_cols_fill_cells'...\n",
      "\n",
      " Calling function 'encode_ilo_un_treaty_data'...\n",
      "Cleansing done. This is some basic information about the data: \n",
      " \n",
      " There are 195 rows in the dataframe and 0.0% have a NA-value in the column 'OBS_RAW_VALUE\n",
      "\n",
      " \n",
      " This is the summary of the column 'TIME_PERIOD': count     195.0\n",
      "mean     2020.0\n",
      "std         0.0\n",
      "min      2020.0\n",
      "25%      2020.0\n",
      "50%      2020.0\n",
      "75%      2020.0\n",
      "max      2020.0\n",
      "Name: TIME_PERIOD, dtype: float64\n",
      "\n",
      " - - - - - \n",
      " Extracting source S-145 \n",
      "\n",
      "The following columns are present in the datasets, and this is the number of unique values they have. \n",
      "The column Participant has 45 unique values.\n",
      "The column Signature has 5 unique values.\n",
      "The column Ratification, Accession(a), Acceptance(A), Approval(AA) has 45 unique values.\n",
      "\n",
      " - - - - - \n",
      " Cleansing source S-145 \n",
      "\n",
      "\n",
      " Calling function 'rename_and_discard_columns'...\n",
      "\n",
      " Calling function 'add_and_discard_countries'...\n",
      "\n",
      " Calling function 'add_cols_fill_cells'...\n",
      "\n",
      " Calling function 'encode_ilo_un_treaty_data'...\n",
      "Cleansing done. This is some basic information about the data: \n",
      " \n",
      " There are 195 rows in the dataframe and 0.0% have a NA-value in the column 'OBS_RAW_VALUE\n",
      "\n",
      " \n",
      " This is the summary of the column 'TIME_PERIOD': count     195.0\n",
      "mean     2020.0\n",
      "std         0.0\n",
      "min      2020.0\n",
      "25%      2020.0\n",
      "50%      2020.0\n",
      "75%      2020.0\n",
      "max      2020.0\n",
      "Name: TIME_PERIOD, dtype: float64\n",
      "\n",
      " - - - - - \n",
      " Extracting source S-162 \n",
      "\n",
      "The following columns are present in the datasets, and this is the number of unique values they have. \n",
      "The column Participant 2 has 175 unique values.\n",
      "The column Signature has 64 unique values.\n",
      "The column Ratification, Accession(a), Succession(d) has 170 unique values.\n",
      "\n",
      " - - - - - \n",
      " Cleansing source S-162 \n",
      "\n",
      "\n",
      " Calling function 'rename_and_discard_columns'...\n",
      "\n",
      " Calling function 'add_and_discard_countries'...\n",
      "\n",
      " Calling function 'add_cols_fill_cells'...\n",
      "\n",
      " Calling function 'encode_ilo_un_treaty_data'...\n",
      "Cleansing done. This is some basic information about the data: \n",
      " \n",
      " There are 195 rows in the dataframe and 0.0% have a NA-value in the column 'OBS_RAW_VALUE\n",
      "\n",
      " \n",
      " This is the summary of the column 'TIME_PERIOD': count     195.0\n",
      "mean     2020.0\n",
      "std         0.0\n",
      "min      2020.0\n",
      "25%      2020.0\n",
      "50%      2020.0\n",
      "75%      2020.0\n",
      "max      2020.0\n",
      "Name: TIME_PERIOD, dtype: float64\n",
      "\n",
      " - - - - - \n",
      " Extracting source S-171 \n",
      "\n",
      "The following columns are present in the datasets, and this is the number of unique values they have. \n",
      "The column Participant has 45 unique values.\n",
      "The column Signature, Succession to signature(d) has 17 unique values.\n",
      "The column Ratification, Accession(a), Succession(d) has 37 unique values.\n",
      "\n",
      " - - - - - \n",
      " Cleansing source S-171 \n",
      "\n",
      "\n",
      " Calling function 'rename_and_discard_columns'...\n",
      "\n",
      " Calling function 'add_and_discard_countries'...\n",
      "\n",
      " Calling function 'add_cols_fill_cells'...\n",
      "\n",
      " Calling function 'encode_ilo_un_treaty_data'...\n",
      "Cleansing done. This is some basic information about the data: \n",
      " \n",
      " There are 195 rows in the dataframe and 0.0% have a NA-value in the column 'OBS_RAW_VALUE\n",
      "\n",
      " \n",
      " This is the summary of the column 'TIME_PERIOD': count     195.0\n",
      "mean     2020.0\n",
      "std         0.0\n",
      "min      2020.0\n",
      "25%      2020.0\n",
      "50%      2020.0\n",
      "75%      2020.0\n",
      "max      2020.0\n",
      "Name: TIME_PERIOD, dtype: float64\n",
      "\n",
      " - - - - - \n",
      " Extracting source S-173 \n",
      "\n",
      "The following columns are present in the datasets, and this is the number of unique values they have. \n",
      "The column Participant has 180 unique values.\n",
      "The column Signature has 62 unique values.\n",
      "The column Ratification, Accession(a), Succession(d) has 162 unique values.\n",
      "\n",
      " - - - - - \n",
      " Cleansing source S-173 \n",
      "\n",
      "\n",
      " Calling function 'rename_and_discard_columns'...\n",
      "\n",
      " Calling function 'add_and_discard_countries'...\n",
      "\n",
      " Calling function 'add_cols_fill_cells'...\n",
      "\n",
      " Calling function 'encode_ilo_un_treaty_data'...\n",
      "Cleansing done. This is some basic information about the data: \n",
      " \n",
      " There are 195 rows in the dataframe and 0.0% have a NA-value in the column 'OBS_RAW_VALUE\n",
      "\n",
      " \n",
      " This is the summary of the column 'TIME_PERIOD': count     195.0\n",
      "mean     2020.0\n",
      "std         0.0\n",
      "min      2020.0\n",
      "25%      2020.0\n",
      "50%      2020.0\n",
      "75%      2020.0\n",
      "max      2020.0\n",
      "Name: TIME_PERIOD, dtype: float64\n",
      "\n",
      " - - - - - \n",
      " Extracting source S-182 \n",
      "\n",
      "The following columns are present in the datasets, and this is the number of unique values they have. \n",
      "The column Participant has 80 unique values.\n",
      "The column Signature has 36 unique values.\n",
      "The column Definitive signature(s), Ratification, Acceptance(A), Approval(AA), Accession(a) has 50 unique values.\n",
      "\n",
      " - - - - - \n",
      " Cleansing source S-182 \n",
      "\n",
      "\n",
      " Calling function 'rename_and_discard_columns'...\n",
      "\n",
      " Calling function 'add_and_discard_countries'...\n",
      "\n",
      " Calling function 'add_cols_fill_cells'...\n",
      "\n",
      " Calling function 'encode_ilo_un_treaty_data'...\n",
      "Cleansing done. This is some basic information about the data: \n",
      " \n",
      " There are 195 rows in the dataframe and 0.0% have a NA-value in the column 'OBS_RAW_VALUE\n",
      "\n",
      " \n",
      " This is the summary of the column 'TIME_PERIOD': count     195.0\n",
      "mean     2020.0\n",
      "std         0.0\n",
      "min      2020.0\n",
      "25%      2020.0\n",
      "50%      2020.0\n",
      "75%      2020.0\n",
      "max      2020.0\n",
      "Name: TIME_PERIOD, dtype: float64\n",
      "\n",
      " - - - - - \n",
      " Extracting source S-191 \n",
      "\n",
      "The following columns are present in the datasets, and this is the number of unique values they have. \n",
      "The column Participant has 197 unique values.\n",
      "The column Signature has 59 unique values.\n",
      "The column Ratification, Acceptance(A), Accession(a), Succession(d) has 185 unique values.\n",
      "\n",
      " - - - - - \n",
      " Cleansing source S-191 \n",
      "\n",
      "\n",
      " Calling function 'rename_and_discard_columns'...\n",
      "\n",
      " Calling function 'add_and_discard_countries'...\n",
      "\n",
      " Calling function 'add_cols_fill_cells'...\n",
      "\n",
      " Calling function 'encode_ilo_un_treaty_data'...\n",
      "Cleansing done. This is some basic information about the data: \n",
      " \n",
      " There are 195 rows in the dataframe and 0.0% have a NA-value in the column 'OBS_RAW_VALUE\n",
      "\n",
      " \n",
      " This is the summary of the column 'TIME_PERIOD': count     195.0\n",
      "mean     2020.0\n",
      "std         0.0\n",
      "min      2020.0\n",
      "25%      2020.0\n",
      "50%      2020.0\n",
      "75%      2020.0\n",
      "max      2020.0\n",
      "Name: TIME_PERIOD, dtype: float64\n",
      "\n",
      " - - - - - \n",
      " Extracting source S-192 \n",
      "\n",
      "The following columns are present in the datasets, and this is the number of unique values they have. \n",
      "The column Participant has 64 unique values.\n",
      "The column Signature has 24 unique values.\n",
      "The column Accession(a), Ratification has 46 unique values.\n",
      "\n",
      " - - - - - \n",
      " Cleansing source S-192 \n",
      "\n",
      "\n",
      " Calling function 'rename_and_discard_columns'...\n",
      "\n",
      " Calling function 'add_and_discard_countries'...\n",
      "\n",
      " Calling function 'add_cols_fill_cells'...\n",
      "\n",
      " Calling function 'encode_ilo_un_treaty_data'...\n",
      "Cleansing done. This is some basic information about the data: \n",
      " \n",
      " There are 195 rows in the dataframe and 0.0% have a NA-value in the column 'OBS_RAW_VALUE\n",
      "\n",
      " \n",
      " This is the summary of the column 'TIME_PERIOD': count     195.0\n",
      "mean     2020.0\n",
      "std         0.0\n",
      "min      2020.0\n",
      "25%      2020.0\n",
      "50%      2020.0\n",
      "75%      2020.0\n",
      "max      2020.0\n",
      "Name: TIME_PERIOD, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# UN Treaty HTML sources\n",
    "api_sources = crba_data_dictionary_source[\n",
    "    (crba_data_dictionary_source[\"SOURCE_BODY\"] == \"UN Treaties\")\n",
    "].merge(\n",
    "    right = crba_data_dictionary_snapshot,\n",
    "    on = \"SOURCE_ID\"\n",
    ").merge(\n",
    "    right = crba_data_dictionary_indicator,\n",
    "    on = 'INDICATOR_ID'\n",
    ")\n",
    "\n",
    "# Loop to extract data from API sources\n",
    "for index, row in api_sources.iterrows():\n",
    "    # Log\n",
    "    print(\"\\n - - - - - \\n Extracting source {} \\n\".format(row[\"SOURCE_ID\"]))\n",
    "    \n",
    "    # Exraction section\n",
    "    dataframe = extract.HTMLExtractor().extract(url = row[\"ADDRESS\"])\n",
    "    \n",
    "    # Save dataframe\n",
    "    dataframe.to_csv(\n",
    "        data_sources_raw / str(row[\"SOURCE_ID\"] + \"_raw.csv\"),\n",
    "        sep = \";\")\n",
    "\n",
    "    # Cleansing\n",
    "    # Log that we are entering cleasning\n",
    "    print(\"\\n - - - - - \\n Cleansing source {} \\n\".format(row[\"SOURCE_ID\"]))\n",
    "\n",
    "    # Cleansing\n",
    "    dataframe = cleanse.Cleanser().rename_and_discard_columns(\n",
    "        raw_data=dataframe,\n",
    "        mapping_dictionary=mapping_dict,\n",
    "        final_sdmx_col_list=sdmx_df_columns_all\n",
    "    )\n",
    "\n",
    "    dataframe = cleanse.Cleanser().add_and_discard_countries(\n",
    "        grouped_data=dataframe,\n",
    "        crba_country_list=country_crba_list,\n",
    "        country_list_full = country_full_list\n",
    "    )\n",
    "\n",
    "    dataframe = cleanse.Cleanser().add_cols_fill_cells(\n",
    "        grouped_data_iso_filt=dataframe,\n",
    "        dim_cols=sdmx_df_columns_dims,\n",
    "        time_cols=sdmx_df_columns_time,\n",
    "        indicator_name_string=row[\"INDICATOR_NAME_x\"],\n",
    "        index_name_string=row[\"INDEX\"],\n",
    "        issue_name_string=row[\"ISSUE\"],\n",
    "        category_name_string=row[\"CATEGORY\"],\n",
    "        indicator_code_string=row[\"INDICATOR_CODE\"],\n",
    "        indicator_source_string=row[\"ADDRESS\"],\n",
    "        indicator_source_body_string=row[\"SOURCE_BODY\"],\n",
    "        indicator_description_string=row[\"INDICATOR_DESCRIPTION\"],\n",
    "        source_title_string=row[\"SOURCE_TITLE\"],\n",
    "        indicator_explanation_string=row[\"INDICATOR_EXPLANATION\"],\n",
    "        indicator_data_extraction_methodology_string=row[\"EXTRACTION_METHODOLOGY\"],\n",
    "        source_api_link_string=row[\"ENDPOINT_URL\"]\n",
    "    )\n",
    "\n",
    "    dataframe_cleansed = cleanse.Cleanser().encode_ilo_un_treaty_data(\n",
    "        dataframe = dataframe,\n",
    "        treaty_source_body = row[\"SOURCE_BODY\"]\n",
    "    )\n",
    "\n",
    "    cleanse.Cleanser().create_log_report(\n",
    "        cleansed_data=dataframe_cleansed\n",
    "    )\n",
    "\n",
    "    # Append dataframe to combined dataframe\n",
    "    combined_cleansed_csv = combined_cleansed_csv.append(\n",
    "        other = dataframe_cleansed\n",
    "    )\n",
    "\n",
    "    # Save cleansed data\n",
    "    dataframe_cleansed.to_csv(\n",
    "        data_sources_cleansed / str(row[\"SOURCE_ID\"] + \"_cleansed.csv\"),\n",
    "        sep = \";\")\n",
    "    \n",
    "    # Normalizing section\n",
    "    dataframe_normalized = scaler.normalizer(\n",
    "        cleansed_data = dataframe_cleansed,\n",
    "        sql_subset_query_string=row[\"DIMENSION_VALUES_NORMALIZATION\"],\n",
    "        # dim_cols=sdmx_df_columns_dims,\n",
    "        variable_type = row[\"VALUE_LABELS\"],\n",
    "        is_inverted = row[\"INVERT_NORMALIZATION\"],\n",
    "        whisker_factor=1.5,\n",
    "        raw_data_col=\"RAW_OBS_VALUE\",\n",
    "        scaled_data_col_name=\"SCALED_OBS_VALUE\",\n",
    "        maximum_score=10,\n",
    "        )\n",
    "    \n",
    "    dataframe_normalized.to_csv(\n",
    "        data_sources_normalized / str(row[\"SOURCE_ID\"] + \"_normalized.csv\"),\n",
    "        sep = \";\")\n",
    "\n",
    "    # Append dataframe to combined dataframe\n",
    "    combined_normalized_csv = combined_normalized_csv.append(\n",
    "        other = dataframe_normalized\n",
    "    )"
   ]
  },
  {
   "source": [
    "## Other sources\n",
    "### WPA sources"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ction 'encode_categorical_variables'...\n",
      "Cleansing done. This is some basic information about the data: \n",
      " \n",
      " There are 195 rows in the dataframe and 0.0% have a NA-value in the column 'OBS_RAW_VALUE\n",
      "\n",
      " \n",
      " This is the summary of the column 'TIME_PERIOD': count     195.0\n",
      "mean     2020.0\n",
      "std         0.0\n",
      "min      2020.0\n",
      "25%      2020.0\n",
      "50%      2020.0\n",
      "75%      2020.0\n",
      "max      2020.0\n",
      "Name: TIME_PERIOD, dtype: float64\n",
      "\n",
      " - - - - - \n",
      " Extracting source S-43 \n",
      "\n",
      "\n",
      " Extracting data and store it as raw data\n",
      "   iso3  pay_sex  TIME_PERIOD\n",
      "0   AFG        2         2020\n",
      "1   ALB        5         2020\n",
      "2   DZA        5         2020\n",
      "3   AND        4         2020\n",
      "4   AGO        5         2020\n",
      "5   ATG        4         2020\n",
      "6   ARG        5         2020\n",
      "7   ARM        5         2020\n",
      "8   AUS        3         2020\n",
      "9   AUT        5         2020\n",
      "10  AZE        5         2020\n",
      "11  BHS        5         2020\n",
      "12  BHR        4         2020\n",
      "13  BGD        5         2020\n",
      "14  BRB        1         2020\n",
      "15  BLR        3         2020\n",
      "16  BEL        5         2020\n",
      "17  BLZ        1         2020\n",
      "18  BEN        5         2020\n",
      "19  BTN        5         2020\n",
      "20  BOL        4         2020\n",
      "21  BIH        5         2020\n",
      "22  BWA        1         2020\n",
      "23  BRA        5         2020\n",
      "24  BRN        1         2020\n",
      "25  BGR        5         2020\n",
      "26  BFA        5         2020\n",
      "27  BDI        4         2020\n",
      "28  KHM        5         2020\n",
      "29  CMR        4         2020\n",
      "There was an issue with source S-43\n",
      "\n",
      " - - - - - \n",
      " Cleansing source S-43 \n",
      "\n",
      "\n",
      " - - - - - \n",
      " Cleansing source S-43 \n",
      "\n",
      "\n",
      " Calling function 'rename_and_discard_columns'...\n",
      "[2 5 4 3 1]\n",
      "\n",
      " Calling function 'add_and_discard_countries'...\n",
      "\n",
      " Calling function 'add_cols_fill_cells'...\n",
      "\n",
      " Calling function 'encode_categorical_variables'...\n",
      "Cleansing done. This is some basic information about the data: \n",
      " \n",
      " There are 195 rows in the dataframe and 0.0% have a NA-value in the column 'OBS_RAW_VALUE\n",
      "\n",
      " \n",
      " This is the summary of the column 'TIME_PERIOD': count     195.0\n",
      "mean     2020.0\n",
      "std         0.0\n",
      "min      2020.0\n",
      "25%      2020.0\n",
      "50%      2020.0\n",
      "75%      2020.0\n",
      "max      2020.0\n",
      "Name: TIME_PERIOD, dtype: float64\n",
      "\n",
      " - - - - - \n",
      " Extracting source S-44 \n",
      "\n",
      "\n",
      " Extracting data and store it as raw data\n",
      "   iso3  sh_covered  TIME_PERIOD\n",
      "0   AFG         NaN         2020\n",
      "1   ALB         5.0         2020\n",
      "2   DZA         5.0         2020\n",
      "3   AND         5.0         2020\n",
      "4   AGO         5.0         2020\n",
      "5   ATG         1.0         2020\n",
      "6   ARG         5.0         2020\n",
      "7   ARM         5.0         2020\n",
      "8   AUS         5.0         2020\n",
      "9   AUT         5.0         2020\n",
      "10  AZE         5.0         2020\n",
      "11  BHS         5.0         2020\n",
      "12  BHR         1.0         2020\n",
      "13  BGD         3.0         2020\n",
      "14  BRB         1.0         2020\n",
      "15  BLR         1.0         2020\n",
      "16  BEL         5.0         2020\n",
      "17  BLZ         5.0         2020\n",
      "18  BEN         5.0         2020\n",
      "19  BTN         5.0         2020\n",
      "20  BOL         5.0         2020\n",
      "21  BIH         5.0         2020\n",
      "22  BWA         1.0         2020\n",
      "23  BRA         5.0         2020\n",
      "24  BRN         1.0         2020\n",
      "25  BGR         5.0         2020\n",
      "26  BFA         5.0         2020\n",
      "27  BDI         5.0         2020\n",
      "28  KHM         5.0         2020\n",
      "29  CMR         5.0         2020\n",
      "There was an issue with source S-44\n",
      "\n",
      " - - - - - \n",
      " Cleansing source S-44 \n",
      "\n",
      "\n",
      " - - - - - \n",
      " Cleansing source S-44 \n",
      "\n",
      "\n",
      " Calling function 'rename_and_discard_columns'...\n",
      "[nan  5.  1.  3.]\n",
      "\n",
      " Calling function 'add_and_discard_countries'...\n",
      "\n",
      " Calling function 'add_cols_fill_cells'...\n",
      "\n",
      " Calling function 'encode_categorical_variables'...\n",
      "Cleansing done. This is some basic information about the data: \n",
      " \n",
      " There are 195 rows in the dataframe and 0.0% have a NA-value in the column 'OBS_RAW_VALUE\n",
      "\n",
      " \n",
      " This is the summary of the column 'TIME_PERIOD': count     195.0\n",
      "mean     2020.0\n",
      "std         0.0\n",
      "min      2020.0\n",
      "25%      2020.0\n",
      "50%      2020.0\n",
      "75%      2020.0\n",
      "max      2020.0\n",
      "Name: TIME_PERIOD, dtype: float64\n",
      "\n",
      " - - - - - \n",
      " Extracting source S-45 \n",
      "\n",
      "\n",
      " Extracting data and store it as raw data\n",
      "   iso3  fb_ccschsupp  TIME_PERIOD\n",
      "0   AFG           1.0         2014\n",
      "1   ALB           NaN         2014\n",
      "2   DZA           4.0         2014\n",
      "3   AND           1.0         2014\n",
      "4   AGO           1.0         2014\n",
      "5   ATG           1.0         2014\n",
      "6   ARG           3.0         2014\n",
      "7   ARM           1.0         2014\n",
      "8   AUS           4.0         2014\n",
      "9   AUT           3.0         2014\n",
      "10  AZE           4.0         2014\n",
      "11  BHS           1.0         2014\n",
      "12  BHR           1.0         2014\n",
      "13  BGD           1.0         2014\n",
      "14  BRB           1.0         2014\n",
      "15  BLR           1.0         2014\n",
      "16  BEL           1.0         2014\n",
      "17  BLZ           1.0         2014\n",
      "18  BEN           NaN         2014\n",
      "19  BTN           1.0         2014\n",
      "20  BOL           1.0         2014\n",
      "21  BIH           1.0         2014\n",
      "22  BWA           1.0         2014\n",
      "23  BRA           1.0         2014\n",
      "24  BRN           1.0         2014\n",
      "25  BGR           4.0         2014\n",
      "26  BFA           1.0         2014\n",
      "27  BDI           1.0         2014\n",
      "28  KHM           1.0         2014\n",
      "29  CMR           1.0         2014\n",
      "There was an issue with source S-45\n",
      "\n",
      " - - - - - \n",
      " Cleansing source S-45 \n",
      "\n",
      "\n",
      " - - - - - \n",
      " Cleansing source S-45 \n",
      "\n",
      "\n",
      " Calling function 'rename_and_discard_columns'...\n",
      "[ 1. nan  4.  3.  5.]\n",
      "\n",
      " Calling function 'add_and_discard_countries'...\n",
      "\n",
      " Calling function 'add_cols_fill_cells'...\n",
      "\n",
      " Calling function 'encode_categorical_variables'...\n",
      "Cleansing done. This is some basic information about the data: \n",
      " \n",
      " There are 195 rows in the dataframe and 0.0% have a NA-value in the column 'OBS_RAW_VALUE\n",
      "\n",
      " \n",
      " This is the summary of the column 'TIME_PERIOD': count     195.000000\n",
      "mean     2014.061538\n",
      "std         0.606076\n",
      "min      2014.000000\n",
      "25%      2014.000000\n",
      "50%      2014.000000\n",
      "75%      2014.000000\n",
      "max      2020.000000\n",
      "Name: TIME_PERIOD, dtype: float64\n",
      "\n",
      " - - - - - \n",
      " Extracting source S-49 \n",
      "\n",
      "\n",
      " Extracting data and store it as raw data\n",
      "   iso3  minwage_ppp  TIME_PERIOD\n",
      "0   AFG          4.0         2014\n",
      "1   ALB          5.0         2014\n",
      "2   DZA          5.0         2014\n",
      "3   AND          NaN         2014\n",
      "4   AGO          3.0         2014\n",
      "5   ATG          5.0         2014\n",
      "6   ARG          5.0         2014\n",
      "7   ARM          4.0         2014\n",
      "8   AUS          5.0         2014\n",
      "9   AUT          5.0         2014\n",
      "10  AZE          4.0         2014\n",
      "11  BHS          5.0         2014\n",
      "12  BHR          1.0         2014\n",
      "13  BGD          2.0         2014\n",
      "14  BRB          5.0         2014\n",
      "15  BLR          5.0         2014\n",
      "16  BEL          5.0         2014\n",
      "17  BLZ          5.0         2014\n",
      "18  BEN          3.0         2014\n",
      "19  BTN          4.0         2014\n",
      "20  BOL          4.0         2014\n",
      "21  BIH          5.0         2014\n",
      "22  BWA          4.0         2014\n",
      "23  BRA          5.0         2014\n",
      "24  BRN          1.0         2014\n",
      "25  BGR          5.0         2014\n",
      "26  BFA          3.0         2014\n",
      "27  BDI          2.0         2014\n",
      "28  KHM          2.0         2014\n",
      "29  CMR          3.0         2014\n",
      "There was an issue with source S-49\n",
      "\n",
      " - - - - - \n",
      " Cleansing source S-49 \n",
      "\n",
      "\n",
      " - - - - - \n",
      " Cleansing source S-49 \n",
      "\n",
      "\n",
      " Calling function 'rename_and_discard_columns'...\n",
      "[  4.   5.  nan   3.   1.   2. 999.]\n",
      "\n",
      " Calling function 'add_and_discard_countries'...\n",
      "\n",
      " Calling function 'add_cols_fill_cells'...\n",
      "\n",
      " Calling function 'encode_categorical_variables'...\n",
      "Cleansing done. This is some basic information about the data: \n",
      " \n",
      " There are 195 rows in the dataframe and 0.0% have a NA-value in the column 'OBS_RAW_VALUE\n",
      "\n",
      " \n",
      " This is the summary of the column 'TIME_PERIOD': count     195.000000\n",
      "mean     2014.061538\n",
      "std         0.606076\n",
      "min      2014.000000\n",
      "25%      2014.000000\n",
      "50%      2014.000000\n",
      "75%      2014.000000\n",
      "max      2020.000000\n",
      "Name: TIME_PERIOD, dtype: float64\n",
      "\n",
      " - - - - - \n",
      " Extracting source S-63 \n",
      "\n",
      "\n",
      " Extracting data and store it as raw data\n",
      "   iso3  mtlv_job_protect  TIME_PERIOD\n",
      "0   AFG                 2         2019\n",
      "1   ALB                 5         2019\n",
      "2   DZA                 2         2019\n",
      "3   AND                 5         2019\n",
      "4   AGO                 5         2019\n",
      "5   ATG                 2         2019\n",
      "6   ARG                 5         2019\n",
      "7   ARM                 3         2019\n",
      "8   AUS                 5         2019\n",
      "9   AUT                 5         2019\n",
      "10  AZE                 2         2019\n",
      "11  BHS                 5         2019\n",
      "12  BHR                 5         2019\n",
      "13  BGD                 2         2019\n",
      "14  BRB                 5         2019\n",
      "15  BLR                 2         2019\n",
      "16  BEL                 5         2019\n",
      "17  BLZ                 5         2019\n",
      "18  BEN                 5         2019\n",
      "19  BTN                 2         2019\n",
      "20  BOL                 2         2019\n",
      "21  BIH                 5         2019\n",
      "22  BWA                 5         2019\n",
      "23  BRA                 5         2019\n",
      "24  BRN                 5         2019\n",
      "25  BGR                 5         2019\n",
      "26  BFA                 5         2019\n",
      "27  BDI                 5         2019\n",
      "28  KHM                 5         2019\n",
      "29  CMR                 5         2019\n",
      "There was an issue with source S-63\n",
      "\n",
      " - - - - - \n",
      " Cleansing source S-63 \n",
      "\n",
      "\n",
      " - - - - - \n",
      " Cleansing source S-63 \n",
      "\n",
      "\n",
      " Calling function 'rename_and_discard_columns'...\n",
      "[2 5 3 1]\n",
      "\n",
      " Calling function 'add_and_discard_countries'...\n",
      "\n",
      " Calling function 'add_cols_fill_cells'...\n",
      "\n",
      " Calling function 'encode_categorical_variables'...\n",
      "Cleansing done. This is some basic information about the data: \n",
      " \n",
      " There are 195 rows in the dataframe and 0.0% have a NA-value in the column 'OBS_RAW_VALUE\n",
      "\n",
      " \n",
      " This is the summary of the column 'TIME_PERIOD': count     195.000000\n",
      "mean     2019.010256\n",
      "std         0.101013\n",
      "min      2019.000000\n",
      "25%      2019.000000\n",
      "50%      2019.000000\n",
      "75%      2019.000000\n",
      "max      2020.000000\n",
      "Name: TIME_PERIOD, dtype: float64\n",
      "\n",
      " - - - - - \n",
      " Extracting source S-64 \n",
      "\n",
      "\n",
      " Extracting data and store it as raw data\n",
      "   iso3  ptlv_job_protect  TIME_PERIOD\n",
      "0   AFG               2.0         2019\n",
      "1   ALB               1.0         2019\n",
      "2   DZA               2.0         2019\n",
      "3   AND               5.0         2019\n",
      "4   AGO               5.0         2019\n",
      "5   ATG               1.0         2019\n",
      "6   ARG               2.0         2019\n",
      "7   ARM               2.0         2019\n",
      "8   AUS               3.0         2019\n",
      "9   AUT               5.0         2019\n",
      "10  AZE               2.0         2019\n",
      "11  BHS               1.0         2019\n",
      "12  BHR               2.0         2019\n",
      "13  BGD               1.0         2019\n",
      "14  BRB               1.0         2019\n",
      "15  BLR               2.0         2019\n",
      "16  BEL               5.0         2019\n",
      "17  BLZ               1.0         2019\n",
      "18  BEN               2.0         2019\n",
      "19  BTN               2.0         2019\n",
      "20  BOL               2.0         2019\n",
      "21  BIH               2.0         2019\n",
      "22  BWA               1.0         2019\n",
      "23  BRA               5.0         2019\n",
      "24  BRN               1.0         2019\n",
      "25  BGR               5.0         2019\n",
      "26  BFA               1.0         2019\n",
      "27  BDI               2.0         2019\n",
      "28  KHM               1.0         2019\n",
      "29  CMR               1.0         2019\n",
      "There was an issue with source S-64\n",
      "\n",
      " - - - - - \n",
      " Cleansing source S-64 \n",
      "\n",
      "\n",
      " - - - - - \n",
      " Cleansing source S-64 \n",
      "\n",
      "\n",
      " Calling function 'rename_and_discard_columns'...\n",
      "[ 2.  1.  5.  3. nan]\n",
      "\n",
      " Calling function 'add_and_discard_countries'...\n",
      "\n",
      " Calling function 'add_cols_fill_cells'...\n",
      "\n",
      " Calling function 'encode_categorical_variables'...\n",
      "Cleansing done. This is some basic information about the data: \n",
      " \n",
      " There are 195 rows in the dataframe and 0.0% have a NA-value in the column 'OBS_RAW_VALUE\n",
      "\n",
      " \n",
      " This is the summary of the column 'TIME_PERIOD': count     195.000000\n",
      "mean     2019.010256\n",
      "std         0.101013\n",
      "min      2019.000000\n",
      "25%      2019.000000\n",
      "50%      2019.000000\n",
      "75%      2019.000000\n",
      "max      2020.000000\n",
      "Name: TIME_PERIOD, dtype: float64\n",
      "\n",
      " - - - - - \n",
      " Extracting source S-65 \n",
      "\n",
      "\n",
      " Extracting data and store it as raw data\n",
      "   iso3  maternal_leave  TIME_PERIOD\n",
      "0   AFG               2         2019\n",
      "1   ALB               5         2019\n",
      "2   DZA               3         2019\n",
      "3   AND               3         2019\n",
      "4   AGO               2         2019\n",
      "5   ATG               2         2019\n",
      "6   ARG               2         2019\n",
      "7   ARM               5         2019\n",
      "8   AUS               3         2019\n",
      "9   AUT               5         2019\n",
      "10  AZE               5         2019\n",
      "11  BHS               2         2019\n",
      "12  BHR               2         2019\n",
      "13  BGD               3         2019\n",
      "14  BRB               2         2019\n",
      "15  BLR               5         2019\n",
      "16  BEL               4         2019\n",
      "17  BLZ               3         2019\n",
      "18  BEN               3         2019\n",
      "19  BTN               2         2019\n",
      "20  BOL               2         2019\n",
      "21  BIH               5         2019\n",
      "22  BWA               2         2019\n",
      "23  BRA               3         2019\n",
      "24  BRN               2         2019\n",
      "25  BGR               5         2019\n",
      "26  BFA               3         2019\n",
      "27  BDI               2         2019\n",
      "28  KHM               2         2019\n",
      "29  CMR               3         2019\n",
      "There was an issue with source S-65\n",
      "\n",
      " - - - - - \n",
      " Cleansing source S-65 \n",
      "\n",
      "\n",
      " - - - - - \n",
      " Cleansing source S-65 \n",
      "\n",
      "\n",
      " Calling function 'rename_and_discard_columns'...\n",
      "[2 5 3 4 1]\n",
      "\n",
      " Calling function 'add_and_discard_countries'...\n",
      "\n",
      " Calling function 'add_cols_fill_cells'...\n",
      "\n",
      " Calling function 'encode_categorical_variables'...\n",
      "Cleansing done. This is some basic information about the data: \n",
      " \n",
      " There are 195 rows in the dataframe and 0.0% have a NA-value in the column 'OBS_RAW_VALUE\n",
      "\n",
      " \n",
      " This is the summary of the column 'TIME_PERIOD': count     195.000000\n",
      "mean     2019.010256\n",
      "std         0.101013\n",
      "min      2019.000000\n",
      "25%      2019.000000\n",
      "50%      2019.000000\n",
      "75%      2019.000000\n",
      "max      2020.000000\n",
      "Name: TIME_PERIOD, dtype: float64\n",
      "\n",
      " - - - - - \n",
      " Extracting source S-66 \n",
      "\n",
      "\n",
      " Extracting data and store it as raw data\n",
      "   iso3  maternal_min_wrr_ilo  TIME_PERIOD\n",
      "0   AFG                     5         2019\n",
      "1   ALB                     3         2019\n",
      "2   DZA                     5         2019\n",
      "3   AND                     5         2019\n",
      "4   AGO                     5         2019\n",
      "5   ATG                     3         2019\n",
      "6   ARG                     5         2019\n",
      "7   ARM                     2         2019\n",
      "8   AUS                     2         2019\n",
      "9   AUT                     2         2019\n",
      "10  AZE                     2         2019\n",
      "11  BHS                     5         2019\n",
      "12  BHR                     5         2019\n",
      "13  BGD                     5         2019\n",
      "14  BRB                     5         2019\n",
      "15  BLR                     3         2019\n",
      "16  BEL                     2         2019\n",
      "17  BLZ                     5         2019\n",
      "18  BEN                     5         2019\n",
      "19  BTN                     5         2019\n",
      "20  BOL                     2         2019\n",
      "21  BIH                     3         2019\n",
      "22  BWA                     3         2019\n",
      "23  BRA                     5         2019\n",
      "24  BRN                     5         2019\n",
      "25  BGR                     5         2019\n",
      "26  BFA                     5         2019\n",
      "27  BDI                     5         2019\n",
      "28  KHM                     3         2019\n",
      "29  CMR                     5         2019\n",
      "There was an issue with source S-66\n",
      "\n",
      " - - - - - \n",
      " Cleansing source S-66 \n",
      "\n",
      "\n",
      " - - - - - \n",
      " Cleansing source S-66 \n",
      "\n",
      "\n",
      " Calling function 'rename_and_discard_columns'...\n",
      "[5 3 2 4 1]\n",
      "\n",
      " Calling function 'add_and_discard_countries'...\n",
      "\n",
      " Calling function 'add_cols_fill_cells'...\n",
      "\n",
      " Calling function 'encode_categorical_variables'...\n",
      "Cleansing done. This is some basic information about the data: \n",
      " \n",
      " There are 195 rows in the dataframe and 0.0% have a NA-value in the column 'OBS_RAW_VALUE\n",
      "\n",
      " \n",
      " This is the summary of the column 'TIME_PERIOD': count     195.000000\n",
      "mean     2019.010256\n",
      "std         0.101013\n",
      "min      2019.000000\n",
      "25%      2019.000000\n",
      "50%      2019.000000\n",
      "75%      2019.000000\n",
      "max      2020.000000\n",
      "Name: TIME_PERIOD, dtype: float64\n",
      "\n",
      " - - - - - \n",
      " Extracting source S-67 \n",
      "\n",
      "\n",
      " Extracting data and store it as raw data\n",
      "   iso3  paternal_leave  TIME_PERIOD\n",
      "0   AFG             2.0         2019\n",
      "1   ALB             1.0         2019\n",
      "2   DZA             2.0         2019\n",
      "3   AND             3.0         2019\n",
      "4   AGO             2.0         2019\n",
      "5   ATG             1.0         2019\n",
      "6   ARG             2.0         2019\n",
      "7   ARM             5.0         2019\n",
      "8   AUS             5.0         2019\n",
      "9   AUT             5.0         2019\n",
      "10  AZE             5.0         2019\n",
      "11  BHS             1.0         2019\n",
      "12  BHR             2.0         2019\n",
      "13  BGD             1.0         2019\n",
      "14  BRB             1.0         2019\n",
      "15  BLR             5.0         2019\n",
      "16  BEL             5.0         2019\n",
      "17  BLZ             1.0         2019\n",
      "18  BEN             2.0         2019\n",
      "19  BTN             2.0         2019\n",
      "20  BOL             2.0         2019\n",
      "21  BIH             2.0         2019\n",
      "22  BWA             1.0         2019\n",
      "23  BRA             2.0         2019\n",
      "24  BRN             1.0         2019\n",
      "25  BGR             5.0         2019\n",
      "26  BFA             1.0         2019\n",
      "27  BDI             2.0         2019\n",
      "28  KHM             1.0         2019\n",
      "29  CMR             1.0         2019\n",
      "There was an issue with source S-67\n",
      "\n",
      " - - - - - \n",
      " Cleansing source S-67 \n",
      "\n",
      "\n",
      " - - - - - \n",
      " Cleansing source S-67 \n",
      "\n",
      "\n",
      " Calling function 'rename_and_discard_columns'...\n",
      "[ 2.  1.  3.  5. nan]\n",
      "\n",
      " Calling function 'add_and_discard_countries'...\n",
      "\n",
      " Calling function 'add_cols_fill_cells'...\n",
      "\n",
      " Calling function 'encode_categorical_variables'...\n",
      "Cleansing done. This is some basic information about the data: \n",
      " \n",
      " There are 195 rows in the dataframe and 0.0% have a NA-value in the column 'OBS_RAW_VALUE\n",
      "\n",
      " \n",
      " This is the summary of the column 'TIME_PERIOD': count     195.000000\n",
      "mean     2019.010256\n",
      "std         0.101013\n",
      "min      2019.000000\n",
      "25%      2019.000000\n",
      "50%      2019.000000\n",
      "75%      2019.000000\n",
      "max      2020.000000\n",
      "Name: TIME_PERIOD, dtype: float64\n",
      "\n",
      " - - - - - \n",
      " Extracting source S-68 \n",
      "\n",
      "\n",
      " Extracting data and store it as raw data\n",
      "   iso3  breastfeed_duration  TIME_PERIOD\n",
      "0   AFG                  5.0         2019\n",
      "1   ALB                  5.0         2019\n",
      "2   DZA                  1.0         2019\n",
      "3   AND                  5.0         2019\n",
      "4   AGO                  5.0         2019\n",
      "5   ATG                  1.0         2019\n",
      "6   ARG                  5.0         2019\n",
      "7   ARM                  5.0         2019\n",
      "8   AUS                  1.0         2019\n",
      "9   AUT                  5.0         2019\n",
      "10  AZE                  5.0         2019\n",
      "11  BHS                  1.0         2019\n",
      "12  BHR                  5.0         2019\n",
      "13  BGD                  1.0         2019\n",
      "14  BRB                  1.0         2019\n",
      "15  BLR                  5.0         2019\n",
      "16  BEL                  5.0         2019\n",
      "17  BLZ                  1.0         2019\n",
      "18  BEN                  5.0         2019\n",
      "19  BTN                  2.0         2019\n",
      "20  BOL                  5.0         2019\n",
      "21  BIH                  5.0         2019\n",
      "22  BWA                  5.0         2019\n",
      "23  BRA                  5.0         2019\n",
      "24  BRN                  1.0         2019\n",
      "25  BGR                  5.0         2019\n",
      "26  BFA                  5.0         2019\n",
      "27  BDI                  5.0         2019\n",
      "28  KHM                  5.0         2019\n",
      "29  CMR                  5.0         2019\n",
      "There was an issue with source S-68\n",
      "\n",
      " - - - - - \n",
      " Cleansing source S-68 \n",
      "\n",
      "\n",
      " - - - - - \n",
      " Cleansing source S-68 \n",
      "\n",
      "\n",
      " Calling function 'rename_and_discard_columns'...\n",
      "[ 5.  1.  2.  4. nan]\n",
      "\n",
      " Calling function 'add_and_discard_countries'...\n",
      "\n",
      " Calling function 'add_cols_fill_cells'...\n",
      "\n",
      " Calling function 'encode_categorical_variables'...\n",
      "Cleansing done. This is some basic information about the data: \n",
      " \n",
      " There are 195 rows in the dataframe and 0.0% have a NA-value in the column 'OBS_RAW_VALUE\n",
      "\n",
      " \n",
      " This is the summary of the column 'TIME_PERIOD': count     195.000000\n",
      "mean     2019.010256\n",
      "std         0.101013\n",
      "min      2019.000000\n",
      "25%      2019.000000\n",
      "50%      2019.000000\n",
      "75%      2019.000000\n",
      "max      2020.000000\n",
      "Name: TIME_PERIOD, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 1. Create a flat file of all WPA sources\n",
    "# Read and join all world policy analysis centre data\n",
    "wpa_child_labor = pd.read_excel(\n",
    "    io = data_in / 'data_raw_manually_extracted' / 'S_8, S_9' / 'WORLD_child_labor.xls'\n",
    ")\n",
    "\n",
    "wpa_childhood = pd.read_excel(\n",
    "    io = data_in / 'data_raw_manually_extracted' / 'S_10, S_13, S_36, S_45, S_49' / 'WORLD_Dataset_Childhood_4.16.15.xls'\n",
    ")\n",
    "\n",
    "wpa_adult_labor = pd.read_excel(\n",
    "    io = data_in / 'data_raw_manually_extracted' / 'S_40, S_41, S_63, S_64, S_65, S_66, S_67, S_68' / 'WORLD_Dataset_Adult_Labor_9.17.2018.xls'\n",
    ")\n",
    "\n",
    "wpa_discrimination = pd.read_excel(\n",
    "    io = data_in / 'data_raw_manually_extracted' / 'S_42, S_43, S_44' / 'WORLD_discrimination_at_work.xls'\n",
    ")\n",
    "\n",
    "# Create list to write a loop\n",
    "wpa_combined_list=[\n",
    "    wpa_childhood,\n",
    "    wpa_adult_labor,\n",
    "    wpa_discrimination\n",
    " ]\n",
    "\n",
    "# Loop to join all dataframes\n",
    "wpa_combined = wpa_child_labor\n",
    "\n",
    "for df in wpa_combined_list:\n",
    "    wpa_combined = wpa_combined.merge(\n",
    "        right=df,\n",
    "        on=['iso2', 'iso3']\n",
    "    )\n",
    "\n",
    "# 2. Loop\n",
    "wpa_sources = crba_data_dictionary_source[\n",
    "    (crba_data_dictionary_source[\"SOURCE_BODY\"] == \"World Policy Analysis Centre\")\n",
    "].merge(\n",
    "    right = crba_data_dictionary_snapshot,\n",
    "    on = \"SOURCE_ID\"\n",
    ").merge(\n",
    "    right = crba_data_dictionary_indicator,\n",
    "    on = 'INDICATOR_ID'\n",
    ")\n",
    "\n",
    "# Loop to extract data from API sources\n",
    "for index, row in wpa_sources.iterrows():\n",
    "    # Log\n",
    "    print(\"\\n - - - - - \\n Extracting source {} \\n\".format(row[\"SOURCE_ID\"]))\n",
    "    \n",
    "    # Exraction section\n",
    "    #try:\n",
    "    # Extract data \n",
    "    # Log that we are entering cleasning\n",
    "    print(\"\\n Extracting data and store it as raw data\")\n",
    "\n",
    "    dataframe = wpa_combined[['iso3', row['WPA_OBS_RAW_COL']]] \n",
    "    dataframe['TIME_PERIOD'] = row['WPA_YEAR_COL'] \n",
    "\n",
    "    print(dataframe.head(30))\n",
    "\n",
    "    # Save dataframe\n",
    "    dataframe.to_csv(\n",
    "        data_sources_raw / str(row[\"SOURCE_ID\"] + \"_raw.csv\"),\n",
    "        sep = \";\")\n",
    "    #except:\n",
    "    print(\"There was an issue with source {}\".format(row[\"SOURCE_ID\"]))\n",
    "    \n",
    "    # Log that we are entering cleasning\n",
    "    print(\"\\n - - - - - \\n Cleansing source {} \\n\".format(row[\"SOURCE_ID\"]))\n",
    "    \n",
    "    # Cleansing \n",
    "    print(\"\\n - - - - - \\n Cleansing source {} \\n\".format(row[\"SOURCE_ID\"]))\n",
    "\n",
    "    dataframe = cleanse.Cleanser().rename_and_discard_columns(\n",
    "        raw_data=dataframe,\n",
    "        mapping_dictionary=mapping_dict,\n",
    "        final_sdmx_col_list=sdmx_df_columns_all\n",
    "    )\n",
    "\n",
    "    print(dataframe['RAW_OBS_VALUE'].unique())\n",
    "\n",
    "    dataframe = cleanse.Cleanser().add_and_discard_countries(\n",
    "        grouped_data=dataframe,\n",
    "        crba_country_list=country_crba_list,\n",
    "        country_list_full = country_full_list\n",
    "    )\n",
    "\n",
    "    dataframe = cleanse.Cleanser().add_cols_fill_cells(\n",
    "        grouped_data_iso_filt=dataframe,\n",
    "        dim_cols=sdmx_df_columns_dims,\n",
    "        time_cols=sdmx_df_columns_time,\n",
    "        indicator_name_string=row[\"INDICATOR_NAME_x\"],\n",
    "        index_name_string=row[\"INDEX\"],\n",
    "        issue_name_string=row[\"ISSUE\"],\n",
    "        category_name_string=row[\"CATEGORY\"],\n",
    "        indicator_code_string=row[\"INDICATOR_CODE\"],\n",
    "        indicator_source_string=row[\"ADDRESS\"],\n",
    "        indicator_source_body_string=row[\"SOURCE_BODY\"],\n",
    "        indicator_description_string=row[\"INDICATOR_DESCRIPTION\"],\n",
    "        indicator_explanation_string=row[\"INDICATOR_EXPLANATION\"],\n",
    "        indicator_data_extraction_methodology_string=row[\"EXTRACTION_METHODOLOGY\"],\n",
    "        source_title_string=row[\"SOURCE_TITLE\"],\n",
    "        source_api_link_string=row[\"ENDPOINT_URL\"]\n",
    "    )\n",
    "\n",
    "    dataframe_cleansed = cleanse.Cleanser().encode_categorical_variables(\n",
    "        dataframe = dataframe,\n",
    "        encoding_string = row[\"VALUE_ENCODING\"],\n",
    "        encoding_labels = row[\"VALUE_LABELS\"]\n",
    "    )\n",
    "\n",
    "    cleanse.Cleanser().create_log_report(\n",
    "        cleansed_data=dataframe_cleansed\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Append dataframe to combined dataframe\n",
    "    combined_cleansed_csv = combined_cleansed_csv.append(\n",
    "        other = dataframe_cleansed\n",
    "    )\n",
    "    \n",
    "\n",
    "    # Save cleansed data\n",
    "    dataframe_cleansed.to_csv(\n",
    "        data_sources_cleansed / str(row[\"SOURCE_ID\"] + \"_cleansed.csv\"),\n",
    "        sep = \";\")\n",
    "\n",
    "    # Normalizing\n",
    "    dataframe_normalized = scaler.normalizer(\n",
    "        cleansed_data = dataframe_cleansed,\n",
    "        sql_subset_query_string=row[\"DIMENSION_VALUES_NORMALIZATION\"],\n",
    "        # dim_cols=sdmx_df_columns_dims,\n",
    "        variable_type = row[\"VALUE_LABELS\"],\n",
    "        is_inverted = row[\"INVERT_NORMALIZATION\"],\n",
    "        whisker_factor=1.5,\n",
    "        raw_data_col=\"RAW_OBS_VALUE\",\n",
    "        scaled_data_col_name=\"SCALED_OBS_VALUE\",\n",
    "        maximum_score=10,\n",
    "        )\n",
    "\n",
    "    dataframe_normalized.to_csv(\n",
    "        data_sources_normalized / str(row[\"SOURCE_ID\"] + \"_normalized.csv\"),\n",
    "        sep = \";\")\n",
    "\n",
    "    # Append dataframe to combined dataframe\n",
    "    combined_normalized_csv = combined_normalized_csv.append(\n",
    "        other = dataframe_normalized\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export concatented dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # # CLEANSED DATA\n",
    "\n",
    "# Idenify all dimension columns in combined dataframe\n",
    "available_dim_cols = []\n",
    "for col in combined_cleansed_csv.columns:\n",
    "    dim_col = re.findall(\"DIM_.+\", col)\n",
    "    # print(dim_col)\n",
    "    if len(dim_col) == 1:\n",
    "        available_dim_cols += dim_col\n",
    "\n",
    "# Fill _T for all NA values of dimension columns\n",
    "# 5b Fill in current year for time variable\n",
    "combined_cleansed_csv[available_dim_cols] = combined_cleansed_csv[\n",
    "    available_dim_cols\n",
    "].fillna(value=\"_T\")\n",
    "\n",
    "# Export combined cleansed dataframe as a sample\n",
    "combined_cleansed_csv.to_csv(\n",
    "    path_or_buf = cwd / 'data_out' / 'combined_cleansed.csv',\n",
    "    sep = \";\"\n",
    ")\n",
    "\n",
    "# # # # # NORMALIZED DATA\n",
    "\n",
    "# Idenify all dimension columns in combined dataframe\n",
    "available_dim_cols = []\n",
    "for col in combined_normalized_csv.columns:\n",
    "    dim_col = re.findall(\"DIM_.+\", col)\n",
    "    # print(dim_col)\n",
    "    if len(dim_col) == 1:\n",
    "        available_dim_cols += dim_col\n",
    "\n",
    "# Fill _T for all NA values of dimension columns\n",
    "# 5b Fill in current year for time variable\n",
    "combined_normalized_csv[available_dim_cols] = combined_normalized_csv[\n",
    "    available_dim_cols\n",
    "].fillna(value=\"_T\")\n",
    "\n",
    "# S-101 has one duplicate row for country TON, drop that\n",
    "# This command should commented out when checking for other duplicat\n",
    "combined_normalized_csv = combined_normalized_csv.drop_duplicates()\n",
    "\n",
    "# Export combined cleansed dataframe as a sample\n",
    "combined_normalized_csv.to_csv(\n",
    "    path_or_buf = cwd / 'data_out' / 'combined_normalized.csv',\n",
    "    sep = \";\"\n",
    ")"
   ]
  },
  {
   "source": [
    "### Some Exploratory data analysis on final dataframe"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1243, 41)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 390.429545 248.518125\" width=\"390.429545pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2020-11-13T12:39:02.894121</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.2, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M -0 248.518125 \nL 390.429545 248.518125 \nL 390.429545 0 \nL -0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 39.65 224.64 \nL 374.45 224.64 \nL 374.45 7.2 \nL 39.65 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path clip-path=\"url(#p24850146a1)\" d=\"M 54.868182 224.64 \nL 65.013636 224.64 \nL 65.013636 224.603165 \nL 54.868182 224.603165 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path clip-path=\"url(#p24850146a1)\" d=\"M 65.013636 224.64 \nL 75.159091 224.64 \nL 75.159091 224.64 \nL 65.013636 224.64 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path clip-path=\"url(#p24850146a1)\" d=\"M 75.159091 224.64 \nL 85.304545 224.64 \nL 85.304545 224.64 \nL 75.159091 224.64 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path clip-path=\"url(#p24850146a1)\" d=\"M 85.304545 224.64 \nL 95.45 224.64 \nL 95.45 224.529495 \nL 85.304545 224.529495 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_7\">\n    <path clip-path=\"url(#p24850146a1)\" d=\"M 95.45 224.64 \nL 105.595455 224.64 \nL 105.595455 224.56633 \nL 95.45 224.56633 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_8\">\n    <path clip-path=\"url(#p24850146a1)\" d=\"M 105.595455 224.64 \nL 115.740909 224.64 \nL 115.740909 224.529495 \nL 105.595455 224.529495 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path clip-path=\"url(#p24850146a1)\" d=\"M 115.740909 224.64 \nL 125.886364 224.64 \nL 125.886364 224.455826 \nL 115.740909 224.455826 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path clip-path=\"url(#p24850146a1)\" d=\"M 125.886364 224.64 \nL 136.031818 224.64 \nL 136.031818 224.234816 \nL 125.886364 224.234816 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path clip-path=\"url(#p24850146a1)\" d=\"M 136.031818 224.64 \nL 146.177273 224.64 \nL 146.177273 224.050642 \nL 136.031818 224.050642 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_12\">\n    <path clip-path=\"url(#p24850146a1)\" d=\"M 146.177273 224.64 \nL 156.322727 224.64 \nL 156.322727 224.64 \nL 146.177273 224.64 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_13\">\n    <path clip-path=\"url(#p24850146a1)\" d=\"M 156.322727 224.64 \nL 166.468182 224.64 \nL 166.468182 224.308486 \nL 156.322727 224.308486 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_14\">\n    <path clip-path=\"url(#p24850146a1)\" d=\"M 166.468182 224.64 \nL 176.613636 224.64 \nL 176.613636 221.877384 \nL 166.468182 221.877384 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_15\">\n    <path clip-path=\"url(#p24850146a1)\" d=\"M 176.613636 224.64 \nL 186.759091 224.64 \nL 186.759091 223.0561 \nL 176.613636 223.0561 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_16\">\n    <path clip-path=\"url(#p24850146a1)\" d=\"M 186.759091 224.64 \nL 196.904545 224.64 \nL 196.904545 222.429907 \nL 186.759091 222.429907 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_17\">\n    <path clip-path=\"url(#p24850146a1)\" d=\"M 196.904545 224.64 \nL 207.05 224.64 \nL 207.05 222.245733 \nL 196.904545 222.245733 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_18\">\n    <path clip-path=\"url(#p24850146a1)\" d=\"M 207.05 224.64 \nL 217.195455 224.64 \nL 217.195455 203.496777 \nL 207.05 203.496777 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_19\">\n    <path clip-path=\"url(#p24850146a1)\" d=\"M 217.195455 224.64 \nL 227.340909 224.64 \nL 227.340909 220.735502 \nL 217.195455 220.735502 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_20\">\n    <path clip-path=\"url(#p24850146a1)\" d=\"M 227.340909 224.64 \nL 237.486364 224.64 \nL 237.486364 221.693209 \nL 227.340909 221.693209 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_21\">\n    <path clip-path=\"url(#p24850146a1)\" d=\"M 237.486364 224.64 \nL 247.631818 224.64 \nL 247.631818 218.193895 \nL 237.486364 218.193895 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_22\">\n    <path clip-path=\"url(#p24850146a1)\" d=\"M 247.631818 224.64 \nL 257.777273 224.64 \nL 257.777273 224.64 \nL 247.631818 224.64 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_23\">\n    <path clip-path=\"url(#p24850146a1)\" d=\"M 257.777273 224.64 \nL 267.922727 224.64 \nL 267.922727 208.764165 \nL 257.777273 208.764165 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_24\">\n    <path clip-path=\"url(#p24850146a1)\" d=\"M 267.922727 224.64 \nL 278.068182 224.64 \nL 278.068182 214.289398 \nL 267.922727 214.289398 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_25\">\n    <path clip-path=\"url(#p24850146a1)\" d=\"M 278.068182 224.64 \nL 288.213636 224.64 \nL 288.213636 210.163891 \nL 278.068182 210.163891 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_26\">\n    <path clip-path=\"url(#p24850146a1)\" d=\"M 288.213636 224.64 \nL 298.359091 224.64 \nL 298.359091 192.777826 \nL 288.213636 192.777826 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_27\">\n    <path clip-path=\"url(#p24850146a1)\" d=\"M 298.359091 224.64 \nL 308.504545 224.64 \nL 308.504545 103.895252 \nL 298.359091 103.895252 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_28\">\n    <path clip-path=\"url(#p24850146a1)\" d=\"M 308.504545 224.64 \nL 318.65 224.64 \nL 318.65 195.098424 \nL 308.504545 195.098424 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_29\">\n    <path clip-path=\"url(#p24850146a1)\" d=\"M 318.65 224.64 \nL 328.795455 224.64 \nL 328.795455 17.554286 \nL 318.65 17.554286 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_30\">\n    <path clip-path=\"url(#p24850146a1)\" d=\"M 328.795455 224.64 \nL 338.940909 224.64 \nL 338.940909 169.27717 \nL 328.795455 169.27717 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_31\">\n    <path clip-path=\"url(#p24850146a1)\" d=\"M 338.940909 224.64 \nL 349.086364 224.64 \nL 349.086364 144.671468 \nL 338.940909 144.671468 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"patch_32\">\n    <path clip-path=\"url(#p24850146a1)\" d=\"M 349.086364 224.64 \nL 359.231818 224.64 \nL 359.231818 29.267779 \nL 349.086364 29.267779 \nz\n\" style=\"fill:#1f77b4;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#p24850146a1)\" d=\"M 88.686364 224.64 \nL 88.686364 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m7ef56a6a2d\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"88.686364\" xlink:href=\"#m7ef56a6a2d\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 1995 -->\n      <g transform=\"translate(75.961364 239.238438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n        <path d=\"M 10.984375 1.515625 \nL 10.984375 10.5 \nQ 14.703125 8.734375 18.5 7.8125 \nQ 22.3125 6.890625 25.984375 6.890625 \nQ 35.75 6.890625 40.890625 13.453125 \nQ 46.046875 20.015625 46.78125 33.40625 \nQ 43.953125 29.203125 39.59375 26.953125 \nQ 35.25 24.703125 29.984375 24.703125 \nQ 19.046875 24.703125 12.671875 31.3125 \nQ 6.296875 37.9375 6.296875 49.421875 \nQ 6.296875 60.640625 12.9375 67.421875 \nQ 19.578125 74.21875 30.609375 74.21875 \nQ 43.265625 74.21875 49.921875 64.515625 \nQ 56.59375 54.828125 56.59375 36.375 \nQ 56.59375 19.140625 48.40625 8.859375 \nQ 40.234375 -1.421875 26.421875 -1.421875 \nQ 22.703125 -1.421875 18.890625 -0.6875 \nQ 15.09375 0.046875 10.984375 1.515625 \nz\nM 30.609375 32.421875 \nQ 37.25 32.421875 41.125 36.953125 \nQ 45.015625 41.5 45.015625 49.421875 \nQ 45.015625 57.28125 41.125 61.84375 \nQ 37.25 66.40625 30.609375 66.40625 \nQ 23.96875 66.40625 20.09375 61.84375 \nQ 16.21875 57.28125 16.21875 49.421875 \nQ 16.21875 41.5 20.09375 36.953125 \nQ 23.96875 32.421875 30.609375 32.421875 \nz\n\" id=\"DejaVuSans-57\"/>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-57\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-57\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#p24850146a1)\" d=\"M 145.05 224.64 \nL 145.05 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"145.05\" xlink:href=\"#m7ef56a6a2d\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 2000 -->\n      <g transform=\"translate(132.325 239.238438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#p24850146a1)\" d=\"M 201.413636 224.64 \nL 201.413636 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"201.413636\" xlink:href=\"#m7ef56a6a2d\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 2005 -->\n      <g transform=\"translate(188.688636 239.238438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#p24850146a1)\" d=\"M 257.777273 224.64 \nL 257.777273 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"257.777273\" xlink:href=\"#m7ef56a6a2d\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 2010 -->\n      <g transform=\"translate(245.052273 239.238438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#p24850146a1)\" d=\"M 314.140909 224.64 \nL 314.140909 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"314.140909\" xlink:href=\"#m7ef56a6a2d\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 2015 -->\n      <g transform=\"translate(301.415909 239.238438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#p24850146a1)\" d=\"M 370.504545 224.64 \nL 370.504545 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"370.504545\" xlink:href=\"#m7ef56a6a2d\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 2020 -->\n      <g transform=\"translate(357.779545 239.238438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_13\">\n      <path clip-path=\"url(#p24850146a1)\" d=\"M 39.65 224.64 \nL 374.45 224.64 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m1961d7a887\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#m1961d7a887\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0 -->\n      <g transform=\"translate(26.2875 228.439219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_15\">\n      <path clip-path=\"url(#p24850146a1)\" d=\"M 39.65 187.805117 \nL 374.45 187.805117 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#m1961d7a887\" y=\"187.805117\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 1000 -->\n      <g transform=\"translate(7.2 191.604335)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_17\">\n      <path clip-path=\"url(#p24850146a1)\" d=\"M 39.65 150.970233 \nL 374.45 150.970233 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#m1961d7a887\" y=\"150.970233\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 2000 -->\n      <g transform=\"translate(7.2 154.769452)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_19\">\n      <path clip-path=\"url(#p24850146a1)\" d=\"M 39.65 114.13535 \nL 374.45 114.13535 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#m1961d7a887\" y=\"114.13535\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 3000 -->\n      <g transform=\"translate(7.2 117.934569)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_21\">\n      <path clip-path=\"url(#p24850146a1)\" d=\"M 39.65 77.300467 \nL 374.45 77.300467 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#m1961d7a887\" y=\"77.300467\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 4000 -->\n      <g transform=\"translate(7.2 81.099685)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_23\">\n      <path clip-path=\"url(#p24850146a1)\" d=\"M 39.65 40.465583 \nL 374.45 40.465583 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#m1961d7a887\" y=\"40.465583\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 5000 -->\n      <g transform=\"translate(7.2 44.264802)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_33\">\n    <path d=\"M 39.65 224.64 \nL 39.65 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_34\">\n    <path d=\"M 374.45 224.64 \nL 374.45 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_35\">\n    <path d=\"M 39.65 224.64 \nL 374.45 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_36\">\n    <path d=\"M 39.65 7.2 \nL 374.45 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p24850146a1\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"39.65\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "# Print number of observations older than 2010\n",
    "old_data = combined_normalized_csv[combined_normalized_csv['TIME_PERIOD']<2010] \n",
    "print(old_data.shape)\n",
    "\n",
    "# Visualize distribution of age of observations\n",
    "combined_normalized_csv.loc[(combined_normalized_csv['TIME_PERIOD']>1990) & (combined_normalized_csv['TIME_PERIOD']<2020), 'TIME_PERIOD'].hist(bins = 30)\n",
    "\n",
    "# Number of observations older than 2010\n",
    "old_data_grouped = old_data.groupby('INDICATOR_NAME').count()\n",
    "\n",
    "# Retrieve total number of observations\n",
    "combined_normalized_csv_grouped = combined_normalized_csv.groupby('INDICATOR_NAME').count()\n",
    "\n",
    "# Compare the number of rows older 2010 and total number of rows per indicators\n",
    "old_data_analysis = old_data_grouped[['COUNTRY_ISO_3']].merge(\n",
    "    right = combined_normalized_csv_grouped[['COUNTRY_ISO_3']],\n",
    "    on = 'INDICATOR_NAME'\n",
    ")\n",
    "\n",
    "# Add column indicating % of obs older\n",
    "old_data_analysis['OBS_PERCENT_OLDER_2010'] = round((old_data_analysis[\"COUNTRY_ISO_3_x\"] / old_data_analysis[\"COUNTRY_ISO_3_y\"]) * 100, 1) \n",
    "\n",
    "\n",
    "old_data_analysis.to_csv(\n",
    "    path_or_buf = cwd / 'data_out' / 'percentage_old_data_per_indicator.csv',\n",
    "    sep = \";\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEVELOPMENT AND TRASH AREA"
   ]
  },
  {
   "source": [
    "### Develop ear extractor function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      " - - - - - \n",
      " Extracting source S-200 \n",
      "\n",
      "The following columns are present in the datasets, and this is the number of unique values they have. \n",
      "The column GHO has 1 unique values.\n",
      "The column PUBLISHSTATE has 1 unique values.\n",
      "The column YEAR has 1 unique values.\n",
      "The column REGION has 6 unique values.\n",
      "The column COUNTRY has 132 unique values.\n",
      "The column Display Value has 3 unique values.\n",
      "The column Numeric has 1 unique values.\n",
      "The column Low has 1 unique values.\n",
      "The column High has 1 unique values.\n",
      "The column Comments has 1 unique values.\n",
      "There was a problem with extraction of source S-200 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CSV sources\n",
    "api_sources = crba_data_dictionary_source[\n",
    "    (crba_data_dictionary_source[\"SOURCE_TYPE\"] == \"API (ILO)\") | \n",
    "    (crba_data_dictionary_source[\"SOURCE_TYPE\"] == \"API (UNESCO)\") | \n",
    "    (crba_data_dictionary_source[\"SOURCE_TYPE\"] == \"API (WHO)\") | \n",
    "    (crba_data_dictionary_source[\"SOURCE_TYPE\"] == \"API (UNICEF)\")\n",
    "].merge(\n",
    "    right = crba_data_dictionary_snapshot,\n",
    "    on = \"SOURCE_ID\"\n",
    ").merge(\n",
    "    right = crba_data_dictionary_indicator,\n",
    "    on = 'INDICATOR_ID'\n",
    ")\n",
    "\n",
    "# # # # # # # # # # # #\n",
    "# Delete again (only for temporary debugging 12.11.20)\n",
    "# # # # # # # # # # # # \n",
    "api_sources = api_sources[api_sources[\"SOURCE_ID\"] == 'S-200']\n",
    "\n",
    "# define emty dataframe\n",
    "combined_cleansed_csv = pd.DataFrame()\n",
    "combined_normalized_csv = pd.DataFrame()\n",
    "\n",
    "# Loop to extract data from API sources\n",
    "for index, row in api_sources.iterrows():\n",
    "    # Log\n",
    "    print(\"\\n - - - - - \\n Extracting source {} \\n\".format(row[\"SOURCE_ID\"]))\n",
    "    \n",
    "    # Extraction section\n",
    "    try:\n",
    "        # Extract data\n",
    "        dataframe = extract.CSVExtractor.extract(url = row[\"ENDPOINT_URL\"])\n",
    "        \n",
    "        # Save raw data\n",
    "        dataframe.to_csv(\n",
    "            data_sources_raw / str(row[\"SOURCE_ID\"] + \"_raw.csv\"),\n",
    "            sep = \";\"\n",
    "            )\n",
    "    \n",
    "    except:\n",
    "       print(\"There was a problem with extraction of source {} \\n\".format(row[\"SOURCE_ID\"]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_year_from_timeperiod(dataframe, year_col = \"TIME_PERIOD\", time_cov_col = \"COVERAGE_TIME\"):\n",
    "    # Log info for user\n",
    "    print(\"\\n Calling function 'extract_year_from_timeperiod'...\")\n",
    "\n",
    "    # Determine if the time period variable is normal, or of type e.g. \"2012 - 2014\"\n",
    "    is_time_period = re.search(\"-\", str(dataframe[year_col].unique()))\n",
    "    \n",
    "    # If it is, extract the actual year \n",
    "    if is_time_period:\n",
    "        # Store original column in new column\n",
    "        dataframe[time_cov_col] = dataframe[year_col]\n",
    "\n",
    "        # Define temp function (for readability rather than using lambda)\n",
    "        def year_extractor_temp(period_string):\n",
    "            \"\"\" From string 'startyear - endyear' extract average year\n",
    "            e.g. '2014 - 2016' -->  will return 2015\n",
    "            \"\"\"\n",
    "            result = int((int(re.findall('^\\d+',period_string)[0]) + int(re.findall('\\d+$',period_string)[0]))/ 2) # Return as integer, note that return of .findall is a list\n",
    "\n",
    "            return result\n",
    "        \n",
    "        # Apply function to column values and overwrite column values\n",
    "        dataframe[time_cov_col] =  dataframe[time_cov_col].apply(lambda x: year_extractor_temp(x))\n",
    "\n",
    "        # Log info for user\n",
    "        print(\"\\n TIME_PERIOD column contained time periods (no atomic years). Successfully extrated year. \")\n",
    "\n",
    "    else:\n",
    "        # If time period is just containing normal year values, do nothing\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe['YEAR'][:5]\n",
    "dataframe['YEAR'][1]\n",
    "\n",
    "(int(re.findall('^\\d+',dataframe['YEAR'][1])[0]) + int(re.findall('\\d+$',dataframe['YEAR'][1])[0]) )/ 2 \n",
    "\n",
    "match = re.search(\"-\", str(dataframe['YEAR'].unique()))\n",
    "if match:\n",
    "    dataframe['COVERAGE_TIME'] = dataframe['YEAR']\n",
    "    def year_extractor_temp(period_string):\n",
    "        \"\"\" From string 'startyear - endyear' extract average year\n",
    "        e.g. '2014 - 2016' -->  will return 2015\n",
    "        \"\"\"\n",
    "        result = int((int(re.findall('^\\d+',period_string)[0]) + int(re.findall('\\d+$',period_string)[0]))/ 2)\n",
    "\n",
    "        return result\n",
    "    dataframe['YEAR'] =  dataframe['YEAR'].apply(lambda x: year_extractor_temp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract selenium sources --> This code is stable as of 06.11.20, TO DO is to put this into a loop (which must be done in container, so I can only do it once James has looked at the issue with Chrome driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michael\\anaconda3\\envs\\unicef-test\\lib\\site-packages\\ipykernel_launcher.py:32: DeprecationWarning: use options instead of chrome_options\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Calling function 'rename_and_discard_columns'...\n",
      "\n",
      " Calling function 'add_and_discard_countries'...\n",
      "\n",
      " Calling function 'add_cols_fill_cells'...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COUNTRY_NAME</th>\n",
       "      <th>ATTR_TREATY_STATUS</th>\n",
       "      <th>ATTR_FOOTNOTE_OF_SOURCE</th>\n",
       "      <th>COUNTRY_ISO_2</th>\n",
       "      <th>COUNTRY_ISO_3</th>\n",
       "      <th>_merge</th>\n",
       "      <th>RAW_OBS_VALUE</th>\n",
       "      <th>ATTR_ENCODING_LABELS</th>\n",
       "      <th>SCALED_OBS_VALUE</th>\n",
       "      <th>OBS_STATUS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>In Force</td>\n",
       "      <td></td>\n",
       "      <td>AF</td>\n",
       "      <td>AFG</td>\n",
       "      <td>both</td>\n",
       "      <td>2</td>\n",
       "      <td>2=Yes, 1=No; as answer to the following questi...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Albania</td>\n",
       "      <td>In Force</td>\n",
       "      <td>Excluding Article 11 by virtue of the ratifica...</td>\n",
       "      <td>AL</td>\n",
       "      <td>ALB</td>\n",
       "      <td>both</td>\n",
       "      <td>2</td>\n",
       "      <td>2=Yes, 1=No; as answer to the following questi...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>In Force</td>\n",
       "      <td></td>\n",
       "      <td>DZ</td>\n",
       "      <td>DZA</td>\n",
       "      <td>both</td>\n",
       "      <td>2</td>\n",
       "      <td>2=Yes, 1=No; as answer to the following questi...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Argentina</td>\n",
       "      <td>In Force</td>\n",
       "      <td></td>\n",
       "      <td>AR</td>\n",
       "      <td>ARG</td>\n",
       "      <td>both</td>\n",
       "      <td>2</td>\n",
       "      <td>2=Yes, 1=No; as answer to the following questi...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Armenia</td>\n",
       "      <td>In Force</td>\n",
       "      <td>Excluding Article 11 by virtue of the ratifica...</td>\n",
       "      <td>AM</td>\n",
       "      <td>ARM</td>\n",
       "      <td>both</td>\n",
       "      <td>2</td>\n",
       "      <td>2=Yes, 1=No; as answer to the following questi...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>USA</td>\n",
       "      <td>right_only</td>\n",
       "      <td>1</td>\n",
       "      <td>2=Yes, 1=No; as answer to the following questi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UZB</td>\n",
       "      <td>right_only</td>\n",
       "      <td>1</td>\n",
       "      <td>2=Yes, 1=No; as answer to the following questi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>VUT</td>\n",
       "      <td>right_only</td>\n",
       "      <td>1</td>\n",
       "      <td>2=Yes, 1=No; as answer to the following questi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>VNM</td>\n",
       "      <td>right_only</td>\n",
       "      <td>1</td>\n",
       "      <td>2=Yes, 1=No; as answer to the following questi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ZWE</td>\n",
       "      <td>right_only</td>\n",
       "      <td>1</td>\n",
       "      <td>2=Yes, 1=No; as answer to the following questi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>197 rows Ã 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    COUNTRY_NAME ATTR_TREATY_STATUS  \\\n",
       "0    Afghanistan           In Force   \n",
       "1        Albania           In Force   \n",
       "2        Algeria           In Force   \n",
       "3      Argentina           In Force   \n",
       "4        Armenia           In Force   \n",
       "..           ...                ...   \n",
       "192          NaN                NaN   \n",
       "193          NaN                NaN   \n",
       "194          NaN                NaN   \n",
       "195          NaN                NaN   \n",
       "196          NaN                NaN   \n",
       "\n",
       "                               ATTR_FOOTNOTE_OF_SOURCE COUNTRY_ISO_2  \\\n",
       "0                                                                 AF   \n",
       "1    Excluding Article 11 by virtue of the ratifica...            AL   \n",
       "2                                                                 DZ   \n",
       "3                                                                 AR   \n",
       "4    Excluding Article 11 by virtue of the ratifica...            AM   \n",
       "..                                                 ...           ...   \n",
       "192                                                NaN           NaN   \n",
       "193                                                NaN           NaN   \n",
       "194                                                NaN           NaN   \n",
       "195                                                NaN           NaN   \n",
       "196                                                NaN           NaN   \n",
       "\n",
       "    COUNTRY_ISO_3      _merge RAW_OBS_VALUE  \\\n",
       "0             AFG        both             2   \n",
       "1             ALB        both             2   \n",
       "2             DZA        both             2   \n",
       "3             ARG        both             2   \n",
       "4             ARM        both             2   \n",
       "..            ...         ...           ...   \n",
       "192           USA  right_only             1   \n",
       "193           UZB  right_only             1   \n",
       "194           VUT  right_only             1   \n",
       "195           VNM  right_only             1   \n",
       "196           ZWE  right_only             1   \n",
       "\n",
       "                                  ATTR_ENCODING_LABELS  SCALED_OBS_VALUE  \\\n",
       "0    2=Yes, 1=No; as answer to the following questi...              10.0   \n",
       "1    2=Yes, 1=No; as answer to the following questi...              10.0   \n",
       "2    2=Yes, 1=No; as answer to the following questi...              10.0   \n",
       "3    2=Yes, 1=No; as answer to the following questi...              10.0   \n",
       "4    2=Yes, 1=No; as answer to the following questi...              10.0   \n",
       "..                                                 ...               ...   \n",
       "192  2=Yes, 1=No; as answer to the following questi...               0.0   \n",
       "193  2=Yes, 1=No; as answer to the following questi...               0.0   \n",
       "194  2=Yes, 1=No; as answer to the following questi...               0.0   \n",
       "195  2=Yes, 1=No; as answer to the following questi...               0.0   \n",
       "196  2=Yes, 1=No; as answer to the following questi...               0.0   \n",
       "\n",
       "    OBS_STATUS  \n",
       "0          nan  \n",
       "1          nan  \n",
       "2          nan  \n",
       "3          nan  \n",
       "4          nan  \n",
       "..         ...  \n",
       "192        nan  \n",
       "193        nan  \n",
       "194        nan  \n",
       "195        nan  \n",
       "196        nan  \n",
       "\n",
       "[197 rows x 10 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# Specify location of chromedriver\n",
    "cwd = os.getcwd()\n",
    "driver_location = cwd + '\\\\chromedriver.exe'\n",
    "\n",
    "# Add option to make it headless (so that it doesn't open an actual chrome window)\n",
    "options = Options()\n",
    "options.headless = True\n",
    "driver = webdriver.Chrome(driver_location, chrome_options=options)\n",
    "\n",
    "# Get HTTP response\n",
    "# response = driver.get(\"https://www.ilo.org/dyn/normlex/en/f?p=NORMLEXPUB:11300:0::NO:11300:P11300_INSTRUMENT_ID:312256:NO\")\n",
    "# response = driver.get(\"https://www.ilo.org/dyn/normlex/en/f?p=NORMLEXPUB:11300:0::NO::P11300_INSTRUMENT_ID:312283\")\n",
    "# response = driver.get(\"https://www.ilo.org/dyn/normlex/en/f?p=NORMLEXPUB:11300:0::NO:11300:P11300_INSTRUMENT_ID:312328:NO\")\n",
    "response = driver.get(\"https://www.ilo.org/dyn/normlex/en/f?p=NORMLEXPUB:11300:0::NO:11300:P11300_INSTRUMENT_ID:312240:NO\")\n",
    "\n",
    "# Get response\n",
    "# response = driver.get(html_url)\n",
    "\n",
    "# Retrieve the actual html\n",
    "html = driver.page_source\n",
    "\n",
    "# Soupify\n",
    "soup = bs.BeautifulSoup(html)\n",
    "\n",
    "# Extract the target table as attribute\n",
    "target_table = str(\n",
    "    soup.find_all(\"table\", {\"cellspacing\": \"0\", \"class\": \"horizontalLine\"})\n",
    ")\n",
    "\n",
    "# Create dataframe with the data\n",
    "raw_data = pd.read_html(io=target_table, header=0)[\n",
    "    0\n",
    "]  # return is a list of DFs, specify [0] to get actual DF\n",
    "\n",
    "# Cleansing\n",
    "dataframe = cleanse.Cleanser().rename_and_discard_columns(\n",
    "    raw_data=raw_data,\n",
    "    mapping_dictionary=mapping_dict,\n",
    "    final_sdmx_col_list=sdmx_df_columns_all\n",
    ")\n",
    "\n",
    "dataframe = cleanse.Cleanser().decompose_country_footnote_ilo_normlex(\n",
    "    dataframe = dataframe,\n",
    "    country_name_list = country_full_list.COUNTRY_NAME\n",
    ")\n",
    "\n",
    "dataframe = cleanse.Cleanser().add_and_discard_countries(\n",
    "    grouped_data=dataframe,\n",
    "    crba_country_list=country_crba_list,\n",
    "    country_list_full = country_full_list\n",
    ")\n",
    "\n",
    "dataframe_cleansed = cleanse.Cleanser().encode_ilo_un_treaty_data(\n",
    "    dataframe = dataframe,\n",
    "    treaty_source_body='ILO NORMLEX'\n",
    ")\n",
    "\n",
    "# Normalizing section\n",
    "dataframe_normalized = scaler.normalizer(\n",
    "    cleansed_data = dataframe_cleansed,\n",
    "    sql_subset_query_string=None\n",
    ")\n",
    "\n",
    "dataframe_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.sub(dataframe[\"ATTR_FOOTNOTE_OF_SOURCE\"][5]\n",
    "       \n",
    "# Speifically for ILO NORMLEX - extract country name if additonal info is given\n",
    "#dataframe[\"ATTR_FOOTNOTE_OF_SOURCE\"] = dataframe[\"COUNTRY_NAME\"]\n",
    "#dataframe[\"COUNTRY_NAME\"] = dataframe[\"COUNTRY_NAME\"].apply(extract_country_name)\n",
    "#dataframe[\"ATTR_FOOTNOTE_OF_SOURCE\"] = dataframe.apply(lambda x: re.sub(x['COUNTRY_NAME'], \"\", x[\"ATTR_FOOTNOTE_OF_SOURCE\"]), 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4 as bs\n",
    "import pandas as pd\n",
    "import selenium\n",
    "import os\n",
    "from pathlib import Path\n",
    "from selenium import webdriver\n",
    "\n",
    "# cwd = Path('.')\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Current working directory\n",
    "driver_location = cwd + '\\\\geckodriver.exe'\n",
    "\n",
    "print(driver_location)\n",
    "\n",
    "# Open the targete html. Must be done with selenium, because it doesnt work with normal URL request\n",
    "#driver = webdriver.Firefox(executable_path=\"D:/Documents/2020/28_UNICEF/10_working_repo/data-etl/geckodriver.exe\")\n",
    "driver = webdriver.Firefox(executable_path=driver_location)\n",
    "\n",
    "# Get HTTP response\n",
    "response = driver.get(\"https://www.ilo.org/dyn/normlex/en/f?p=NORMLEXPUB:11300:0::NO:11300:P11300_INSTRUMENT_ID:312256:NO\")\n"
   ]
  },
  {
   "source": [
    "## World policy analsis centre loop\n",
    "(deprecated, already taken into the operational mode above)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n - - - - - \n Extracting source S-49 \n\n\n Extracting data and store it as raw data\n   iso3  minwage_ppp  TIME_PERIOD\n0   AFG          4.0         2014\n1   ALB          5.0         2014\n2   DZA          5.0         2014\n3   AND          NaN         2014\n4   AGO          3.0         2014\n5   ATG          5.0         2014\n6   ARG          5.0         2014\n7   ARM          4.0         2014\n8   AUS          5.0         2014\n9   AUT          5.0         2014\n10  AZE          4.0         2014\n11  BHS          5.0         2014\n12  BHR          1.0         2014\n13  BGD          2.0         2014\n14  BRB          5.0         2014\n15  BLR          5.0         2014\n16  BEL          5.0         2014\n17  BLZ          5.0         2014\n18  BEN          3.0         2014\n19  BTN          4.0         2014\n20  BOL          4.0         2014\n21  BIH          5.0         2014\n22  BWA          4.0         2014\n23  BRA          5.0         2014\n24  BRN          1.0         2014\n25  BGR          5.0         2014\n26  BFA          3.0         2014\n27  BDI          2.0         2014\n28  KHM          2.0         2014\n29  CMR          3.0         2014\n30  CAN          5.0         2014\n31  CPV          1.0         2014\n32  CAF          2.0         2014\n33  TCD          3.0         2014\n34  CHL          5.0         2014\n35  CHN          3.0         2014\n36  COL          5.0         2014\n37  COM          3.0         2014\n38  COD          3.0         2014\n39  COG          4.0         2014\n40  CRI          5.0         2014\n41  CIV          3.0         2014\n42  HRV          5.0         2014\n43  CUB          NaN         2014\n44  CYP          5.0         2014\n45  CZE          5.0         2014\n46  DNK        999.0         2014\n47  DJI          1.0         2014\n48  DMA          5.0         2014\n49  DOM          4.0         2014\nThere was an issue with source S-49\n\n - - - - - \n Cleansing source S-49 \n\n\n - - - - - \n Cleansing source S-49 \n\n\n Calling function 'rename_and_discard_columns'...\n[  4.   5.  nan   3.   1.   2. 999.]\n\n Calling function 'add_and_discard_countries'...\n\n Calling function 'add_cols_fill_cells'...\n\n Calling function 'encode_categorical_variables'...\nCleansing done. This is some basic information about the data: \n \n There are 195 rows in the dataframe and 0.0% have a NA-value in the column 'OBS_RAW_VALUE\n\n \n This is the summary of the column 'TIME_PERIOD': count     195.000000\nmean     2014.061538\nstd         0.606076\nmin      2014.000000\n25%      2014.000000\n50%      2014.000000\n75%      2014.000000\nmax      2020.000000\nName: TIME_PERIOD, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 1. Create a flat file of all WPA sources\n",
    "# Read and join all world policy analysis centre data\n",
    "wpa_child_labor = pd.read_excel(\n",
    "    io = data_in / 'data_raw_manually_extracted' / 'S_8, S_9' / 'WORLD_child_labor.xls'\n",
    ")\n",
    "\n",
    "wpa_childhood = pd.read_excel(\n",
    "    io = data_in / 'data_raw_manually_extracted' / 'S_10, S_13, S_36, S_45, S_49' / 'WORLD_Dataset_Childhood_4.16.15.xls'\n",
    ")\n",
    "\n",
    "wpa_adult_labor = pd.read_excel(\n",
    "    io = data_in / 'data_raw_manually_extracted' / 'S_40, S_41, S_63, S_64, S_65, S_66, S_67, S_68' / 'WORLD_Dataset_Adult_Labor_9.17.2018.xls'\n",
    ")\n",
    "\n",
    "wpa_discrimination = pd.read_excel(\n",
    "    io = data_in / 'data_raw_manually_extracted' / 'S_42, S_43, S_44' / 'WORLD_discrimination_at_work.xls'\n",
    ")\n",
    "\n",
    "# Create list to write a loop\n",
    "wpa_combined_list=[\n",
    "    wpa_childhood,\n",
    "    wpa_adult_labor,\n",
    "    wpa_discrimination\n",
    " ]\n",
    "\n",
    "# Loop to join all dataframes\n",
    "wpa_combined = wpa_child_labor\n",
    "\n",
    "for df in wpa_combined_list:\n",
    "    wpa_combined = wpa_combined.merge(\n",
    "        right=df,\n",
    "        on=['iso2', 'iso3']\n",
    "    )\n",
    "\n",
    "# 2. Loop\n",
    "\n",
    "# WPA sources\n",
    "wpa_sources = crba_data_dictionary_source[\n",
    "    (crba_data_dictionary_source[\"SOURCE_BODY\"] == \"World Policy Analysis Centre\")\n",
    "].merge(\n",
    "    right = crba_data_dictionary_snapshot,\n",
    "    on = \"SOURCE_ID\"\n",
    ").merge(\n",
    "    right = crba_data_dictionary_indicator,\n",
    "    on = 'INDICATOR_ID'\n",
    ")\n",
    "\n",
    "#print(wpa_sources)\n",
    "\n",
    "# # # # # # # Temp, delete this again # # # # \n",
    "wpa_sources = wpa_sources[wpa_sources[\"SOURCE_ID\"] == 'S-49']\n",
    "# combined_cleansed_csv = pd.DataFrame()\n",
    "# combined_normalized_csv = pd.DataFrame()\n",
    "\n",
    "#print(wpa_sources[\"SOURCE_ID\"] == 'S-92')\n",
    "\n",
    "# Loop to extract data from API sources\n",
    "for index, row in wpa_sources.iterrows():\n",
    "    # Log\n",
    "    print(\"\\n - - - - - \\n Extracting source {} \\n\".format(row[\"SOURCE_ID\"]))\n",
    "    \n",
    "    # Exraction section\n",
    "    #try:\n",
    "    # Extract data \n",
    "    # Log that we are entering cleasning\n",
    "    print(\"\\n Extracting data and store it as raw data\")\n",
    "\n",
    "    dataframe = wpa_combined[['iso3', row['WPA_OBS_RAW_COL']]] \n",
    "    dataframe['TIME_PERIOD'] = row['WPA_YEAR_COL'] \n",
    "\n",
    "    print(dataframe.head(50))\n",
    "\n",
    "    # Save dataframe\n",
    "    dataframe.to_csv(\n",
    "        data_sources_raw / str(row[\"SOURCE_ID\"] + \"_raw.csv\"),\n",
    "        sep = \";\")\n",
    "    #except:\n",
    "    print(\"There was an issue with source {}\".format(row[\"SOURCE_ID\"]))\n",
    "    \n",
    "    # Log that we are entering cleasning\n",
    "    print(\"\\n - - - - - \\n Cleansing source {} \\n\".format(row[\"SOURCE_ID\"]))\n",
    "    \n",
    "    # Cleansing \n",
    "    print(\"\\n - - - - - \\n Cleansing source {} \\n\".format(row[\"SOURCE_ID\"]))\n",
    "\n",
    "    dataframe = cleanse.Cleanser().rename_and_discard_columns(\n",
    "        raw_data=dataframe,\n",
    "        mapping_dictionary=mapping_dict,\n",
    "        final_sdmx_col_list=sdmx_df_columns_all\n",
    "    )\n",
    "\n",
    "    print(dataframe['RAW_OBS_VALUE'].unique())\n",
    "\n",
    "    dataframe = cleanse.Cleanser().add_and_discard_countries(\n",
    "        grouped_data=dataframe,\n",
    "        crba_country_list=country_crba_list,\n",
    "        country_list_full = country_full_list\n",
    "    )\n",
    "\n",
    "    dataframe = cleanse.Cleanser().add_cols_fill_cells(\n",
    "        grouped_data_iso_filt=dataframe,\n",
    "        dim_cols=sdmx_df_columns_dims,\n",
    "        time_cols=sdmx_df_columns_time,\n",
    "        indicator_name_string=row[\"INDICATOR_NAME_x\"],\n",
    "        index_name_string=row[\"INDEX\"],\n",
    "        issue_name_string=row[\"ISSUE\"],\n",
    "        category_name_string=row[\"CATEGORY\"],\n",
    "        indicator_code_string=row[\"INDICATOR_CODE\"],\n",
    "        indicator_source_string=row[\"ADDRESS\"],\n",
    "        indicator_source_body_string=row[\"SOURCE_BODY\"],\n",
    "        indicator_description_string=row[\"INDICATOR_DESCRIPTION\"],\n",
    "        indicator_explanation_string=row[\"INDICATOR_EXPLANATION\"],\n",
    "        indicator_data_extraction_methodology_string=row[\"EXTRACTION_METHODOLOGY\"],\n",
    "        source_title_string=row[\"SOURCE_TITLE\"],\n",
    "        source_api_link_string=row[\"ENDPOINT_URL\"]\n",
    "    )\n",
    "\n",
    "    dataframe_cleansed = cleanse.Cleanser().encode_categorical_variables(\n",
    "        dataframe = dataframe,\n",
    "        encoding_string = row[\"VALUE_ENCODING\"],\n",
    "        encoding_labels = row[\"VALUE_LABELS\"]\n",
    "    )\n",
    "\n",
    "    cleanse.Cleanser().create_log_report(\n",
    "        cleansed_data=dataframe_cleansed\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Append dataframe to combined dataframe\n",
    "    combined_cleansed_csv = combined_cleansed_csv.append(\n",
    "        other = dataframe_cleansed\n",
    "    )\n",
    "    \n",
    "\n",
    "    # Save cleansed data\n",
    "    dataframe_cleansed.to_csv(\n",
    "        data_sources_cleansed / str(row[\"SOURCE_ID\"] + \"_cleansed.csv\"),\n",
    "        sep = \";\")\n",
    "\n",
    "    # Normalizing\n",
    "    dataframe_normalized = scaler.normalizer(\n",
    "        cleansed_data = dataframe_cleansed,\n",
    "        sql_subset_query_string=row[\"DIMENSION_VALUES_NORMALIZATION\"],\n",
    "        # dim_cols=sdmx_df_columns_dims,\n",
    "        variable_type = row[\"VALUE_LABELS\"],\n",
    "        is_inverted = row[\"INVERT_NORMALIZATION\"],\n",
    "        whisker_factor=1.5,\n",
    "        raw_data_col=\"RAW_OBS_VALUE\",\n",
    "        scaled_data_col_name=\"SCALED_OBS_VALUE\",\n",
    "        maximum_score=10,\n",
    "        )\n",
    "\n",
    "    dataframe_normalized.to_csv(\n",
    "        data_sources_normalized / str(row[\"SOURCE_ID\"] + \"_normalized.csv\"),\n",
    "        sep = \";\")\n",
    "\n",
    "    # Append dataframe to combined dataframe\n",
    "    combined_normalized_csv = combined_normalized_csv.append(\n",
    "        other = dataframe_normalized\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n Calling function 'encode_categorical_variables'...\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    iso3  edu_comp_begsec  TIME_PERIOD RAW_OBS_VALUE  \\\n",
       "0    AFG              5.0         2019             2   \n",
       "1    ALB              5.0         2019             2   \n",
       "2    DZA              5.0         2019             2   \n",
       "3    AND              5.0         2019             2   \n",
       "4    AGO              1.0         2019             1   \n",
       "..   ...              ...          ...           ...   \n",
       "188  VEN              5.0         2019             2   \n",
       "189  VNM              1.0         2019             1   \n",
       "190  YEM              5.0         2019             2   \n",
       "191  ZMB              1.0         2019             1   \n",
       "192  ZWE              1.0         2019             1   \n",
       "\n",
       "                               ATTR_ENCODING_LABELS  \n",
       "0    2 = Compulsory; 1= Not compulsory; 0 = No data  \n",
       "1    2 = Compulsory; 1= Not compulsory; 0 = No data  \n",
       "2    2 = Compulsory; 1= Not compulsory; 0 = No data  \n",
       "3    2 = Compulsory; 1= Not compulsory; 0 = No data  \n",
       "4    2 = Compulsory; 1= Not compulsory; 0 = No data  \n",
       "..                                              ...  \n",
       "188  2 = Compulsory; 1= Not compulsory; 0 = No data  \n",
       "189  2 = Compulsory; 1= Not compulsory; 0 = No data  \n",
       "190  2 = Compulsory; 1= Not compulsory; 0 = No data  \n",
       "191  2 = Compulsory; 1= Not compulsory; 0 = No data  \n",
       "192  2 = Compulsory; 1= Not compulsory; 0 = No data  \n",
       "\n",
       "[193 rows x 5 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>iso3</th>\n      <th>edu_comp_begsec</th>\n      <th>TIME_PERIOD</th>\n      <th>RAW_OBS_VALUE</th>\n      <th>ATTR_ENCODING_LABELS</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>AFG</td>\n      <td>5.0</td>\n      <td>2019</td>\n      <td>2</td>\n      <td>2 = Compulsory; 1= Not compulsory; 0 = No data</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ALB</td>\n      <td>5.0</td>\n      <td>2019</td>\n      <td>2</td>\n      <td>2 = Compulsory; 1= Not compulsory; 0 = No data</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>DZA</td>\n      <td>5.0</td>\n      <td>2019</td>\n      <td>2</td>\n      <td>2 = Compulsory; 1= Not compulsory; 0 = No data</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>AND</td>\n      <td>5.0</td>\n      <td>2019</td>\n      <td>2</td>\n      <td>2 = Compulsory; 1= Not compulsory; 0 = No data</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>AGO</td>\n      <td>1.0</td>\n      <td>2019</td>\n      <td>1</td>\n      <td>2 = Compulsory; 1= Not compulsory; 0 = No data</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>188</th>\n      <td>VEN</td>\n      <td>5.0</td>\n      <td>2019</td>\n      <td>2</td>\n      <td>2 = Compulsory; 1= Not compulsory; 0 = No data</td>\n    </tr>\n    <tr>\n      <th>189</th>\n      <td>VNM</td>\n      <td>1.0</td>\n      <td>2019</td>\n      <td>1</td>\n      <td>2 = Compulsory; 1= Not compulsory; 0 = No data</td>\n    </tr>\n    <tr>\n      <th>190</th>\n      <td>YEM</td>\n      <td>5.0</td>\n      <td>2019</td>\n      <td>2</td>\n      <td>2 = Compulsory; 1= Not compulsory; 0 = No data</td>\n    </tr>\n    <tr>\n      <th>191</th>\n      <td>ZMB</td>\n      <td>1.0</td>\n      <td>2019</td>\n      <td>1</td>\n      <td>2 = Compulsory; 1= Not compulsory; 0 = No data</td>\n    </tr>\n    <tr>\n      <th>192</th>\n      <td>ZWE</td>\n      <td>1.0</td>\n      <td>2019</td>\n      <td>1</td>\n      <td>2 = Compulsory; 1= Not compulsory; 0 = No data</td>\n    </tr>\n  </tbody>\n</table>\n<p>193 rows Ã 5 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "# Read and join all world policy analysis centre data\n",
    "wpa_child_labor = pd.read_excel(\n",
    "    io = data_in / 'data_raw_manually_extracted' / 'S_8, S_9' / 'WORLD_child_labor.xls'\n",
    ")\n",
    "\n",
    "wpa_childhood = pd.read_excel(\n",
    "    io = data_in / 'data_raw_manually_extracted' / 'S_10, S_13, S_36, S_45, S_49' / 'WORLD_Dataset_Childhood_4.16.15.xls'\n",
    ")\n",
    "\n",
    "wpa_adult_labor = pd.read_excel(\n",
    "    io = data_in / 'data_raw_manually_extracted' / 'S_40, S_41, S_63, S_64, S_65, S_66, S_67, S_68' / 'WORLD_Dataset_Adult_Labor_9.17.2018.xls'\n",
    ")\n",
    "\n",
    "wpa_discrimination = pd.read_excel(\n",
    "    io = data_in / 'data_raw_manually_extracted' / 'S_42, S_43, S_44' / 'WORLD_discrimination_at_work.xls'\n",
    ")\n",
    "\n",
    "# Create list to write a loop\n",
    "wpa_combined_list=[\n",
    "    wpa_childhood,\n",
    "    wpa_adult_labor,\n",
    "    wpa_discrimination\n",
    " ]\n",
    "\n",
    "# Loop to join all dataframes\n",
    "wpa_combined = wpa_child_labor\n",
    "\n",
    "for df in wpa_combined_list:\n",
    "    wpa_combined = wpa_combined.merge(\n",
    "        right=df,\n",
    "        on=['iso2', 'iso3']\n",
    "    )\n",
    "\n",
    "s8_raw = wpa_combined[['iso3', 'admiss_age']]\n",
    "s8_raw['TIME_PERIOD'] = 2019\n",
    "\n",
    "s9_raw = wpa_combined[['iso3', 'light_age']]\n",
    "s9_raw['TIME_PERIOD'] = 2019\n",
    "\n",
    "s13_raw = wpa_combined[['iso3', 'cl_haz_minage']]\n",
    "s13_raw['TIME_PERIOD'] = 2014\n",
    "\n",
    "s36_raw = wpa_combined[['iso3', 'minwage_leg']]\n",
    "s36_raw['TIME_PERIOD'] = 2014\n",
    "\n",
    "s41_raw = wpa_combined[['iso3', 'sickleave_duration']]\n",
    "s41_raw['TIME_PERIOD'] = 2014\n",
    "\n",
    "s42_raw = wpa_combined[['iso3', 'promdemo_sex']]\n",
    "s42_raw['TIME_PERIOD'] = 2014\n",
    "\n",
    "s67_raw = wpa_combined[['iso3', 'paternal_leave']]\n",
    "s67_raw['TIME_PERIOD'] = 2014\n",
    "# # # # \n",
    "\n",
    "\n",
    "s10_raw = wpa_combined[['iso3', 'edu_comp_begsec']]\n",
    "s10_raw['TIME_PERIOD'] = 2019\n",
    "\n",
    "s10_raw\n",
    "\n",
    "dataframe_cleansed = cleanse.Cleanser().encode_categorical_variables(\n",
    "    dataframe = s10_raw,\n",
    "    encoding_string = '2 = 5.0; 1= 1.0',\n",
    "    na_encodings='No data',\n",
    "    obs_raw_value_source='edu_comp_begsec',\n",
    "    obs_raw_value_target=\"RAW_OBS_VALUE\",\n",
    "    encoding_labels='2 = Compulsory; 1= Not compulsory; 0 = No data'\n",
    ")\n",
    "\n",
    "# dataframe_cleansed['RAW_OBS_VALUE'].unique()\n",
    "dataframe_cleansed\n",
    "\n",
    "\n",
    "# # # ## \n",
    "\"\"\"\n",
    "s40_raw = wpa_combined[['iso3', 'paid_anlv']]\n",
    "s40_raw['TIME_PERIOD'] = 2019\n",
    "\n",
    "s40_raw\n",
    "\n",
    "dataframe_cleansed = cleanse.Cleanser().encode_categorical_variables(\n",
    "    dataframe = s40_raw,\n",
    "    encoding_string = '20 days or more=5.0; 15-19 days=4.0; Set externally=3.0; 5-9 days = 2.0; No paid annual leave = 1.0',\n",
    "    na_encodings='No data',\n",
    "    obs_raw_value_source='paid_anlv',\n",
    "    obs_raw_value_target=\"RAW_OBS_VALUE\"\n",
    ")\n",
    "\n",
    "dataframe_cleansed['RAW_OBS_VALUE'].unique()\n",
    "\"\"\"\n",
    "dataframe_cleansed"
   ]
  },
  {
   "source": [
    "## GHG\n",
    "\n",
    "problem: \n",
    "\n",
    "* Cannot use pd_json_normalize --> It won't unnest it becaust he years and actual values are given in [] rather than {}\n",
    "* Stopped here: Must replace the character (but for that it is necessary to convert json to string and back)\n",
    "* Next challnge: Get all data (and not only page 1) --> Main problem is that the API is not working, i.e. can't specify paramters\n",
    "\n",
    "Stopped here 10.11.20 --> Need to get rid of \"[]\" signgs for "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_json = requests.get(\"https://www.climatewatchdata.org/api/v1/data/historical_emissions\").text\n",
    "\n",
    "dataframe_json_cleansed = dataframe_json.replace('[', '{').replace(']','}')\n",
    "json_file = json.dumps(dataframe_json_cleansed)\n",
    "# some_var = json.loads(dataframe_json_cleansed)\n",
    "# type(some_var)\n",
    "# pd.json_normalize(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'id': 518203,\n",
       " 'iso_code3': 'AFG',\n",
       " 'country': 'Afghanistan',\n",
       " 'data_source': 'CAIT',\n",
       " 'sector': 'Total excluding LUCF',\n",
       " 'gas': 'CH4',\n",
       " 'unit': 'MtCOâe',\n",
       " 'emissions': [{'year': 1990, 'value': 8.97},\n",
       "  {'year': 1991, 'value': 9.07},\n",
       "  {'year': 1992, 'value': 9.0},\n",
       "  {'year': 1993, 'value': 8.9},\n",
       "  {'year': 1994, 'value': 8.97},\n",
       "  {'year': 1995, 'value': 9.15},\n",
       "  {'year': 1996, 'value': 9.93},\n",
       "  {'year': 1997, 'value': 10.6},\n",
       "  {'year': 1998, 'value': 11.1},\n",
       "  {'year': 1999, 'value': 11.87},\n",
       "  {'year': 2000, 'value': 10.59},\n",
       "  {'year': 2001, 'value': 9.36},\n",
       "  {'year': 2002, 'value': 11.21},\n",
       "  {'year': 2003, 'value': 11.56},\n",
       "  {'year': 2004, 'value': 11.47},\n",
       "  {'year': 2005, 'value': 11.68},\n",
       "  {'year': 2006, 'value': 14.89},\n",
       "  {'year': 2007, 'value': 18.1},\n",
       "  {'year': 2008, 'value': 22.19},\n",
       "  {'year': 2009, 'value': 25.43},\n",
       "  {'year': 2010, 'value': 30.04},\n",
       "  {'year': 2011, 'value': 39.48},\n",
       "  {'year': 2012, 'value': 48.78},\n",
       "  {'year': 2013, 'value': 58.13},\n",
       "  {'year': 2014, 'value': 67.77},\n",
       "  {'year': 2015, 'value': 76.62},\n",
       "  {'year': 2016, 'value': 78.17}]}"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "#dataframe = extract.CSVExtractor.extract(url = \"https://www.climatewatchdata.org/api/v1/data/historical_emissions\")\n",
    "\n",
    "dataframe_json = requests.get(\"https://www.climatewatchdata.org/api/v1/data/historical_emissions\").json()\n",
    "\n",
    "cleaned_json = dataframe_json['data']# .replace('[', '{').replace\n",
    "type(cleaned_json)\n",
    "\"\"\"\n",
    "# with open(dataframe_json, 'r') as file:\n",
    "    content = file.read()\n",
    "    clean = content.replace(']', '}')  # cleanup here\n",
    "    json_data = json.loads(clean)\n",
    "\"\"\"\n",
    "\n",
    "# dataframe_2 = pd.json_normalize(dataframe_json['data'], max_level=5)\n",
    "\n",
    "# dataframe_3 = pd.json_normalize(dataframe_json['data)\n",
    "\n",
    "\n",
    "#dataframe_3\n",
    "\n",
    "cleaned_json[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "import pandas\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# Specify location of chromedriver\n",
    "cwd = os.getcwd()\n",
    "driver_location = cwd + '\\\\chromedriver.exe'\n",
    "\n",
    "# Add option to make it headless (so that it doesn't open an actual chrome window)\n",
    "options = Options()\n",
    "options.headless = True\n",
    "driver = webdriver.Chrome(driver_location, chrome_options=options)\n",
    "\n",
    "# Get HTTP response\n",
    "# response = driver.get(\"https://www.ilo.org/dyn/normlex/en/f?p=NORMLEXPUB:11300:0::NO:11300:P11300_INSTRUMENT_ID:312256:NO\")\n",
    "response = driver.get(\"https://www.ilo.org/dyn/normlex/en/f?p=NORMLEXPUB:11300:0::NO::P11300_INSTRUMENT_ID:312283\")\n",
    "\n",
    "# Get response\n",
    "# response = driver.get(html_url)\n",
    "\n",
    "# Retrieve the actual html\n",
    "html = driver.page_source\n",
    "\n",
    "# Soupify\n",
    "soup = bs.BeautifulSoup(html)\n",
    "\n",
    "# Extract the target table as attribute\n",
    "target_table = str(\n",
    "    soup.find_all(\"table\", {\"cellspacing\": \"0\", \"class\": \"horizontalLine\"})\n",
    ")\n",
    "\n",
    "# Create dataframe with the data\n",
    "raw_data = pd.read_html(io=target_table, header=0)[\n",
    "    0\n",
    "]  # return is a list of DFs, specify [0] to get actual DF\n",
    "\n",
    "# Cleansing\n",
    "dataframe = cleanse.Cleanser().rename_and_discard_columns(\n",
    "    raw_data=raw_data,\n",
    "    mapping_dictionary=mapping_dict,\n",
    "    final_sdmx_col_list=sdmx_df_columns_all\n",
    ")\n",
    "\n",
    "# dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_country_name(cell, country_name_list = country_full_list.COUNTRY_NAME):\n",
    "    # Determine which country in the full country list is contained in string\n",
    "    subset_list = [x in cell for x in country_name_list]\n",
    "    \n",
    "    # Sometimes several country names match, but we need exactly one\n",
    "    if sum(subset_list)==0:\n",
    "        print(\"No country name match\")\n",
    "    elif sum(subset_list)==1:\n",
    "        # Retrieve the actual country name\n",
    "        country_name = country_name_list[subset_list].item()\n",
    "    else:\n",
    "        # Retrieve the actual country name\n",
    "        country_name = country_name_list[subset_list].iloc[0]\n",
    "    \n",
    "    return country_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_full_list.COUNTRY_NAME[subset_list].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "country_full_list\n",
    "    \n",
    "subset_list = [x in dataframe.COUNTRY_NAME[2] for x in country_full_list.COUNTRY_NAME]\n",
    "\n",
    "dataframe.COUNTRY_NAME[2] = country_full_list.loc[subset_list, \"COUNTRY_NAME\"].item()\n",
    "\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract UN Treaty data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib\n",
    "import bs4 as bs\n",
    "import pandas as pd\n",
    "\n",
    "raw_html_1 = requests.get(\"https://treaties.un.org/pages/ViewDetails.aspx?src=TREATY&mtdsg_no=XVIII-12-a&chapter=18&clang=_en\")\n",
    "\n",
    "# raw_html_1.text\n",
    "raw_html_2 = urllib.request.urlopen(\"https://treaties.un.org/pages/ViewDetails.aspx?src=TREATY&mtdsg_no=XVIII-12-a&chapter=18&clang=_en\")\n",
    "\n",
    "print(raw_html_1)\n",
    "print(raw_html_2)\n",
    "\n",
    "soup = bs.BeautifulSoup(raw_html_1.text, features=\"lxml\")\n",
    "#soup_2 = bs.BeautifulSoup(raw_html_2, features=\"lxml\")\n",
    "\n",
    "#soup_2\n",
    "# soup_1\n",
    "\n",
    "# Extract the target table as attribute\n",
    "target_table = str(\n",
    "    soup.find_all(\n",
    "        \"table\",\n",
    "        {\"class\": \"table table-striped table-bordered table-hover table-condensed\"},\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create dataframe with the data\n",
    "raw_data = pd.read_html(io=target_table, header=0)[\n",
    "    0\n",
    "]\n",
    "\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify duplicates\n",
    "\n",
    "# Checking to see if there are any duplicate entries\n",
    "duplicates = combined_normalized_csv[combined_normalized_csv.duplicated(\n",
    "    subset = [\n",
    "        \"COUNTRY_ISO_3\",\n",
    "        \"TIME_PERIOD\",\n",
    "        \"COUNTRY_NAME\",\n",
    "        \"COUNTRY_ISO_2\",\n",
    "        \"RAW_OBS_VALUE\",\n",
    "        \"DIM_SEX\",\n",
    "        \"DIM_EDU_LEVEL\",\n",
    "        \"DIM_AGE\",\n",
    "        \"DIM_AGE_GROUP\",\n",
    "        \"DIM_MANAGEMENT_LEVEL\",\n",
    "        \"DIM_AREA_TYPE\",\n",
    "        \"DIM_QUANTILE\",\n",
    "        \"DIM_SDG_INDICATOR\",\n",
    "        \"DIM_OCU_TYPE\",\n",
    "        \"DIM_REP_TYPE\",\n",
    "        \"DIM_SECTOR\",\n",
    "        \"DIM_ALCOHOL_TYPE\",\n",
    "        \"INDICATOR_CODE\"\n",
    "        ],\n",
    "    keep = False\n",
    "\n",
    ")]\n",
    "\n",
    "duplicates.to_csv(\n",
    "    path_or_buf = cwd / 'data_out' / 'duplicates.csv',\n",
    "    sep = \";\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}