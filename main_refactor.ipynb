{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Preliminaries\" data-toc-modified-id=\"Preliminaries-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Preliminaries</a></span><ul class=\"toc-item\"><li><span><a href=\"#Define-filepaths\" data-toc-modified-id=\"Define-filepaths-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Define filepaths</a></span></li><li><span><a href=\"#Load-country-list-and-mapping-dictionary\" data-toc-modified-id=\"Load-country-list-and-mapping-dictionary-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Load country list and mapping dictionary</a></span></li><li><span><a href=\"#Read-data-dictionary\" data-toc-modified-id=\"Read-data-dictionary-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Read data dictionary</a></span></li></ul></li><li><span><a href=\"#Staging-(pre-processing)-to-create-exceptional-indicators´-raw-data\" data-toc-modified-id=\"Staging-(pre-processing)-to-create-exceptional-indicators´-raw-data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Staging (pre-processing) to create exceptional indicators´ raw data</a></span></li><li><span><a href=\"#Extract---Transform---Load-Loop\" data-toc-modified-id=\"Extract---Transform---Load-Loop-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Extract - Transform - Load Loop</a></span><ul class=\"toc-item\"><li><span><a href=\"#API-sources\" data-toc-modified-id=\"API-sources-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>API sources</a></span><ul class=\"toc-item\"><li><span><a href=\"#CSV-API-sources\" data-toc-modified-id=\"CSV-API-sources-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>CSV API sources</a></span></li><li><span><a href=\"#JSON-API-sources\" data-toc-modified-id=\"JSON-API-sources-3.1.2\"><span class=\"toc-item-num\">3.1.2&nbsp;&nbsp;</span>JSON API sources</a></span></li></ul></li><li><span><a href=\"#HTML-Sources\" data-toc-modified-id=\"HTML-Sources-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>HTML Sources</a></span><ul class=\"toc-item\"><li><span><a href=\"#UN-Treaty-Sources\" data-toc-modified-id=\"UN-Treaty-Sources-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>UN Treaty Sources</a></span></li></ul></li><li><span><a href=\"#Export-concatented-dataframe\" data-toc-modified-id=\"Export-concatented-dataframe-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Export concatented dataframe</a></span></li></ul></li><li><span><a href=\"#DEVELOPMENT-AND-TRASH-AREA\" data-toc-modified-id=\"DEVELOPMENT-AND-TRASH-AREA-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>DEVELOPMENT AND TRASH AREA</a></span><ul class=\"toc-item\"><li><span><a href=\"#Extract-selenium-sources--->-This-code-is-stable-as-of-06.11.20,-TO-DO-is-to-put-this-into-a-loop-(which-must-be-done-in-container,-so-I-can-only-do-it-once-James-has-looked-at-the-issue-with-Chrome-driver)\" data-toc-modified-id=\"Extract-selenium-sources--->-This-code-is-stable-as-of-06.11.20,-TO-DO-is-to-put-this-into-a-loop-(which-must-be-done-in-container,-so-I-can-only-do-it-once-James-has-looked-at-the-issue-with-Chrome-driver)-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Extract selenium sources --&gt; This code is stable as of 06.11.20, TO DO is to put this into a loop (which must be done in container, so I can only do it once James has looked at the issue with Chrome driver)</a></span></li><li><span><a href=\"#Extract-the-countries\" data-toc-modified-id=\"Extract-the-countries-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Extract the countries</a></span><ul class=\"toc-item\"><li><span><a href=\"#Extract-UN-Treaty-data\" data-toc-modified-id=\"Extract-UN-Treaty-data-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>Extract UN Treaty data</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required standard libraries\n",
    "import pandas as pd\n",
    "import json\n",
    "import urllib\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import bs4 as bs\n",
    "import selenium\n",
    "import html5lib\n",
    "# import nltk\n",
    "import datetime\n",
    "from selenium import webdriver\n",
    "\n",
    "# Extractors \n",
    "import extract\n",
    "\n",
    "# Cleansers (cluster specific)\n",
    "import cleanse\n",
    "\n",
    "# Normalizer (generalised across all clusters)\n",
    "from normalize import scaler\n",
    "\n",
    "# Utils\n",
    "# from utils import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the export path for all data exports\n",
    "from pathlib import Path\n",
    "\n",
    "# Current working directory\n",
    "cwd = Path('.')\n",
    "\n",
    "# Folder with data-in artifacts, quired to run this script\n",
    "data_in = cwd / 'data_in'\n",
    "\n",
    "# Folder to export raw data\n",
    "data_sources_raw = cwd / 'data_out' / 'data_raw'\n",
    "data_sources_raw.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Folder to export cleansed data\n",
    "data_sources_cleansed = cwd / 'data_out' / 'data_cleansed'\n",
    "data_sources_cleansed.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Folder to export normalized data\n",
    "data_sources_normalized = cwd / 'data_out' / 'data_normalized'\n",
    "data_sources_normalized.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load country list and mapping dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the list of countries which contains all different variations of country names \n",
    "country_full_list = pd.read_excel(\n",
    "    data_in / 'all_countrynames_list.xlsx',\n",
    "    keep_default_na = False).drop_duplicates()\n",
    "\n",
    "# Create a version of the list with unique ISO2 and ISO3 codes\n",
    "country_iso_list = country_full_list.drop_duplicates(subset = 'COUNTRY_ISO_2')\n",
    "\n",
    "# Country CRBA list, this is the list of the countries that should be in the final CRBA indicator list\n",
    "country_crba_list = pd.read_excel(\n",
    "    data_in / 'crba_country_list.xlsx',\n",
    "    header = None,\n",
    "    usecols = [0, 1], \n",
    "    names = ['COUNTRY_ISO_3', 'COUNTRY_NAME']).merge(\n",
    "        right = country_iso_list[['COUNTRY_ISO_2', 'COUNTRY_ISO_3']],\n",
    "        how = 'left',\n",
    "        on='COUNTRY_ISO_3',\n",
    "        validate = 'one_to_one')\n",
    "\n",
    "# Run the column mapper script to load the mapping dictionary\n",
    "with open(data_in / 'column_mapping.py') as file:\n",
    "    exec(file.read())\n",
    "\n",
    "# Run the column mapper script to load the mapping dictionary\n",
    "with open(data_in / 'value_mapping.py') as file:\n",
    "    exec(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sources sheet\n",
    "crba_data_dictionary_source = pd.read_excel(\n",
    "    data_in / 'indicator_dictionary_CRBA.xlsx',\n",
    "    sheet_name = \"Source\",\n",
    "    keep_default_na = False\n",
    ")\n",
    "\n",
    "# snapshot sheet\n",
    "crba_data_dictionary_snapshot = pd.read_excel(\n",
    "    data_in / 'indicator_dictionary_CRBA.xlsx',\n",
    "    sheet_name = \"Snapshot\",\n",
    "    keep_default_na = False\n",
    ")\n",
    "\n",
    "# indicator sheet\n",
    "crba_data_dictionary_indicator = pd.read_excel(\n",
    "    data_in / 'indicator_dictionary_CRBA.xlsx',\n",
    "    sheet_name = \"Indicator\",\n",
    "    keep_default_na = False\n",
    ")\n",
    "\n",
    "# Input lists\n",
    "crba_data_dictionary_input_list = pd.read_excel(\n",
    "    data_in / 'indicator_dictionary_CRBA.xlsx',\n",
    "    sheet_name = \"Input_Lists\",\n",
    "    keep_default_na = False\n",
    ")\n",
    "\n",
    "# Add 2-digit shortcodes of index, issue and category to indicators sheet\n",
    "crba_data_dictionary_indicator = crba_data_dictionary_indicator.merge(\n",
    "    right=crba_data_dictionary_input_list[['INDEX', 'INDEX_CODE']],\n",
    "    left_on='INDEX',\n",
    "    right_on='INDEX'\n",
    ").merge(\n",
    "    right=crba_data_dictionary_input_list[['ISSUE', 'ISSUE_CODE']],\n",
    "    left_on='ISSUE',\n",
    "    right_on='ISSUE'\n",
    ").merge(\n",
    "    right=crba_data_dictionary_input_list[['CATEGORY', 'CATEGORY_CODE']],\n",
    "    left_on='CATEGORY',\n",
    "    right_on='CATEGORY'\n",
    ")\n",
    "\n",
    "# Create indicator code prefix (INDEX-ISSUE_CAEGORY CODE)\n",
    "crba_data_dictionary_indicator = crba_data_dictionary_indicator.assign(\n",
    "    INDICATOR_CODE_PREFIX = crba_data_dictionary_indicator.INDEX_CODE +\n",
    "    \"_\" +\n",
    "    crba_data_dictionary_indicator.ISSUE_CODE+\n",
    "    \"_\"+\n",
    "    crba_data_dictionary_indicator.CATEGORY_CODE+\n",
    "    \"_\")\n",
    "\n",
    "# Create indicator code\n",
    "crba_data_dictionary_indicator = crba_data_dictionary_indicator.assign(\n",
    "    INDICATOR_CODE = crba_data_dictionary_indicator.INDICATOR_CODE_PREFIX + crba_data_dictionary_indicator.INDICATOR_NAME.apply(\n",
    "    lambda x: utils.create_ind_code(x)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib, inspect\n",
    "\n",
    "extractors = { \n",
    "    cls.type: cls for name, cls in inspect.getmembers(\n",
    "        importlib.import_module(\"extract\"), \n",
    "        inspect.isclass\n",
    "    ) if hasattr(cls, 'type')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Staging (pre-processing) to create exceptional indicators´ raw data \n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process S-180 and S-181\n",
    "# Important: File requires having filepaths from above defined and pandas already imported\n",
    "\n",
    "with open(data_in / 'staging_create_raw_data.py') as file:\n",
    "    exec(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract - Transform - Load Loop\n",
    "## API sources\n",
    "### CSV API sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV sources\n",
    "api_sources = crba_data_dictionary_source[\n",
    "    (crba_data_dictionary_source[\"SOURCE_TYPE\"] == \"API (ILO)\") | \n",
    "    (crba_data_dictionary_source[\"SOURCE_TYPE\"] == \"API (UNESCO)\") | \n",
    "    (crba_data_dictionary_source[\"SOURCE_TYPE\"] == \"API (WHO)\") | \n",
    "    (crba_data_dictionary_source[\"SOURCE_TYPE\"] == \"API (UNICEF)\")\n",
    "].merge(\n",
    "    right = crba_data_dictionary_snapshot,\n",
    "    on = \"SOURCE_ID\"\n",
    ").merge(\n",
    "    right = crba_data_dictionary_indicator,\n",
    "    on = 'INDICATOR_ID'\n",
    ")\n",
    "\n",
    "# define emty dataframe\n",
    "combined_cleansed_csv = pd.DataFrame()\n",
    "combined_normalized_csv = pd.DataFrame()\n",
    "\n",
    "# Loop to extract data from API sources\n",
    "for index, row in api_sources.iterrows():\n",
    "    # Log\n",
    "    print(\"\\n - - - - - \\n Extracting source {} \\n\".format(row[\"SOURCE_ID\"]))\n",
    "    \n",
    "    # Extraction section\n",
    "    try:\n",
    "        # Extract data\n",
    "        dataframe = extract.CSVExtractor.extract(url = row[\"ENDPOINT_URL\"])\n",
    "        \n",
    "        # Save raw data\n",
    "        dataframe.to_csv(\n",
    "            data_sources_raw / str(row[\"SOURCE_ID\"] + \"_raw.csv\"),\n",
    "            sep = \";\"\n",
    "            )\n",
    "    \n",
    "    except:\n",
    "       print(\"There was a problem with extraction of source {} \\n\".format(row[\"SOURCE_ID\"]))\n",
    "    \n",
    "    \n",
    "    # Log that we are entering cleasning\n",
    "    print(\"\\n - - - - - \\n Cleansing source {} \\n\".format(row[\"SOURCE_ID\"]))\n",
    "    \n",
    "    # Cleansing\n",
    "    dataframe = cleanse.Cleanser().extract_who_raw_data(\n",
    "        raw_data=dataframe,\n",
    "        variable_type = row[\"VALUE_LABELS\"],\n",
    "        display_value_col=\"Display Value\"\n",
    "    )\n",
    "    \n",
    "    dataframe = cleanse.Cleanser().rename_and_discard_columns(\n",
    "        raw_data=dataframe,\n",
    "        mapping_dictionary=mapping_dict,\n",
    "        final_sdmx_col_list=sdmx_df_columns_all\n",
    "    )\n",
    "\n",
    "    dataframe = cleanse.Cleanser().retrieve_latest_observation(\n",
    "        renamed_data=dataframe,\n",
    "        dim_cols = sdmx_df_columns_dims,\n",
    "        country_cols = sdmx_df_columns_country,\n",
    "        time_cols = sdmx_df_columns_time,\n",
    "        attr_cols=sdmx_df_columns_attr,\n",
    "    )\n",
    "\n",
    "    dataframe = cleanse.Cleanser().add_and_discard_countries(\n",
    "        grouped_data=dataframe,\n",
    "        crba_country_list=country_crba_list,\n",
    "        country_list_full = country_full_list\n",
    "    )\n",
    "\n",
    "    dataframe = cleanse.Cleanser().add_cols_fill_cells(\n",
    "        grouped_data_iso_filt=dataframe,\n",
    "        dim_cols=sdmx_df_columns_dims,\n",
    "        time_cols=sdmx_df_columns_time,\n",
    "        indicator_name_string=row[\"INDICATOR_NAME_x\"],\n",
    "        index_name_string=row[\"INDEX\"],\n",
    "        issue_name_string=row[\"ISSUE\"],\n",
    "        category_name_string=row[\"CATEGORY\"],\n",
    "        indicator_code_string=row[\"INDICATOR_CODE\"],\n",
    "        indicator_source_string=row[\"ADDRESS\"],\n",
    "        indicator_source_body_string=row[\"SOURCE_BODY\"],\n",
    "        indicator_description_string=row[\"INDICATOR_DESCRIPTION\"],\n",
    "        indicator_explanation_string=row[\"INDICATOR_EXPLANATION\"],\n",
    "        indicator_data_extraction_methodology_string=row[\"EXTRACTION_METHODOLOGY\"],\n",
    "        source_title_string=row[\"SOURCE_TITLE\"],\n",
    "        source_api_link_string=row[\"ENDPOINT_URL\"]\n",
    "    )\n",
    "\n",
    "    dataframe = cleanse.Cleanser().map_values(\n",
    "        cleansed_data = dataframe,\n",
    "        value_mapping_dict = value_mapper\n",
    "    )\n",
    "    \n",
    "    dataframe_cleansed = cleanse.Cleanser().encode_categorical_variables(\n",
    "        dataframe = dataframe,\n",
    "        encoding_string = row[\"VALUE_LABELS\"]\n",
    "    )\n",
    "\n",
    "    cleanse.Cleanser().create_log_report(\n",
    "        cleansed_data=dataframe_cleansed\n",
    "    )\n",
    "\n",
    "    # Append dataframe to combined dataframe\n",
    "    combined_cleansed_csv = combined_cleansed_csv.append(\n",
    "        other = dataframe_cleansed\n",
    "    )\n",
    "\n",
    "    # Save cleansed data\n",
    "    dataframe_cleansed.to_csv(\n",
    "        data_sources_cleansed / str(row[\"SOURCE_ID\"] + \"_cleansed.csv\"),\n",
    "        sep = \";\")\n",
    "    \n",
    "    # Normalizing\n",
    "    dataframe_normalized = scaler.normalizer(\n",
    "        cleansed_data = dataframe_cleansed,\n",
    "        sql_subset_query_string=row[\"DIMENSION_VALUES_NORMALIZATION\"],\n",
    "        # dim_cols=sdmx_df_columns_dims,\n",
    "        variable_type = row[\"VALUE_LABELS\"],\n",
    "        is_inverted = row[\"INVERT_NORMALIZATION\"],\n",
    "        whisker_factor=1.5,\n",
    "        raw_data_col=\"RAW_OBS_VALUE\",\n",
    "        scaled_data_col_name=\"SCALED_OBS_VALUE\",\n",
    "        maximum_score=10,\n",
    "        )\n",
    "    \n",
    "    dataframe_normalized.to_csv(\n",
    "        data_sources_normalized / str(row[\"SOURCE_ID\"] + \"_normalized.csv\"),\n",
    "        sep = \";\")\n",
    "\n",
    "    # Append dataframe to combined dataframe\n",
    "    combined_normalized_csv = combined_normalized_csv.append(\n",
    "        other = dataframe_normalized\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON API sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# JSON sources\n",
    "api_sources = crba_data_dictionary_source[\n",
    "    (crba_data_dictionary_source[\"SOURCE_TYPE\"] == \"API (SDG)\")\n",
    "].merge(\n",
    "    right = crba_data_dictionary_snapshot,\n",
    "    on = \"SOURCE_ID\"\n",
    ").merge(\n",
    "    right = crba_data_dictionary_indicator,\n",
    "    on = 'INDICATOR_ID'\n",
    ")\n",
    "\n",
    "# Loop to extract data from API sources\n",
    "for index, row in api_sources.iterrows():\n",
    "    # Log\n",
    "    print(\"\\n - - - - - \\n Extracting source {} \\n\".format(row[\"SOURCE_ID\"]))\n",
    "    \n",
    "    # Exraction section\n",
    "    try:\n",
    "        # Extract data \n",
    "        dataframe = extract.JSONExtractor.extract(url = row[\"ENDPOINT_URL\"])\n",
    "        \n",
    "        # Save dataframe\n",
    "        dataframe.to_csv(\n",
    "            data_sources_raw / str(row[\"SOURCE_ID\"] + \"_raw.csv\"),\n",
    "            sep = \";\")\n",
    "    except:\n",
    "        print(\"There was an issue with source {}\".format(row[\"SOURCE_ID\"]))\n",
    "    \n",
    "    # Log that we are entering cleasning\n",
    "    print(\"\\n - - - - - \\n Cleansing source {} \\n\".format(row[\"SOURCE_ID\"]))\n",
    "    \n",
    "    # Cleansing in \n",
    "    dataframe = cleanse.Cleanser().extract_who_raw_data(\n",
    "        raw_data=dataframe,\n",
    "        variable_type = row[\"VALUE_LABELS\"],\n",
    "        display_value_col=\"Display Value\"\n",
    "    )\n",
    "    \n",
    "    dataframe = cleanse.Cleanser().rename_and_discard_columns(\n",
    "        raw_data=dataframe,\n",
    "        mapping_dictionary=mapping_dict,\n",
    "        final_sdmx_col_list=sdmx_df_columns_all\n",
    "    )\n",
    "\n",
    "    dataframe = cleanse.Cleanser().retrieve_latest_observation(\n",
    "        renamed_data=dataframe,\n",
    "        dim_cols = sdmx_df_columns_dims,\n",
    "        country_cols = sdmx_df_columns_country,\n",
    "        time_cols = sdmx_df_columns_time,\n",
    "        attr_cols=sdmx_df_columns_attr,\n",
    "    )\n",
    "\n",
    "    dataframe = cleanse.Cleanser().add_and_discard_countries(\n",
    "        grouped_data=dataframe,\n",
    "        crba_country_list=country_crba_list,\n",
    "        country_list_full = country_full_list\n",
    "    )\n",
    "\n",
    "    dataframe = cleanse.Cleanser().add_cols_fill_cells(\n",
    "        grouped_data_iso_filt=dataframe,\n",
    "        dim_cols=sdmx_df_columns_dims,\n",
    "        time_cols=sdmx_df_columns_time,\n",
    "        indicator_name_string=row[\"INDICATOR_NAME_x\"],\n",
    "        index_name_string=row[\"INDEX\"],\n",
    "        issue_name_string=row[\"ISSUE\"],\n",
    "        category_name_string=row[\"CATEGORY\"],\n",
    "        indicator_code_string=row[\"INDICATOR_CODE\"],\n",
    "        indicator_source_string=row[\"ADDRESS\"],\n",
    "        indicator_source_body_string=row[\"SOURCE_BODY\"],\n",
    "        indicator_description_string=row[\"INDICATOR_DESCRIPTION\"],\n",
    "        source_title_string=row[\"SOURCE_TITLE\"],\n",
    "        indicator_explanation_string=row[\"INDICATOR_EXPLANATION\"],\n",
    "        indicator_data_extraction_methodology_string=row[\"EXTRACTION_METHODOLOGY\"],\n",
    "        source_api_link_string=row[\"ENDPOINT_URL\"]\n",
    "    )\n",
    "\n",
    "    dataframe = cleanse.Cleanser().map_values(\n",
    "        cleansed_data = dataframe,\n",
    "        value_mapping_dict = value_mapper\n",
    "    )\n",
    "    \n",
    "    dataframe_cleansed = cleanse.Cleanser().encode_categorical_variables(\n",
    "        dataframe = dataframe,\n",
    "        encoding_string = row[\"VALUE_LABELS\"]\n",
    "    )\n",
    "\n",
    "    cleanse.Cleanser().create_log_report(\n",
    "        cleansed_data=dataframe_cleansed\n",
    "    )\n",
    "\n",
    "    # Append dataframe to combined dataframe\n",
    "    combined_cleansed_csv = combined_cleansed_csv.append(\n",
    "        other = dataframe_cleansed\n",
    "    )\n",
    "\n",
    "    # Save cleansed data\n",
    "    dataframe_cleansed.to_csv(\n",
    "        data_sources_cleansed / str(row[\"SOURCE_ID\"] + \"_cleansed.csv\"),\n",
    "        sep = \";\")\n",
    "    \n",
    "    # Normalizing section\n",
    "    dataframe_normalized = scaler.normalizer(\n",
    "        cleansed_data = dataframe_cleansed,\n",
    "        sql_subset_query_string=row[\"DIMENSION_VALUES_NORMALIZATION\"],\n",
    "        # dim_cols=sdmx_df_columns_dims,\n",
    "        variable_type = row[\"VALUE_LABELS\"],\n",
    "        is_inverted = row[\"INVERT_NORMALIZATION\"],\n",
    "        whisker_factor=1.5,\n",
    "        raw_data_col=\"RAW_OBS_VALUE\",\n",
    "        scaled_data_col_name=\"SCALED_OBS_VALUE\",\n",
    "        maximum_score=10,\n",
    "        )\n",
    "    \n",
    "    dataframe_normalized.to_csv(\n",
    "        data_sources_normalized / str(row[\"SOURCE_ID\"] + \"_normalized.csv\"),\n",
    "        sep = \";\")\n",
    "\n",
    "    # Append dataframe to combined dataframe\n",
    "    combined_normalized_csv = combined_normalized_csv.append(\n",
    "        other = dataframe_normalized\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML Sources\n",
    "### UN Treaty Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UN Treaty HTML sources\n",
    "api_sources = crba_data_dictionary_source[\n",
    "    (crba_data_dictionary_source[\"SOURCE_BODY\"] == \"UN Treaties\")\n",
    "].merge(\n",
    "    right = crba_data_dictionary_snapshot,\n",
    "    on = \"SOURCE_ID\"\n",
    ").merge(\n",
    "    right = crba_data_dictionary_indicator,\n",
    "    on = 'INDICATOR_ID'\n",
    ")\n",
    "\n",
    "# Loop to extract data from API sources\n",
    "for index, row in api_sources.iterrows():\n",
    "    # Log\n",
    "    print(\"\\n - - - - - \\n Extracting source {} \\n\".format(row[\"SOURCE_ID\"]))\n",
    "    \n",
    "    # Exraction section\n",
    "    dataframe = extract.HTMLExtractor().extract(url = row[\"ADDRESS\"])\n",
    "    \n",
    "    # Save dataframe\n",
    "    dataframe.to_csv(\n",
    "        data_sources_raw / str(row[\"SOURCE_ID\"] + \"_raw.csv\"),\n",
    "        sep = \";\")\n",
    "\n",
    "    # Cleansing\n",
    "    # Log that we are entering cleasning\n",
    "    print(\"\\n - - - - - \\n Cleansing source {} \\n\".format(row[\"SOURCE_ID\"]))\n",
    "    \n",
    "    # Cleansing\n",
    "    dataframe = cleanse.Cleanser().rename_and_discard_columns(\n",
    "        raw_data=dataframe,\n",
    "        mapping_dictionary=mapping_dict,\n",
    "        final_sdmx_col_list=sdmx_df_columns_all\n",
    "    )\n",
    "\n",
    "    dataframe = cleanse.Cleanser().add_and_discard_countries(\n",
    "        grouped_data=dataframe,\n",
    "        crba_country_list=country_crba_list,\n",
    "        country_list_full = country_full_list\n",
    "    )\n",
    "\n",
    "    dataframe = cleanse.Cleanser().add_cols_fill_cells(\n",
    "        grouped_data_iso_filt=dataframe,\n",
    "        dim_cols=sdmx_df_columns_dims,\n",
    "        time_cols=sdmx_df_columns_time,\n",
    "        indicator_name_string=row[\"INDICATOR_NAME_x\"],\n",
    "        index_name_string=row[\"INDEX\"],\n",
    "        issue_name_string=row[\"ISSUE\"],\n",
    "        category_name_string=row[\"CATEGORY\"],\n",
    "        indicator_code_string=row[\"INDICATOR_CODE\"],\n",
    "        indicator_source_string=row[\"ADDRESS\"],\n",
    "        indicator_source_body_string=row[\"SOURCE_BODY\"],\n",
    "        indicator_description_string=row[\"INDICATOR_DESCRIPTION\"],\n",
    "        source_title_string=row[\"SOURCE_TITLE\"],\n",
    "        indicator_explanation_string=row[\"INDICATOR_EXPLANATION\"],\n",
    "        indicator_data_extraction_methodology_string=row[\"EXTRACTION_METHODOLOGY\"],\n",
    "        source_api_link_string=row[\"ENDPOINT_URL\"]\n",
    "    )\n",
    "\n",
    "    dataframe_cleansed = cleanse.Cleanser().encode_ilo_un_treaty_data(\n",
    "        dataframe = dataframe,\n",
    "        treaty_source_body = row[\"SOURCE_BODY\"]\n",
    "    )\n",
    "\n",
    "    cleanse.Cleanser().create_log_report(\n",
    "        cleansed_data=dataframe_cleansed\n",
    "    )\n",
    "\n",
    "    # Append dataframe to combined dataframe\n",
    "    combined_cleansed_csv = combined_cleansed_csv.append(\n",
    "        other = dataframe_cleansed\n",
    "    )\n",
    "\n",
    "    # Save cleansed data\n",
    "    dataframe_cleansed.to_csv(\n",
    "        data_sources_cleansed / str(row[\"SOURCE_ID\"] + \"_cleansed.csv\"),\n",
    "        sep = \";\")\n",
    "    \n",
    "    # Normalizing section\n",
    "    dataframe_normalized = scaler.normalizer(\n",
    "        cleansed_data = dataframe_cleansed,\n",
    "        sql_subset_query_string=row[\"DIMENSION_VALUES_NORMALIZATION\"],\n",
    "        # dim_cols=sdmx_df_columns_dims,\n",
    "        variable_type = row[\"VALUE_LABELS\"],\n",
    "        is_inverted = row[\"INVERT_NORMALIZATION\"],\n",
    "        whisker_factor=1.5,\n",
    "        raw_data_col=\"RAW_OBS_VALUE\",\n",
    "        scaled_data_col_name=\"SCALED_OBS_VALUE\",\n",
    "        maximum_score=10,\n",
    "        )\n",
    "    \n",
    "    dataframe_normalized.to_csv(\n",
    "        data_sources_normalized / str(row[\"SOURCE_ID\"] + \"_normalized.csv\"),\n",
    "        sep = \";\")\n",
    "\n",
    "    # Append dataframe to combined dataframe\n",
    "    combined_normalized_csv = combined_normalized_csv.append(\n",
    "        other = dataframe_normalized\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export concatented dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # # CLEANSED DATA\n",
    "\n",
    "# Idenify all dimension columns in combined dataframe\n",
    "available_dim_cols = []\n",
    "for col in combined_cleansed_csv.columns:\n",
    "    dim_col = re.findall(\"DIM_.+\", col)\n",
    "    # print(dim_col)\n",
    "    if len(dim_col) == 1:\n",
    "        available_dim_cols += dim_col\n",
    "\n",
    "# Fill _T for all NA values of dimension columns\n",
    "# 5b Fill in current year for time variable\n",
    "combined_cleansed_csv[available_dim_cols] = combined_cleansed_csv[\n",
    "    available_dim_cols\n",
    "].fillna(value=\"_T\")\n",
    "\n",
    "# Export combined cleansed dataframe as a sample\n",
    "combined_cleansed_csv.to_csv(\n",
    "    path_or_buf = cwd / 'data_out' / 'combined_cleansed.csv',\n",
    "    sep = \";\"\n",
    ")\n",
    "\n",
    "# # # # # NORMALIZED DATA\n",
    "\n",
    "# Idenify all dimension columns in combined dataframe\n",
    "available_dim_cols = []\n",
    "for col in combined_normalized_csv.columns:\n",
    "    dim_col = re.findall(\"DIM_.+\", col)\n",
    "    # print(dim_col)\n",
    "    if len(dim_col) == 1:\n",
    "        available_dim_cols += dim_col\n",
    "\n",
    "# Fill _T for all NA values of dimension columns\n",
    "# 5b Fill in current year for time variable\n",
    "combined_normalized_csv[available_dim_cols] = combined_normalized_csv[\n",
    "    available_dim_cols\n",
    "].fillna(value=\"_T\")\n",
    "\n",
    "# Export combined cleansed dataframe as a sample\n",
    "combined_normalized_csv.to_csv(\n",
    "    path_or_buf = cwd / 'data_out' / 'combined_normalized.csv',\n",
    "    sep = \";\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEVELOPMENT AND TRASH AREA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4 as bs\n",
    "import pandas as pd\n",
    "import selenium\n",
    "import os\n",
    "from pathlib import Path\n",
    "from selenium import webdriver\n",
    "\n",
    "# cwd = Path('.')\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Current working directory\n",
    "driver_location = cwd + '\\\\geckodriver.exe'\n",
    "\n",
    "print(driver_location)\n",
    "\n",
    "# Open the targete html. Must be done with selenium, because it doesnt work with normal URL request\n",
    "#driver = webdriver.Firefox(executable_path=\"D:/Documents/2020/28_UNICEF/10_working_repo/data-etl/geckodriver.exe\")\n",
    "driver = webdriver.Firefox(executable_path=driver_location)\n",
    "\n",
    "# Get HTTP response\n",
    "response = driver.get(\"https://www.ilo.org/dyn/normlex/en/f?p=NORMLEXPUB:11300:0::NO:11300:P11300_INSTRUMENT_ID:312256:NO\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract selenium sources --> This code is stable as of 06.11.20, TO DO is to put this into a loop (which must be done in container, so I can only do it once James has looked at the issue with Chrome driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michael\\anaconda3\\envs\\unicef-test\\lib\\site-packages\\ipykernel_launcher.py:32: DeprecationWarning: use options instead of chrome_options\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Calling function 'rename_and_discard_columns'...\n",
      "\n",
      " Calling function 'add_and_discard_countries'...\n",
      "\n",
      " Calling function 'add_cols_fill_cells'...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COUNTRY_NAME</th>\n",
       "      <th>ATTR_TREATY_STATUS</th>\n",
       "      <th>ATTR_FOOTNOTE_OF_SOURCE</th>\n",
       "      <th>COUNTRY_ISO_2</th>\n",
       "      <th>COUNTRY_ISO_3</th>\n",
       "      <th>_merge</th>\n",
       "      <th>RAW_OBS_VALUE</th>\n",
       "      <th>ATTR_ENCODING_LABELS</th>\n",
       "      <th>SCALED_OBS_VALUE</th>\n",
       "      <th>OBS_STATUS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>In Force</td>\n",
       "      <td></td>\n",
       "      <td>AF</td>\n",
       "      <td>AFG</td>\n",
       "      <td>both</td>\n",
       "      <td>2</td>\n",
       "      <td>2=Yes, 1=No; as answer to the following questi...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Albania</td>\n",
       "      <td>In Force</td>\n",
       "      <td>Excluding Article 11 by virtue of the ratifica...</td>\n",
       "      <td>AL</td>\n",
       "      <td>ALB</td>\n",
       "      <td>both</td>\n",
       "      <td>2</td>\n",
       "      <td>2=Yes, 1=No; as answer to the following questi...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>In Force</td>\n",
       "      <td></td>\n",
       "      <td>DZ</td>\n",
       "      <td>DZA</td>\n",
       "      <td>both</td>\n",
       "      <td>2</td>\n",
       "      <td>2=Yes, 1=No; as answer to the following questi...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Argentina</td>\n",
       "      <td>In Force</td>\n",
       "      <td></td>\n",
       "      <td>AR</td>\n",
       "      <td>ARG</td>\n",
       "      <td>both</td>\n",
       "      <td>2</td>\n",
       "      <td>2=Yes, 1=No; as answer to the following questi...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Armenia</td>\n",
       "      <td>In Force</td>\n",
       "      <td>Excluding Article 11 by virtue of the ratifica...</td>\n",
       "      <td>AM</td>\n",
       "      <td>ARM</td>\n",
       "      <td>both</td>\n",
       "      <td>2</td>\n",
       "      <td>2=Yes, 1=No; as answer to the following questi...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>USA</td>\n",
       "      <td>right_only</td>\n",
       "      <td>1</td>\n",
       "      <td>2=Yes, 1=No; as answer to the following questi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UZB</td>\n",
       "      <td>right_only</td>\n",
       "      <td>1</td>\n",
       "      <td>2=Yes, 1=No; as answer to the following questi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>VUT</td>\n",
       "      <td>right_only</td>\n",
       "      <td>1</td>\n",
       "      <td>2=Yes, 1=No; as answer to the following questi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>VNM</td>\n",
       "      <td>right_only</td>\n",
       "      <td>1</td>\n",
       "      <td>2=Yes, 1=No; as answer to the following questi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ZWE</td>\n",
       "      <td>right_only</td>\n",
       "      <td>1</td>\n",
       "      <td>2=Yes, 1=No; as answer to the following questi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>197 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    COUNTRY_NAME ATTR_TREATY_STATUS  \\\n",
       "0    Afghanistan           In Force   \n",
       "1        Albania           In Force   \n",
       "2        Algeria           In Force   \n",
       "3      Argentina           In Force   \n",
       "4        Armenia           In Force   \n",
       "..           ...                ...   \n",
       "192          NaN                NaN   \n",
       "193          NaN                NaN   \n",
       "194          NaN                NaN   \n",
       "195          NaN                NaN   \n",
       "196          NaN                NaN   \n",
       "\n",
       "                               ATTR_FOOTNOTE_OF_SOURCE COUNTRY_ISO_2  \\\n",
       "0                                                                 AF   \n",
       "1    Excluding Article 11 by virtue of the ratifica...            AL   \n",
       "2                                                                 DZ   \n",
       "3                                                                 AR   \n",
       "4    Excluding Article 11 by virtue of the ratifica...            AM   \n",
       "..                                                 ...           ...   \n",
       "192                                                NaN           NaN   \n",
       "193                                                NaN           NaN   \n",
       "194                                                NaN           NaN   \n",
       "195                                                NaN           NaN   \n",
       "196                                                NaN           NaN   \n",
       "\n",
       "    COUNTRY_ISO_3      _merge RAW_OBS_VALUE  \\\n",
       "0             AFG        both             2   \n",
       "1             ALB        both             2   \n",
       "2             DZA        both             2   \n",
       "3             ARG        both             2   \n",
       "4             ARM        both             2   \n",
       "..            ...         ...           ...   \n",
       "192           USA  right_only             1   \n",
       "193           UZB  right_only             1   \n",
       "194           VUT  right_only             1   \n",
       "195           VNM  right_only             1   \n",
       "196           ZWE  right_only             1   \n",
       "\n",
       "                                  ATTR_ENCODING_LABELS  SCALED_OBS_VALUE  \\\n",
       "0    2=Yes, 1=No; as answer to the following questi...              10.0   \n",
       "1    2=Yes, 1=No; as answer to the following questi...              10.0   \n",
       "2    2=Yes, 1=No; as answer to the following questi...              10.0   \n",
       "3    2=Yes, 1=No; as answer to the following questi...              10.0   \n",
       "4    2=Yes, 1=No; as answer to the following questi...              10.0   \n",
       "..                                                 ...               ...   \n",
       "192  2=Yes, 1=No; as answer to the following questi...               0.0   \n",
       "193  2=Yes, 1=No; as answer to the following questi...               0.0   \n",
       "194  2=Yes, 1=No; as answer to the following questi...               0.0   \n",
       "195  2=Yes, 1=No; as answer to the following questi...               0.0   \n",
       "196  2=Yes, 1=No; as answer to the following questi...               0.0   \n",
       "\n",
       "    OBS_STATUS  \n",
       "0          nan  \n",
       "1          nan  \n",
       "2          nan  \n",
       "3          nan  \n",
       "4          nan  \n",
       "..         ...  \n",
       "192        nan  \n",
       "193        nan  \n",
       "194        nan  \n",
       "195        nan  \n",
       "196        nan  \n",
       "\n",
       "[197 rows x 10 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# Specify location of chromedriver\n",
    "cwd = os.getcwd()\n",
    "driver_location = cwd + '\\\\chromedriver.exe'\n",
    "\n",
    "# Add option to make it headless (so that it doesn't open an actual chrome window)\n",
    "options = Options()\n",
    "options.headless = True\n",
    "driver = webdriver.Chrome(driver_location, chrome_options=options)\n",
    "\n",
    "# Get HTTP response\n",
    "# response = driver.get(\"https://www.ilo.org/dyn/normlex/en/f?p=NORMLEXPUB:11300:0::NO:11300:P11300_INSTRUMENT_ID:312256:NO\")\n",
    "# response = driver.get(\"https://www.ilo.org/dyn/normlex/en/f?p=NORMLEXPUB:11300:0::NO::P11300_INSTRUMENT_ID:312283\")\n",
    "# response = driver.get(\"https://www.ilo.org/dyn/normlex/en/f?p=NORMLEXPUB:11300:0::NO:11300:P11300_INSTRUMENT_ID:312328:NO\")\n",
    "response = driver.get(\"https://www.ilo.org/dyn/normlex/en/f?p=NORMLEXPUB:11300:0::NO:11300:P11300_INSTRUMENT_ID:312240:NO\")\n",
    "\n",
    "# Get response\n",
    "# response = driver.get(html_url)\n",
    "\n",
    "# Retrieve the actual html\n",
    "html = driver.page_source\n",
    "\n",
    "# Soupify\n",
    "soup = bs.BeautifulSoup(html)\n",
    "\n",
    "# Extract the target table as attribute\n",
    "target_table = str(\n",
    "    soup.find_all(\"table\", {\"cellspacing\": \"0\", \"class\": \"horizontalLine\"})\n",
    ")\n",
    "\n",
    "# Create dataframe with the data\n",
    "raw_data = pd.read_html(io=target_table, header=0)[\n",
    "    0\n",
    "]  # return is a list of DFs, specify [0] to get actual DF\n",
    "\n",
    "# Cleansing\n",
    "dataframe = cleanse.Cleanser().rename_and_discard_columns(\n",
    "    raw_data=raw_data,\n",
    "    mapping_dictionary=mapping_dict,\n",
    "    final_sdmx_col_list=sdmx_df_columns_all\n",
    ")\n",
    "\n",
    "dataframe = cleanse.Cleanser().decompose_country_footnote_ilo_normlex(\n",
    "    dataframe = dataframe,\n",
    "    country_name_list = country_full_list.COUNTRY_NAME\n",
    ")\n",
    "\n",
    "dataframe = cleanse.Cleanser().add_and_discard_countries(\n",
    "    grouped_data=dataframe,\n",
    "    crba_country_list=country_crba_list,\n",
    "    country_list_full = country_full_list\n",
    ")\n",
    "\n",
    "dataframe_cleansed = cleanse.Cleanser().encode_ilo_un_treaty_data(\n",
    "    dataframe = dataframe,\n",
    "    treaty_source_body='ILO NORMLEX'\n",
    ")\n",
    "\n",
    "# Normalizing section\n",
    "dataframe_normalized = scaler.normalizer(\n",
    "    cleansed_data = dataframe_cleansed,\n",
    "    sql_subset_query_string=None\n",
    ")\n",
    "\n",
    "dataframe_normalized\n",
    "# Code works - but still is missing step to map the countries\n",
    "# dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.sub(dataframe[\"ATTR_FOOTNOTE_OF_SOURCE\"][5]\n",
    "       \n",
    "# Speifically for ILO NORMLEX - extract country name if additonal info is given\n",
    "#dataframe[\"ATTR_FOOTNOTE_OF_SOURCE\"] = dataframe[\"COUNTRY_NAME\"]\n",
    "#dataframe[\"COUNTRY_NAME\"] = dataframe[\"COUNTRY_NAME\"].apply(extract_country_name)\n",
    "#dataframe[\"ATTR_FOOTNOTE_OF_SOURCE\"] = dataframe.apply(lambda x: re.sub(x['COUNTRY_NAME'], \"\", x[\"ATTR_FOOTNOTE_OF_SOURCE\"]), 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "import pandas\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# Specify location of chromedriver\n",
    "cwd = os.getcwd()\n",
    "driver_location = cwd + '\\\\chromedriver.exe'\n",
    "\n",
    "# Add option to make it headless (so that it doesn't open an actual chrome window)\n",
    "options = Options()\n",
    "options.headless = True\n",
    "driver = webdriver.Chrome(driver_location, chrome_options=options)\n",
    "\n",
    "# Get HTTP response\n",
    "# response = driver.get(\"https://www.ilo.org/dyn/normlex/en/f?p=NORMLEXPUB:11300:0::NO:11300:P11300_INSTRUMENT_ID:312256:NO\")\n",
    "response = driver.get(\"https://www.ilo.org/dyn/normlex/en/f?p=NORMLEXPUB:11300:0::NO::P11300_INSTRUMENT_ID:312283\")\n",
    "\n",
    "# Get response\n",
    "# response = driver.get(html_url)\n",
    "\n",
    "# Retrieve the actual html\n",
    "html = driver.page_source\n",
    "\n",
    "# Soupify\n",
    "soup = bs.BeautifulSoup(html)\n",
    "\n",
    "# Extract the target table as attribute\n",
    "target_table = str(\n",
    "    soup.find_all(\"table\", {\"cellspacing\": \"0\", \"class\": \"horizontalLine\"})\n",
    ")\n",
    "\n",
    "# Create dataframe with the data\n",
    "raw_data = pd.read_html(io=target_table, header=0)[\n",
    "    0\n",
    "]  # return is a list of DFs, specify [0] to get actual DF\n",
    "\n",
    "# Cleansing\n",
    "dataframe = cleanse.Cleanser().rename_and_discard_columns(\n",
    "    raw_data=raw_data,\n",
    "    mapping_dictionary=mapping_dict,\n",
    "    final_sdmx_col_list=sdmx_df_columns_all\n",
    ")\n",
    "\n",
    "# dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(country_full_list.loc[subset_list, \"COUNTRY_NAME\"].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_full_list.COUNTRY_NAME[47] in dataframe.COUNTRY_NAME[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_full_list.COUNTRY_NAME[47]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe[\"COUNTRY_NAME\"][20]\n",
    "\n",
    "[re.search(x+'*?', dataframe.COUNTRY_NAME[20]) for x in country_full_list.COUNTRY_NAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "temp = extract_country_name(dataframe[\"COUNTRY_NAME\"][20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataframe[\"COUNTRY_NAME\"].apply(extract_country_name).sample(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_country_name(cell, country_name_list = country_full_list.COUNTRY_NAME):\n",
    "    # Determine which country in the full country list is contained in string\n",
    "    subset_list = [x in cell for x in country_name_list]\n",
    "    \n",
    "    # Sometimes several country names match, but we need exactly one\n",
    "    if sum(subset_list)==0:\n",
    "        print(\"No country name match\")\n",
    "    elif sum(subset_list)==1:\n",
    "        # Retrieve the actual country name\n",
    "        country_name = country_name_list[subset_list].item()\n",
    "    else:\n",
    "        # Retrieve the actual country name\n",
    "        country_name = country_name_list[subset_list].iloc[0]\n",
    "    \n",
    "    return country_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_full_list.COUNTRY_NAME[subset_list].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "country_full_list\n",
    "    \n",
    "subset_list = [x in dataframe.COUNTRY_NAME[2] for x in country_full_list.COUNTRY_NAME]\n",
    "\n",
    "dataframe.COUNTRY_NAME[2] = country_full_list.loc[subset_list, \"COUNTRY_NAME\"].item()\n",
    "\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract UN Treaty data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib\n",
    "import bs4 as bs\n",
    "import pandas as pd\n",
    "\n",
    "raw_html_1 = requests.get(\"https://treaties.un.org/pages/ViewDetails.aspx?src=TREATY&mtdsg_no=XVIII-12-a&chapter=18&clang=_en\")\n",
    "\n",
    "# raw_html_1.text\n",
    "raw_html_2 = urllib.request.urlopen(\"https://treaties.un.org/pages/ViewDetails.aspx?src=TREATY&mtdsg_no=XVIII-12-a&chapter=18&clang=_en\")\n",
    "\n",
    "print(raw_html_1)\n",
    "print(raw_html_2)\n",
    "\n",
    "soup = bs.BeautifulSoup(raw_html_1.text, features=\"lxml\")\n",
    "#soup_2 = bs.BeautifulSoup(raw_html_2, features=\"lxml\")\n",
    "\n",
    "#soup_2\n",
    "# soup_1\n",
    "\n",
    "# Extract the target table as attribute\n",
    "target_table = str(\n",
    "    soup.find_all(\n",
    "        \"table\",\n",
    "        {\"class\": \"table table-striped table-bordered table-hover table-condensed\"},\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create dataframe with the data\n",
    "raw_data = pd.read_html(io=target_table, header=0)[\n",
    "    0\n",
    "]\n",
    "\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify duplicates\n",
    "\n",
    "# Checking to see if there are any duplicate entries\n",
    "duplicates = combined_normalized_csv[combined_normalized_csv.duplicated(\n",
    "    subset = [\n",
    "        \"COUNTRY_ISO_3\",\n",
    "        \"TIME_PERIOD\",\n",
    "        \"COUNTRY_NAME\",\n",
    "        \"COUNTRY_ISO_2\",\n",
    "        \"RAW_OBS_VALUE\",\n",
    "        \"DIM_SEX\",\n",
    "        \"DIM_EDU_LEVEL\",\n",
    "        \"DIM_AGE\",\n",
    "        \"DIM_AGE_GROUP\",\n",
    "        \"DIM_MANAGEMENT_LEVEL\",\n",
    "        \"DIM_AREA_TYPE\",\n",
    "        \"DIM_QUANTILE\",\n",
    "        \"DIM_SDG_INDICATOR\",\n",
    "        \"DIM_OCU_TYPE\",\n",
    "        \"DIM_REP_TYPE\",\n",
    "        \"DIM_SECTOR\",\n",
    "        \"INDICATOR_CODE\"\n",
    "        ],\n",
    "    keep = False\n",
    "\n",
    ")]\n",
    "\n",
    "duplicates.to_csv(\n",
    "    path_or_buf = cwd / 'data_out' / 'duplicates.csv',\n",
    "    sep = \";\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
